paragraph
"In recent years, deep pre-training approaches <REF> have brought great break-through in NLP tasks. For question answering systems, it also shows very promising results (like QnA relevance, MRC tasks, etc.). However, due to the sheer amount of parameters, model inference is very time-consuming. Even with powerful GPU machines, the speed is still very limited, as shown in  1 . In a commercial question answering system, two approaches are adopted for model inference. i) for head and body queries, large-scale batchmode processing is used to compute answers in offline. For this part, the number of QnA pairs is at the magnitude of 100 billions, ii) for tail queries, online inference is used and the latency requirement is about 10ms. Both approaches require fast model inference speed. Therefore, we have to perform model compression for inference speedup."
"In this paper, we address these challenges by proposing two novel approaches to pre-train documentlevel hierarchical representations. Both methods pre-train representations from unlabeled documents containing thousands of tokens. Our first approach generalizes the method of <REF> to pre-train hierarchical left-to-right and right-to-left document representations. To allow the hierarchical representations to learn to fuse left and right contextual information from abundant unlabeled text, our second approach extends the masked language model technique  to efficiently pre-train bidirectional hierarchical document-level representations."
"Transformer-based Autoencoder: One of the key points of our model is to build an autoencoder with low reconstruction bias. Inspired by the superiority of Transformer <REF> on many text generation tasks , we propose a Transformer-based autoencoder with low reconstruction bias to learn the latent representation of source text. We first pass source text x through the original Transformer's encoder (E transf ormer )  and get the intermediate representations U . Because the Transformer architecture is suboptimal for language model itself, neither self-attention nor positional embedding in the Transformer is able to effectively incorporate the word-level sequential context . So we add extra positional embeddings H  to U . Next we pass U through a GRU layer with self-attention to further utilize the sequence information. Then we apply a sigmoid activation function on the GRU hidden representations and sum them to get the final latent representation z ( ):"
"The autoregressive loss belongs to a large family of selfsupervised loss functions <REF>. There also exists some work on unsupervised speech representation learning . However, none of the studies are able to show the transferability of the learned representations across different datasets. Our work is largely motivated by the recent success in transfer learning from large-scale pre-trained language models , and we aim to learn general speech representations that can be transferred to different tasks across different datasets."
"The following recent work is aimed towards a better understanding of attention distributions and whether it can be used to explain a model. Raganato et al. <REF> study the self-attention of a Transformer encoder for NMT and observe that some heads mark syntactic dependency relations. Vig  visualizes BERT's  selfattention and finds patterns such as attention to the surrounding words, identical/related words, predictive words and delimiter tokens. Concurrent to our work, Voita et al.  perform a similar analysis of multi-headed attention in NMT and Michel et al.  for BERT . They find that heads specialize towards linguistically interpretable roles, but that a majority can be pruned after training without affecting performance. Jain and Wallace  observe that attention is commonly (implicitly or explicitly) claimed to provide insight into model dynamics. They argue that if attention is used as explanation, it should exhibit two properties: (1) attention should correlate with feature importance measures; and (2) adversarially crafted attention distributions should lead to different predictions, or be considered equally plausible explanations. Such an adversarial attention distribution should maximally differ from the learned attention, while the corresponding output distribution is constrained to be the same within a small range ϵ. With a Bi-RNN or CNN encoder it is possible to construct such adversarial distributions for NLP classification tasks such as binary text classification. They argue that attention heatmaps should thus not be so easily assumed to provide transparency for model predictions ."
"We first discuss empirical results for speeding up BERT pre-training. For this experiment, we use the same dataset as <REF>, which is a concatenation of Wikipedia and BooksCorpus with 2.5B and 800M words respectively. We specifically focus on the SQuAD task in this paper. Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset which contains questions posed by crowdworkers on a set of Wikipedia articles, the answer to which is a segment of text from the provided reading passage 1 . The F1 score on SQuAD-v1 is used as the accuracy metric in our experiments. All our comparisons are with respect to the baseline BERT model in . To train BERT,  first train the model for 900k iterations using sequence length of 128 and then switch to sequence length of 512 for the last 100k iterations. This results in a training time of around 3 days on 16 TPUv3 chips. The baseline BERT model 2 achieves a F1 score of 90.395. To ensure a fair comparison, we follow the same SQuAD fine-tune procedure of  without modifying any configuration (including number of epochs and hyperparameters). As noted earlier, we could get even better results by changing the fine-tune configuration. For instance, by just slightly changing the learning rate in the fine-tune stage, we can obtain a higher F1 score of 91.688 for the batch size of 16K using LAMB. We report a F1 score of 91.345 in , which is the score obtained for the untuned version. Below we describe two different training choices for training BERT using LAMB and discuss the corresponding speedups."
"Recently, several pre-trained transformers such as GPT <REF> and BERT  have been released. Compared to RNNs such as LSTMs  and GRUs , pretrained transformers capture rich world and linguistic knowledge from large-scale external corpora, and significant improvements are obtained by fine-tuning these pre-trained models on several downstream tasks. We follow this promising direction by fine-tuning GPT . Note that the pre-trained transformer in our pipeline can also be easily replaced by BERT."
"Model Architectures of BERT, GPT and ELMo Quoted from <REF> BERT As shown in , both ELMo and GPT models only use unidirectional language models to learn the representation of tokens. BERT  points out that this restriction has severely limited the efficiency of the pre-trained representation."
"We featurize each catalog item i by embedding it into a vector space x i ∈ R d . Seeker requires a high correlation between human perception of similarity and distance metric in the embedded vector space. Based on the properties of the items displayed, embedding strategies described in <REF> have been shown to correlate with human perception."
"Neural network approaches have achieved great success on sentence/document classification tasks, including CNN-based approaches <REF>,    : Distributions of (a) average spoiler sentence position; (b) the length of each spoiler span; (c) itemspecificity of non-spoiler and spoiler sentences (sample means and 95% confidence intervals); (d) DF-IIF of each term and top ranked item-specific terms for an example book; (e) the percentage of spoiler reviews per book/user. , and self-attention-based approaches . In this study, we cast the spoiler sentence detection task as a special sentence classification problem, but focus on modeling domain-specific language patterns. Contributions. To address real-world, large-scale application scenarios and to facilitate the possibility of adopting modern 'data-hungry' language models in this domain, we collect a new largescale book review dataset from goodreads.com. Spoiler tags in this dataset are self-reported by the review authors and are sentence-specific, which makes it an ideal platform for us to build supervised models. Motivated by the results from preliminary analysis on Goodreads, we propose a new model SpoilerNet for the spoiler sentence detection task. Using the new Goodreads dataset and an existing small-scale TV Tropes dataset , we demonstrate the effectiveness of the proposed techniques."
"Graph Neural Network (GNN): GNN is used to build the relationship between nodes like social network, citation link <REF>, knowledge graph , protein-protein interaction , etc.. It could overcome the limitation of Euclidean distance between each node in the inputs and involve more context information from neighbors. In textbased tasks, such as machine translation and sequence tagging, GNN breaks the sequence restriction between each word and learns the graph weight by attention mechanism , which makes it model longer sequence more easily than LSTM and gated recurrent neural network (GRU) , since each node is directly linked with others by learned weights instead of hidden state and gates. Moreover, the learned graph weight, which implies dependencies between nodes, can be easily explained and transferred to other tasks for pre-trained weights . In the image-based tasks, GNN gathers information from all grids  other than surroundings whose size is limited by the receptive fields of convolution neural networks (CNNs), and it can aggregate features over coordinate space to compute complex dependence . Besides modeling the relationship between homogeneous inputs, GNN can also work in the tasks of multimodal inputs such as VQA. The entities retrieved from external facts exchange information through the graph with multiple hops to predict the answer . And the graph enhances the interpretation of network to reason the relationship among detected objects in the image filtered by the question ."
"Motivated by the desire to address the limitations of supervised parsing and by the success of large-scale unsupervised modeling such as ELMo and BERT <REF> Equal contribution, randomly ordered."
"Pre-trained sentence encoders such as ELMo <REF> and BERT  have rapidly improved the state of the art on many NLP tasks, and seem poised to displace both static word embeddings  and discrete pipelines  as the basis for natural language processing systems. While this has been a boon for performance, it has come at the cost of interpretability, and it remains unclear whether such models are in fact learning the kind of abstractions that we intuitively believe are important for representing natural language, or are simply modeling complex co-occurrence statistics."
"We argue the relations between points are critical to represent a point cloud: a single point is non-informative without other points in the same set; in other words, it is simply represented by relations between other points. Inspired by the recent advances in NLP domain <REF>, we introduce Point Attention Transformers (PATs), based on self-attention to model the relations with powerful multi-head design . Combining with ideas of the light-weight but high-performance model, we propose a parameter-efficient Group Shuffle Attention (GSA) to replace the costly MultiHead Attention  with superior performance."
"Meanwhile, recent progress in training deep contextualized language models <REF> provides an opportunity to explore beyond extractive methods as an avenue for commonsense KB construction. These large-scale language models display impressive performance when their underlying representations are tuned to solve end tasks, achieving state-of-the-art results on a variety of complex problems. In this work, we define the COMmonsEnse Transformer (COMET ), which constructs commonsense KBs by using existing tuples as a seed set of knowledge on which to train. Using this seed set, a pre-trained language model learns to adapt its learned representations to knowledge generation, and produces novel tuples that are high quality."
"We propose a curriculum-based transfer learning approach that allows us to train a very competitive SLU end-to-end system from speech that gets state-of-the-art results. This approach can also be applied in order to train a model dedicated to a new slot filling task from an already pre-trained model (here ASR • N ER), in the same spirit as the BERT model for textual language understanding <REF>. We think we will outperform soon the current state-of-art approach by injecting external information. For instance, our current investigations on speaker adaptation and language transfer for the MEDIA task, not presented in this paper by lack of space, also provide very competitive and complementary results ."
"Artetxe and Schwenk (2018) present a 'language agnostic' sentence representation system learnt over machine translation; the agnosticism refers to the joint BPE vocabulary that they construct over all languages, giving their encoders no language information, whilst their decoders are told what language to generate. Similarly, <REF> present pretrained cross-lingual models (XLM), based on modern pretraining mechanisms; specifically, a variant of the masked LM pretraining scheme used in BERT ."
"Right ending: I was very proud of my performance. en ta il co nf lic t <REF>: This figure shows a typical example from the development set of Story Cloze Test. There is an obvious entailment relation between the story context and the right ending, and a contradiction relation between the context and the wrong ending. the human performance, demonstrating the hardness of this task. Until very recently,  and BERT  have shown that a two-stage framework -pre-training a language model on large-scale unsupervised corpora and fine-tuning on target tasks -can bring promising improvements to various natural language understanding tasks, such as reading comprehension  and natural language inference (NLI) . Benefiting from these advances, the SCT performance has been pushed to a new level , though there is still a gap with the human performance."
"The GLUE benchmark (Wang et al., 2019b) is a suite of language understanding tasks which has seen dramatic progress in the past year, with average performance moving from 70.0 at launch to 83.9, state of the art at the time of writing (May 24, 2019). Here, we measure human performance on the benchmark, in order to learn whether significant headroom remains for further progress. We provide a conservative estimate of human performance on the benchmark through crowdsourcing: Our annotators are non-experts who must learn each task from a brief set of instructions and 20 examples. In spite of limited training, these annotators robustly outperform the state of the art on six of the nine GLUE tasks and achieve an average score of 87.1. Given the fast pace of progress however, the headroom we observe is quite limited. To reproduce the datapoor setting that our annotators must learn in, we also train the BERT model <REF> in limited-data regimes, and conclude that low-resource sentence classification remains a challenge for modern neural network approaches to text understanding."
"In recent years, pre-trained word and sentence representations achieved very competitive performance in many NLP tasks, e.g., fine-tuned word embeddings using distant training (Cliche, 2017) and tweet sentence representations DeepMoji <REF> on sentiment analysis, and contextualized word representations BERT  on 11 NLP tasks. Motivated by these successes, in this task we explored different word and sentence representations. We then fed these representations into a Recurrent Convolutional Neural Network (RCNN)  for classification. RCNN includes a Long short-term memory (LSTM) network  to capture word ordering information and a max-pooling layer  to learn discriminative features. We also experimented LSTM and CNN in our preliminary analysis but achieved worse performance as compared to RCNN. Our final system adopted fine-tuned word embeddings and DeepMoji as our choices of word and sentence representations, respectively, due to their superior performance on the validation dataset. The code is publicly available at Github 1 ."
"We adopt BERT (Bidirectional Encoder Representation from Transformers <REF>) as our base model since its variants achieve dominant performance on MRC  and CRC  tasks. However, BERT is designed to learn features for a wide spectrum of NLP tasks 3 with a large amount of training examples 4 . The task-awareness of BERT can be hindered by the weak supervision of the (RC) 2 dataset. To resolve this challenge, we introduce a novel pre-tuning stage between pre-training and end-task fine-tuning for BERT. The pre-tuning stage is formulated in a similar fashion as the RCRC task but requires no annotated RCRC data and just domain QA pairs (from CQA) and reviews, which are readily available online . We bring certain characteristics of the RCRC task (inputs/outputs) to pre-tuning to encourage BERT's weight to be prepared for understanding the current question and locate the answer if there exists one. The proposed pre-tuning step is general and can potentially be used in MRC or CRC tasks in other domains."
"In recent years, there has been an increase in the number of annotated RC datasets such as SQuAD <REF>, NewsQA , TriviaQA  and RACE 8 Early testing of our model was actually done on SQuAD. However, since taking part on the heavily contested public leaderboard requires more computational resources than we could muster, we decided to focus on other datasets. In lieu of reviewer requests, we include preliminary results of our model on SQuAD dev set.  For fairer comparison, we make several enhancements to the R-NET model as follows: (1) We replaced the additive attention with scaled dot-product attention similar to ours. (2) We added shortcut connections after the encoder layer. (3) We replaced the original Pointer networks with our BiRNN Pointer Layer. We found that these enhancements consistently lead to improved performance. The original R-NET performs at ≈ 2% lower on NewsQA. . Spurred on by the avaliability of data, many neural models have also been proposed to tackle these challenges. These models include BiDAF , Match-LSTM , DCN/DCN+ , R-NET , DrQA , AoA Reader , Reinforced Mnemonic Reader , ReasoNet , AMANDA , R 3 Reinforced Reader Ranker  and QANet . Many of these models innovate at either (1) the bidirectional attention layer (BiDAF, DCN), (2) invoking multi-hop reasoning (Mnemonic Reader, ReasoNet), (3) reinforcement learning (R 3 , DCN+), (4) self-attention (AMANDA, R-NET, QANet) and finally, (5) improvements at the encoder level (QANet). While not specifically targeted at reading comprehension, a multitude of pretraining schemes  have recently proven to be very effective for language understanding tasks."
"However, data-driven end-to-end empathetic chatbot currently suffers from two limitations: 1). Model capacity. 2). The paucity of data for both emotion recognition and empathetic response generation <REF>. Thanks to the recent success of large pre-trained language models , both problems can be mitigated."
"Since we assign each token a label based on its representation (embedding) and a token might have different meanings among different contexts, there is a natural demand that we should represent a token by taking its context into account. Here, it is achieved with contextual embedding methods <REF> that embed each token based on both the token itself and its context. The Contextual Embedding component here takes a sequence of tokens x = (x 1 , x 2 , . . . , x n ) as input, and output an embedding matrix E n×h , where h is the embedding size. The i th row in this embedding matrix is the contextual word embedding of the i th token in x."
"We approached this task as an opportunity to test the effectiveness of transfer learning and semisupervised learning techniques. In Subtask A, the high class imbalance and relatively smaller size of the training data made it an ideal setup for evaluating the efficacy of recent transfer learning techniques. Using pre-trained language models for contextual word representations has been shown to improve many Natural Language Processing (NLP) tasks <REF>. This transfer learning technique is also an effective method when less labelled data is available as shown in . In this work, we use the BERT model  for obtaining contextual representations. This results in enhanced scores even for simple baseline classifiers."
• Fine-tuning on in-domain data of sentence encoder <REF>.
"Gains on additional tasks were reported by <REF> and later by other researchers.  introduced a similar approach, ULMFiT, showing a benefit for text classification methods. A successor approach, bidirectional encoder representations from transformers (BERT; ) that introduced several innovations to the learning method and learned from more data, achieved a further 45% error reduction (relative to ELMo) on the first task and 7% on the second. On the SWAG benchmark, recently introduced to test grounded commonsense reasoning ,  found that ELMo gave 5% relative error reduction compared to non-contextual word vectors, and BERT gave another 66% relative to ELMo. At this writing, there are many open questions about the relative performance of the different methods. A full explanation of the differences in the learning algorithms, particularly the neural network architectures, is out of scope for this introduction, but it's fair to say that the space of possible learners for contextual word vectors has not yet been fully explored; see  for some exploration. Some of the findings on BERT suggest that the role of finetuning may be critical. While ELMo is derived from language modeling, the modeling problem solved by BERT (i.e., the objective function minimized during estimation) is rather different.  The effects of the dataset used to learn the language model have not been fully assessed, except for the unsurprising pattern that larger datasets tend to offer more benefit."
"Inspired by <REF> which capture the global dependencies between input and output by aggregating information from the elements of the input, we build a relation attention module which consists of the transformer unit proposed in . The relation attention module captures the global information in parallel, which is more efficient than the above-mentioned strategies. Specifically, following BERT , the architecture of our relation attention module is a multi-layer bidirectional transformer encoder. We present it in the supplementary material due to the page limit."
"Fine-Tuning BERT This is another extractive RC model that benefits from the recent advance in pretrained general language encoders <REF>. In our work, we select the BERT model  which has achieved the best performance on SQuAD."
"Under the framework, we design a position embedding mechanism based on representation learning to capture the position information. Our position embedding is inspired by word position embedding in neural language processing (NLP) which distinguishes different semantics of one word in different positions of a sentence <REF>. All positions are embedded into a vector space. Embedding vectors indicate features of each position and they are learned with the prediction model. Different from the usage in NLP, we use position embedding not only in the input layer but also in convolutional layers to modify the representation of input features. Due to the utilization of position information, PAN can provide different prediction patterns for different positions with less information."
"Vector embedding of sub-symbolic relations is a powerful concept often applied in natural language processing (NLP). Vector embeddings of words, sentences and contexts enable mappings between words in terms of vector operations in an ndimensional space, where typically n > 100. Initially, relatively simple vector space models <REF> were used and can represent some important word and document relations . Lately, neural network based approaches like Word2Vec  have shown great performance, and thereby the use of simpler embeddings like one-hot vectors have mostly disappeared. Word2Vec maps the words (symbols) to a manifold in a vector space, thereby creating model with sub-symbolic representations. Recent advancements have been achieved using attention models  to create embeddings that produce different mappings for the same word given different contexts . Sub-symbolic vector embeddings of this type has recently been used for ontology alignment purposes ."
"We normalize all of the scores using softmax, and train to minimize the cross-entropy loss.  Pre-trained LM To take advantage of the recent success in pre-trained language models <REF> we also make use of ELMo contextualized embeddings instead of the embedding matrix and the character LSTM concatentation."
"In modern search systems, the above target could be achieved by deploying a Web question answering (QA) component <REF> upon the retrieval module: Given a search query and a set of topranked passages, the Web QA component determines whether the candidate set contains any direct answer and extracts the answer as the output if it exists. Without loss of generality, the Web QA component could be implemented by applying machine reading comprehension (MRC) models  over the retrieved passages. In recent years, with the development of deep learning techniques, state-of-the-art MRC performances have been achieved by a variety of deep MRC models ."
"Extreme summarization revisits interesting problems in abstractive summarization with the relatively simpler objective of generating single sentence summaries rather than multiline summaries <REF>. Models trained for extreme summarization require document-level inference, abstraction, and paraphrasing to generate summaries which are informative and consistent with the input document. Throughout this paper we have argued that our model is better suited for this task than recurrent abstractive models due to its ability to foreground pertinent content using topic vectors and model long-range dependencies using a multi-layer convolutional architecture. In the future, we would like to create more linguistically-aware encoders and decoders incorporating co-reference and entity linking. It would also be interesting to use contextualised word representations  to enhance modeling of long-range dependencies within our model."
"To find out what causes the main issues for the models, compared to the human wizard, we analysed only cases where models had to make forced decisions. We manually checked video recordings of the interactions and discovered that the majority of such cases contained utterances such as ""to the left"", ""in the corner"", ""he same as the previous one"" A human is able to infer much more information from such phrases, while our speech system is only corpus-based. This bottleneck can be addressed by implementing a more sophisticated natural language recognition system, for instance, BERT <REF>."
"Given a headline decomposition D 1 ...D p and a T ext Initialize Samples to empty list Initialize Question to empty string for i from 1 to p: Answer = D i Add (Question, T ext, Answer) to Samples Update Question to Question + Answer Add (Question, T ext, _) to Samples The tuples (Question, T ext, Answer) are then used for training a traditional question-answer model. We used the BERT transformer base uncased model <REF> adapted to question-answering by the Huggingface team [13]."
"In computer vision, a shortage of labeled data for a particular task is offset by the widespread availability of models trained on the ImageNet dataset <REF>: To distinguish a thousand object categories, these models learned transformations of the raw input images that form a good starting point for many other vision tasks. Similarly, in neural language processing, word prediction models trained on large text corpora have shown to yield good model initializations for other language processing tasks , . However, no comparable task and dataset -and models pretrained on it -exists for the audio domain."
"Related work. ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which collects more than 1.2 million varied images, has inspired the development of a series of deep learning models <REF>.The last hidden layer signals of deep models are widely used as image feature embeddings for transfer learning ; the upper-layers of deep models can also be fine-tuned for different tasks. Natural language processing benefits largely from the introduction of word2vec  and its extension to sentences, documents, and sub-word information . In addition to ImageNet, there are various datasets which contribute significantly to model development and real-world applications ."
"It is therefore surprising that BERT <REF> achieves 77% test set accuracy with its best run   : Baselines and BERT results. Our results come from 20 different random seeds (± gives the standard deviation). The mean for BERT Large is skewed by the 5/20 random seeds for which it failed to train, a problem noted by . We therefore consider the median a better measure of BERT's average performance. The mean of the non-degenerate runs for BERT (Large) is 0.716 ± 0.04. To investigate BERT's decision making we looked at data points it finds easy to classify over multiple runs.  performed a similar analysis with the SemEval submissions, and consistent with their results we found that BERT exploits the presence of cue words in the warrant, especially ""not."" Through probing experiments designed to isolate such effects, we demonstrate in this work that BERT's surprising performance can be entirely accounted for in terms of exploiting spurious statistical cues."
"Most NN-based dialog act recognition approaches perform tokenization at the wordlevel and use uncontextualized pre-trained embeddings to represent the tokens. These word embeddings are typically trained on large corpora using embedding approaches which capture information concerning words that commonly appear together <REF>. However, they are unable to provide information concerning word function and segment structure, which is relevant for the identification of many dialog acts. Furthermore, in these models the representation of a word is the same for all its occurrences. Since the same word may have different meanings according to the context surrounding it, the performance on many NLP tasks has been improved by using embedding approaches that generate contextualized word representations that are able to capture wordsense information . Additionally, in previous studies, we have shown that there are cues for intention at a sub-word level, both in lemmas and affixes, which cannot be captured using word-level tokenization  and require a complementary character-based approach to be captured. Finally, there are also cues for intention in word abstractions, such as syntactic units, especially when considering dialog acts that are related to the structure of the segment. However, in spite of having been used on other NLP tasks, these abstractions have not been explored in previous studies on automatic dialog act recognition."
"Deep language representations, e.g. those learned by the Transformer <REF> via language modeling , have been shown to implicitly capture useful semantic and syntactic properties of text solely by unsupervised pre-training , as demonstrated by state-of-the-art performance on a wide range of natural language processing tasks , including supervised relation extraction .  even found language models to perform fairly well on answering open-domain questions without being trained on the actual task, suggesting they capture a limited amount of ""common-sense"" knowledge. We hypothesize that pre-trained language models provide a stronger signal for distant supervision, better guiding relation extraction based on the knowledge acquired during unsupervised pre-training. Replacing explicit linguistic and side-information with implicit features improves domain and language independence and could increase the diversity of the recognized relations."
"One of the primary drivers of the success of machine learning methods in open-world perception settings, such as computer vision <REF> and NLP , has been the ability of high-capacity function approximators, such as deep neural networks, to learn generalizable models from large amounts of data. Reinforcement learning (RL) has proven comparatively difficult to scale to unstructured realworld settings because most RL algorithms require active data collection. As a result, RL algorithms can learn complex behaviors in simulation, where data collection is straightforward, but real-world performance is limited by the expense of active data collection. In some domains, such as autonomous driving  and recommender systems , previously collected datasets are plentiful. Algorithms that can utilize such datasets effectively would not only make real-world RL more practical, but also would enable substantially better generalization by incorporating diverse prior experience."
"Scientific articles are the primary source of knowledge for biomedical entities and their relations. These entities include human phenotypes, genes, proteins, chemicals, diseases, and other biomedical entities inserted in specific domains. A comprehensive source for articles on this topic is the PubMed <REF> platform, combining over 29 million citations while providing access to their metadata. Processing this volume of information is only feasible by using text mining solutions. Automatic methods for Information Extraction (IE) aim at obtaining useful information from large data-sets . Text mining uses IE methods to process text documents. Text mining systems usually include Named-Entity Recognition (NER), Named-Entity Linking (NEL), and Relation Extraction (RE) tasks. NER consists of recognizing entities mentioned in the text by identifying the offset of its first and last character. NEL consists of mapping the recognized entities to entries in a given knowledge base. RE consists of identifying relations between the entities mentioned in a given document. RE can be performed by different methods, namely, by order of complexity, co-occurrence, pattern-based (manually and automatically created), rulebased (manually and automatically created), and machine learning (featurebased, kernel-based, and recurrent neural networks (RNN)). In recent years, deep learning techniques, such as RNN, have proved to achieve outstanding results at various Natural Language Processing (NLP) tasks, among them RE. The success of deep learning for biomedical NLP is in part due to the development of word vector models like Word2Vec , and, more recently, ELMo , BERT , GPT , Transformer-XL , and GPT-2 . These models learn word vector representations that capture the syntactic and semantic word relationships and are known as word embeddings. Long Short-Term Memory (LSTM) RNN constitute a variant of artificial neural networks presented as an alternative to regular RNN . LSTM networks deal with more complex sentences, making them more fitting for biomedical literature. To improve their results in a given domain, it is possible to integrate external sources of knowledge such as domainspecific ontologies."
"On the other side, deep neural networks have achieved impressive performance on a lot of tasks such as image classification <REF> and machine translation . Researchers start to solve the NL2SQL problem by deep neural networks. BERT , or Bidirectional Encoder Representations from Transformers , is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks. We employ BERT for the representation layer."
"Attention Mechanisms. Besides those mentioned above, other interesting attention mechanisms include performing multi-round alignment to avoid the problems of attention redundancy and attention deficiency , and using mutual attention as a skip-connector to densely connect pairwise layers <REF>  , which are based on certain word-level or sentence-level models pretrained on large external corpora in certain supervised or unsupervised manners."
"(MRPC) <REF>, where the task is to identify whether or not a pair of sentences have the same meaning ( ). We fine-tune the BERT BASE model as described in , except without weight decay and using SGD with a learning rate of 0.01 instead of the Adam optimizer. Our implementation is adapted from . Experiments were run with importance weights of inverse powers of 4 for each class, including with no importance weighting. All classifiers achieved test accuracies between 78% and 85%."
"We empirically investigate SAN for CWS by building a SAN-CRF word segmentor, studying the influence of global and local attention for segmentation accuracy. Based on the SAN-CRF segmentation model, we investigate two further questions. First, in Chinese, characters are highly polysemantic and the same character can have different meanings in different context. SAN has also been shown a useful method for training contextualized word representations <REF>. We compare context-independent character representations  with contextualized character representations in both in-domain and crossdomain CWS evaluation."
"Similar strategies have also achieved good performance in other medical problems with different types of medical images <REF>. In addition to computer vision, powerful natural language processing models such as transformer  and BERT  with parameters trained on general natural language data, have also been fine-tuned to analyze unstructured medical data . Because these models are pre-trained on general data, they can only encode some general knowledge, which is not specific to medical problems. Moreover, such models are only available with certain complicated architectures with a huge amount of general training data. It is difficult to judge how and why such a mechanism will be effective in which clinical scenarios."
"Transformers <REF> have shown state of the art performance across a variety of NLP tasks, including, but not limited to, machine translation , question answering , text classification , and semantic role labeling . Central to its architectural improvements, the Transformer extends the standard attention mechanism  via multi-headed attention (MHA), where attention is computed independently by N h parallel attention mechanisms (heads). It has been shown that beyond improving performance, MHA can help with subject-verb agreement  and that some heads are predictive of dependency structures . Since then, several extensions to the general methodology have been proposed . However, it is still not entirely clear: what do the multiple heads in these models buy us? In this paper, we make the surprising observation that -in both Transformer-based models for machine translation and BERT-based  natural language inference -most attention heads can be individually removed after training without any significant downside in terms of test performance ( §3.2). Remarkably, many attention layers can even be reduced to a single attention head without impacting test performance ( §3.3)."
"One major limitation of almost all taxonomy learning system is that they do not distinguish between words and concepts. In general, however, a many-to-many relationship holds between them. For example, the word ""Venus"" may either refer to the concept of a particular planet or to a Roman goddess, and ""morning star"" can either refer to the same planet or to a type of weapon. Instead, most automatically constructed taxonomies conflate the different senses of a word and typically only learn the most predominant one. This limitation is reinforced by context-free word representations 1 (Turney and Pantel 2010; <REF>: methods that encode each word as one point in a vector space of meaning and are thereby unable to account for multiple senses of a word. Such representations are widely used in taxonomy learning and many other disciplines of natural language processing (NLP). In 2018, an emerging trend in NLP have been task-independent deep neural network architectures based on language model pre-training, which have achieved state-of-the-art results in a number of competitive disciplines, such as questions answering or natural language inference ). One quality that is common to all of these systems is that they allow for contextualized word representation: depending on their contexts, occurrences of the same word can be mapped to the same, similar or very different points in the vector space."
"One way to construct a questionanswering system over database is leveraging the semantic parsing. Semantic parsing is a task that transform the natural language to logic form which computer can execute. Transforming from natural language to SQL (NL2SQL) is kind of semantic parsing task. The generated SQL can be executed in the database system to can the answer from the database. In recent years, deep learning techniques <REF> is applied to semantic parsing  [8] . But deep learning need large amount of labeled data, when the parameter number is very large. The logic form of natural language is also very hard to label, compared to other natural language processing (NLP) tasks such as text classification or sentence similarity. There are some works  solve the hard labeling problem in weak supervision methods. Our work try to solve this problem in another way. The upper level view of the problem of semantic parsing for question answering is retrieve the answer from the database when given a question. Human can solve this problem even without an explicit logic form. Human can read the schema or columns' info in the database and answer the question. We think the deep model can integrate the logic or reasoning modules like  or other deep model to search on the database without an explicit logic form. We present our idea in    We use BERT-based  to implement our idea. BERT is a new language representation model, which stands for Bidirectional Encoder Representations from Transformers. Pre-trained word representations on a large (unlabeled) language corpus, such as  , have shown promising results in a lot of NLP tasks. BERT is also a pre-trained deep model [12]  which use large amount of plain text to pre-train. As a result, the pre-trained BERT representations can be fine-tuned with just one additional output layer to create stateof-the-art models for a wide range of tasks, such as question answering and text classification."
"github.com/ jayadevbhaskaran/gendered-sentiment representations as building blocks for NLP tasks. The rise of this paradigm is characterized by the use of language models for pretraining, exemplified by models such as ELMo <REF>, ULMFit , GPT (Radford, 2018), and BERT ."
"In this study, we project the discrete binary representation of SGAs perturbing a gene into a continuous embedding space, which we call ""gene embeddings"" of corresponding SGAs. While such embeddings can be directly learned using the GIT model, it has been shown in the field of NLP that the pre-trained word embeddings can significantly improve the performance in other related NLP tasks <REF> .. .  2018). Such pre-trained word embeddings can capture the knowledge of co-occurrence pattern of the words in languages and exhibit sound semantic properties: words of similar semantic meanings are close in embedding space, e.g., e ""each"" ≈ e ""every"" . We therefore propose an algorithm called ""Gene2Vec"" to pre-train the gene embeddings, which is closely related the skip gram word2vec  pre-training algorithm. The biology rationale behind Gene2Vec algorithm is that we are able to portrait the co-occurrence pattern of SGAs in each tumor, i.e., mutually exclusive mutations , using gene embeddings and gene context embeddings."
"In the natural language processing (NLP) literature, the march of the neural networks has been an unending yet predictable one, with new architectures constantly surpassing previous ones in not only performance and supposed insight but also complexity and depth. In the midst of all this neural progress, it becomes easy to dismiss earlier, ""first-generation"" neural networks as obsolete. Ostensibly, this appears to be true: <REF> show that using pretrained deep word representations achieves state of the art on a variety of tasks. Recently,  have pushed this line of work even further with bidirectional encoder representations from transformers (BERT), deeper models that greatly improve * Equal contribution. Ordering decided by coin toss. state of the art on more tasks. More recently, OpenAI has described GPT-2, a state-of-the-art, larger transformer model trained on even more data.  Such large neural networks are, however, problematic in practice. Due to the large number of parameters, BERT and GPT-2, for example, are undeployable in resource-restricted systems such as mobile devices. They may be inapplicable in realtime systems either, because of low inference-time efficiency. Furthermore, the continued slowdown of Moore's Law and Dennard scaling  suggests that there exists a point in time when we must compress our models and carefully evaluate our choice of the neural architecture."
"This technical note describes a new baseline for the Natural Questions <REF>. Our model is based on BERT  and reduces the gap between the model F1 scores reported in the original dataset paper and the human upper bound by 30% and 50% relative for the long and short answer tasks respectively. This baseline has been submitted to the official NQ leaderboard † . Code, preprocessed data and pretrained model are available ‡ ."
"4. Poetry Artistry. We observe that the model have a fair chance to generate high quality poems that express the poetry themes artistically, which is close to one written by specialized poets. <REF> shows four poems, among which only one was written by a Chinese poet more than one thousand years ago, while the remaining three poems are generated by our system. We refer the readers to the blog 2 or the papers  to get a basic understanding of Transformer, which is the underlying model of the proposed method.  depicts the process of training the poetry generation model."
"Using this general recipe, further datasets can be created with other combinations, for example pairing sets of region descriptions with further descriptions either from the same or from a different scene, or for the task of predicting the number of distinct entities introduced by a sequence of region descriptions. For reasons of space, we do not show examples here. Entailment tasks, triggered by the aforementioned ""natural language inference"" dataset <REF> have in recent years become a staple NLP task. They are typically tackled with very high-capacity machine learning models that classify distributed representations of the candidate relata, e.g., as in . With the perspective developed here, we can liken such approaches to the syntactic way of defining entailment ( ), in that these approaches only take the surface form into account (and implicitly learn and use the required common sense knowledge)."
"This framework is inspired by recent success stories such as BERT <REF>. To validate this hypothesis we start by simply breaking the original paper's single-stage training approach to the aforementioned two-stage approach. However, we only use recombinant examples for the pre-train stage and use the original, non-augmented training examples for fine-tuning. This framework allows for a high level of flexibility on the data, augmentation strategies, and objective functions that can be used for each stage as summarized in . For instance we can leverage unlabeled data and train the encoder and decoder as independent Language Models (LMs) as in . In addition, we can use less precise recombinant strategies such as the co-occurrence strategy that would've been detrimental in the previous formulation. The baseline for this work is the parsing and token accuracy reported for three standard semantic parsing datasets in . Parsing accuracy is defined as the proportion of the predicted logical forms that exactly match the true logical form ."
"For our task, we wanted to choose a set of five distinct personality types. Let the set of utterances that belong to each personality type be U p = {u 1 p , . . . , u n p } where p ∈ {1, . . . , 215}. We first calculate the pooled BERT representation <REF> of each of the utterances. To get the representation of the personality P, we simply average the BERT representations of all the utterances that belong to that personality. The representation of each personality is given by:"
"Our results contain a few key findings. The first is that knowing the relevant claims is critical to obtaining stateof-the-art performance; even knowing only oracle claims is sufficient to perform better than all of the other baselines, although there is a still a large improvement when context is additionally provided. However, model-based approaches for claim selection do not seem to help: hard claim selection using the neural ranker performs just as well as the vanilla models, and our proposed approach for end-to-end claim selection has a negative impact. This motivates the need for more effective methods of claim selection. The decreasing performances of the BiLSTM seq2seq models by the increasing target post-modifier and sentence lengths show the difficulty of generating long texts and handling long input data. Finally, we observe that the transformerbased seq2seq models are not particularly wellsuited to this task. In all cases their performance is inferior to the BiLSTM-based approaches. Largescale, pre-trained transformer-based language models, such as GPT-2 <REF> and BERT , might be an interesting addition to the baselines, by framing the task as filling in the blanks for post-modifiers. However, when restricted to approaches that only use our dataset for training, we expect those based on language models to struggle due to the separation of entities among train, validation, and test."
"As the use of Transformers <REF> has become ubiquitous in recent NLP researches  , we have incorporated this model to our action generation task using our modified network structure. The structure of the generator is shown in . To make the paper to be self-contained, first we describe two main components in generator: Multi-Head Attention (MHA) layer and Point-Wise Fully Connected Feed-Forward (PFCF) layer."
"Input embedding layer maps each word to a high dimensional vector space. We employ the pretrained embedding matrix GloVe <REF> and pretrained model BERT  to obtain the fixed word embedding of each word. Then each word will be represented by an embedding vector e t ∈ R d emb ×1 , where d emb is the dimension of word vectors. After embedding layer, the context embedding is denoted as a matrix E c ∈ R d emb ×N , and the i-th aspect embedding is denoted as a matrix E a i ∈ R d emb ×M i ."
"Main-stream MRC models <REF>) extract text spans in passages given queries. Text span extraction can be simplified to two multi-class classification tasks, i.e., predicting the starting and the ending positions of the answer. Similar strategy can be extended to multi-passage MRC  where the answer needs to be selected from multiple passages. Multipassage MRC tasks can be easily simplified to single-passage MRC tasks by concatenating passages .  first rank the passages and then run single-passage MRC on the selected passage.  train the passage ranking model jointly with the reading comprehension model. Pretraining methods like BERT  or  have proved to be extremely helpful in MRC tasks."
"Humans rate the disinformation generated by Grover as trustworthy, even more so than humanwritten disinformation. Thus, developing robust verification techniques against generators such as Grover is an important research area. We consider a setting in which a discriminator has access to 5000 Grover generations, but unlimited access to real news. In this setting, the best existing fake news discriminators are, themselves, deep pretrained language models (73% accuracy) <REF>. However, we find that Grover, when used in a discriminative setting, performs even better at 92% accuracy. This seemingly counterintuitive finding represents an exciting opportunity for defense against neural fake news: the best models for generating neural disinformation are also the best models at detecting it."
"Advances in techniques and hardware for training deep neural networks have recently enabled impressive accuracy improvements across many fundamental NLP tasks <REF>, with the most computationally-hungry models obtaining the highest scores . As a result, training a state-of-the-art model now requires substantial computational resources which demand considerable energy, along with the associated financial and environmental costs. Research and development of new models multiplies these costs by thousands of times by requiring retraining to experiment with model architectures and hyperparameters. Whereas a decade ago most  NLP models could be trained and developed on a commodity laptop or server, many now require multiple instances of specialized hardware such as GPUs or TPUs, therefore limiting access to these highly accurate models on the basis of finances. Even when these expensive computational resources are available, model training also incurs a substantial cost to the environment due to the energy required to power this hardware for weeks or months at a time. Though some of this energy may come from renewable or carbon credit-offset resources, the high energy demands of these models are still a concern since (1) energy is not currently derived from carbon-neural sources in many locations, and (2) when renewable energy is available, it is still limited to the equipment we have to produce and store it, and energy spent training a neural network might better be allocated to heating a family's home. It is estimated that we must cut carbon emissions by half over the next decade to deter escalating rates of natural disaster, and based on the estimated CO 2 emissions listed in , model training and development likely make up a substantial portion of the greenhouse gas emissions attributed to many NLP researchers."
"NLP has seen an upheaval in the last year, with contextual word embeddings, such as ELMo <REF> and BERT , setting state-of-the-art performance on many tasks. These empirical successes suggest that unsupervised pre-training from large corpora could be a vital part of NLP models. In specific domains like biomedicine, NLP datasets are much smaller than their general-domain counterparts 1 , which leads to a lot of ad-hoc models: some infer through knowledge bases , while others leverage large-scale general domain datasets for domain adaptation . However, unlabeled biomedical texts are abundant, and their full potential has perhaps not yet been fully realized."
"While effective for straightforward conditional generation, such an approach is inflexible and cannot readily be applied to other inference tasks such as non-left-to-right generation or infilling. In this work, we present a more general approach called Kontextuell Encoder Representations Made by Insertion Transformations, or KERMIT for short. KERMIT is a simple architecture that directly models the joint distribution p(x, y) and its decompositions (such as the marginals p(x) and p(y) courses proved popular <REF> Die sehr beliebt  The quite {Kurse, waren} : An example of the KERMIT insertion objective for the English ↔ German translation pair ""The courses proved quite popular"" ↔ ""Die Kurse waren sehr beliebt"". The model is trained to predict the set of words that need to be inserted at each location. By incurring a loss on both sides, our system learns a fully generative model of the joint distribution over (x, y) pairs, and can accommodate arbitrary generation orders.   and the conditionals p(y | x) and p(x | y)) in a unified manner. In contrast with traditional seq2seq models, KERMIT does not rely on a prespecified factorization, but is instead able to condition on whatever information is available and infer what remains."
"Since comparing two texts-a mention in context and a candidate entity description-is a task similar to reading comprehension and natural language inference tasks, we use an architecture based on a deep Transformer <REF> which has achieved state-of-the-art performance on such tasks ."
"GPUs are widely used for many high-memory-footprint applications, including those for High Performance Computing (HPC) and Deep Learning (DL). HPC applications like weather prediction and the modeling of fluid and molecular dynamics have grown to require very large models <REF>. DL networks are also developing in a direction where either the model sizes are too big to run on GPUs, or they are large enough such that the only a small batch size can fit on the GPU, resulting in low utilization, and accuracy issues . Today, applications with large memory footprints must: (i) scale out to many GPUs for capacity purposes (inefficient resource utilization) , (ii) explicitly orchestrate data movement between the host CPU and the GPU to stay within device memory limitations (adding algorithmic complexity) , or (iii) rely on off-GPU memory accesses or Unified Memory  to automatically oversubscribe device memory (limiting performance) . In this paper, we explore memory compression as a solution to this challenge."
"Recent work has shown mounting evidence that pretraining sentence encoder neural networks on unsupervised tasks like language modeling, and then fine-tuning them on individual target tasks, can yield significantly better target task performance than could be achieved using target task training data alone <REF>. Largescale unsupervised pretraining in works like these seems to produce sentence encoders with substantial knowledge of the target language (which, so far, is generally English). These works have shown that the one-size-fits-all approach of finetuning a large pretrained model with a thin output layer for a given task can achieve results as good or * Equal contribution.  As of 02/24/2019. better than carefully-designed task-specific models without such pretraining. However, it is not obvious that the model parameters obtained during unsupervised pretraining should be ideally suited to supporting this kind of transfer learning. Especially when only a small amount of training data is available for the target task, fine-tuning experiments are potentially brittle, and rely on the pretrained encoder parameters to be reasonably close to an ideal setting for the target task. During target task training, the encoder must learn and adapt enough to be able to solve the target task-potentially involving a very different input distribution and output label space than was seen in pretraining-but it must also avoid overfitting or catastrophic forgetting of what was learned during pretraining."
"In this work, we conduct a thorough empirical analysis of generalization and transfer across 10 RC benchmarks. We train models on one or more source RC datasets, and then evaluate their performance on a target test set, either without any additional target training examples (generalization) or with additional target examples (transfer). We experiment with DOCQA , a standard and popular RC model, as well as a model based on BERT <REF>, which provides powerful contextual representations."
"Deep neural network models have increased in popularity in the field of NLP. They have pushed the state of the art of text representation and information retrieval. More specifically, these techniques enhanced NLP algorithms through the use of contextualized text embeddings at word, sentence, and paragraph levels <REF>."
"With deep learning's unprecedented success in fields such as image classification <REF>, language understanding , and multimodal learning , researchers have now begun to apply deep learning to facilitate scientific discovery in fields such as physics , biology , chemistry , and healthcare . However, one important challenge for applications of deep learning to these natural science problems comes from the stringent requirement for the assessment of uncertainty. Characterizing uncertainty of model predictions is now an active area of research , and being able to assess model uncertainty allows us to handpick difficult cases and treat them separately for better performance  (e.g., by passing to a human expert). Moreover, knowing uncertainty is important for fundamental machine learning research ; for example, many reinforcement learning algorithms (such as Thompson sampling ) requires estimating uncertainty of the distribution ."
"We primarily focus on detecting whether a tweet contains offensive content or not (subtask A), and then determining the target of the offensive content (subtask C). For subtask A, we use pre-trained word embeddings by fine-tuning the BERT model <REF> for detecting offensive tweets. For subtasks B and C, BERT did not perform well, either because of limited training data or because we did not find the appropriate hyperparameters. Thus we use an SVM classifier with character n-grams as features. We accidentally flipped the predicted labels in our submission to subtask B, which is why we do not report results of subtask B here. Among all teams participating in OffensEval, our models ranks 3rd out of 103 on subtask A and 27th out of 65 on subtask C. (see ."
"We evaluate our parsing model on English Penn Treebank (PTB) and Chinese Penn Treebank (CTB), using unlabeled attachment scores (UAS) and labeled attachment scores (LAS) as the metrics. Punctuations are ignored as in previous work <REF>. Pre-trained word embeddings and language model have been shown useful in a lot of tasks. Therefore, we also add the latest ELMo  and BERT  pre-trained language model layer features to enhance our representation."
"Machine comprehension of written passages has made tremendous progress recently. Large quantities of supervised training data for reading comprehension (e.g. SQuAD <REF>), the wide adoption and intense experimentation of neural modeling , and the advancements in vector representations of word embeddings  all contribute significantly to the achievements obtained so far. The first factor, the availability of large scale datasets, empowers the latter two factors. To date, there is still very limited well-annotated large-scale data suitable for modeling human-human spoken dialogues. Therefore, it is not straightforward to directly port over the recent endeavors in reading comprehension to dialogue comprehension tasks."
"In the Gendered Pronoun Resolution challenge which is based on GAP dataset, we designed a unique augmentation strategy for token-level contextual embedding models and applied it to feature based BERT <REF> approach for a 7th place finish. BERT is a large bidirectional transformer trained with masked language  The code is available at https://github.com/boliu61/gendered-pronoun-resolution model, which is fine-tuned to state-of-the-art results on a variety of NLP benchmark tasks. Four version of BERT model weights were released in October 2018, following a family of NLP transfer learning models in the same year, ELMo , ULMFit  and OpenAI GPT ."
"Word-prediction Objective: The objective pushes the system to make better predictions of words in a given sentence. As the nature of the objective is to predict words, these are also called generative models. In one of the two classes of models of this type, an encoder-decoder model is learnt using a corpus of contiguous sentences <REF> to make predictions of the words in the next sentence given the words in the current one. After training, the decoder is usually discarded as it is only needed during training and is not designed to produce sentence representations. In the other class of models of this type, a large language model is learnt  on unlabelled corpora, which could be an autoregressive model or a masked language model, which gives extremely powerful language encoders but requires massive computing resources and training time."
"The prerequisite of our system is to transform utterance and body gesture into numerical features. Speech recognition algorithms <REF>,  convert utterance to text, which can be encoded to a vector sequence using word embedding algorithms , , , , . As for body gesture, the models proposed in , ,  extract 2D coordinates of body keypoints and transform them into 3D space. However, the coordinate representation includes much noise, so we develop keypoints rotation and normalization methods in the gesture parsing module to discard irrelevant information. To demonstrate the prediction of our system, the body gestures generated by the listening and speaking models are reconstructed on avatar or robot by motion synthesis module."
"Recently, models such as the Convolutional Neural Network (CNN) <REF>, the Recurrent Neural Network (RNN) , memory network  have been introduced for sentiment classification tasks. These methods are endowed with powerful computation ability, capable of learning high-level features directly from low-level continuous representations. Apart from diverse models, language model pre-training has attracted wide attention and fine-tuning on pre-trained language model has shown to be effective for improving many downstream natural language processing tasks. BERT  obtained new state-of-the-art results on a broad range of diverse tasks by introducing deep masked language model. Such pre-trained model can be used as sentiment classifier, transferring pre-learned knowledge to current problem."
"We have presented CLEARumor, our architecture for the RumourEval 2019 shared tasks. In future we aim to generalize our approach, e.g., we currently use domain-specific features for characterizing the post author popularity, such as number of followers for Twitter, which are not available for all social media platforms. Besides investigating how well our approach translates to other languages, we are interested in studying the results for other pretrained word representation approaches, e.g., BERT <REF>."
"Conventional seq2seq models for NTTS rely on a single encoder for linguistic inputs (phonemes/character embeddings). This encoder cannot be solely relied upon to capture higher-level text characteristics like syntax or semantics. The relation between syntax, semantics and prosody is complex. Many linguistic theories try to tie these phenomena but they struggle to explain some edge cases and are mutually inconsistent <REF> . Thus, it might be unsatisfactory to apply linguistic knowledge directly to prosody modelling by conditioning the model on manually selected features. Recent advances in representation learning for text  have allowed us to come up with linguistic representations that not only capture the semantics of a word, but are also context-dependent as a function of the entire sentence. Contextual word embeddings (CWE) can be used to present to the model additional conditioning features that can help model the prosodic variations in each word, based on the context in which it is present.  investigated the effect of data reduction on seq2seq acoustic models. They train a multispeaker model with limited data from several speakers.  pre-train the decoder of their acoustic model on a large amount of unpaired data where the decoder learns the task of predicting the next frame. They also propose conditioning the model on traditional word-vectors like GloVe and Word2vec  to provide additional linguistic information. Both these works don't look at varying prosody or speakingstyle. There has been a growing interest in adaptive techniques for voice cloning , and style adaptation  with limited data. However, these models require extensive fine-tuning. Additional investigation is needed on the performance of such adaptive models on more multistyle setting."
"Neural language understanding systems have been extremely successful at information extraction tasks, such as question answering (QA). An array of existing datasets are available, which test a system's ability to extract factual answers text <REF>, as well as datasets that emphasize simple, commonsense inference (e.g., entailment between sentences) . However, it is difficult to evaluate a model's reasoning ability in isolation using existing datasets. Most datasets combine several challenges of language processing into one, such as co-reference / entity resolution, incorporating world knowledge, and semantic parsing. Moreover, the state-of-the-art on all these existing benchmarks relies heavily on large, pre-trained language models , highlighting that the primary difficulty in these datasets is incorporating the statistics of natural language, rather than reasoning."
"Current state-of-the-art neural networks need extensive computational resources to be trained and can have capacities of close to one billion connections between neurons <REF>. One solution that nature found to improve neural network scaling is to use sparsity: the more neurons a brain has, the fewer connections neurons make with each other . Similarly, for deep neural networks, it has been shown that sparse weight configurations exist which train faster and achieve the same errors as dense networks . However, currently, these sparse configurations are found by starting from a dense network, which is pruned and re-trained repeatedly -an expensive procedure."
"Language modeling is the core component of both written and spoken language understanding systems. Recent work on transformer networks <REF> trained on massive written text corpora have repeatedly demonstrated step changes in performance on several tasks including text summarization, question answering (Q&A), intent classification, natural language inference (NLI), as well as in zero-shot settings, in which they generalize to new tasks and new data without being pre trained on that data or those tasks. Leveraging these models in spoken language understanding systems, however, is obfuscated by the fact that noise injected by automatic speech recognition (ASR) processing components contains structure that is otherwise not taken advantage of. Previously, researchers have developed approaches to learn real-valued representations of spoken language that are sensitive to acoustic and phonetic similarities , and more recently semantic similarity such as Speech2Vec . These approaches have clear drawbacks, however, in that they are stand-alone models, they are trained on one task, and they do not leverage the rich, pre trained feature layers offered by models such as BERT  and GPT-2 ."
"Generative pretraining of sentence encoders <REF> has led to strong improvements on numerous natural language understanding benchmarks . In this context, a Transformer  language model is learned on a large unsupervised text corpus, and then fine-tuned on natural language understanding (NLU) tasks such as classification  or natural language inference . Although there has been a surge of interest in learning general-purpose sentence representations, research in that area has been essentially monolingual, and largely focused around English benchmarks . Recent developments in learning and evaluating cross-lingual sentence representations in many languages  aim at mitigating the English-centric bias and suggest that it is possible to build universal cross-lingual encoders that can encode any sentence into a shared embedding space."
"Gaining a better understanding of the adaptation phase is key in making the most use out of pretrained representations. To this end, we compare two state-of-the-art pretrained models, ELMo <REF> and BERT  using both and across seven diverse tasks including named entity recognition, natural language inference (NLI), and paraphrase detection. We seek to characterize the conditions under which one approach substantially outperforms the other, and whether it is dependent on the pretraining objective or target task. We find that and have comparable performance in most cases, except when the source and target tasks are either highly similar or highly dissimilar. We furthermore shed light on the practical challenges of adaptation and provide a set of guidelines to the NLP practitioner, as summarized in  induce universal representations suitable for any downstream task.  have been an essential component in state-of-the-art NLP systems. Word representations are often fixed and fed into a task specific model ( ), although can provide improvements . Recently, contextual word representations learned supervisedly (e.g., through machine translation;  or unsupervisedly (typically through language modeling;  have significantly improved over noncontextual vectors."
"Incremental learning is a paradigm in which the model is expected to learn a set of tasks sequentially, where the task is a machine learning problem such as classification of a set of classes. By the success of deep neural networks in machine learning <REF>, there are many proposals for incremental learning using deep neural networks  School of Electrical Engineering and Computer Science, Gwangju Institute of Science and Technology (GIST), Gwangju, South Korea. Correspondence to: Jonghyun Choi <jhc@gist.ac.kr>. arXiv Preprint. 2018). The neural networks, however, have been known to suffer from two prominent problems especially in the incremental learning set-up; catastrophic forgetting of the old knowledge and intransigence on learning new knowledge. While the forgetting may be less harmful in the beginning of the incremental learning process (in the early tasks), the knowledge on past tasks vanishes as the learning progress. In addition, since the budget for storing the samples from the old group of classes (e.g., previously given groups of classes in classification), is limited for practicality in incremental learning, there are a larger number of samples belonging to the new group of classes than the ones belonging to the old group of classes. The imbalance of the old and new classes makes the model prone to overfitting to the new task by which worsens forgetting. On the other hand, attempting to overcome forgetting by giving extra supervision on old classes might hinder the model from learning new classes, causing intransigence."
"Pre-trained word embeddings have been shown to be of great use for downstream NLP tasks <REF>. Many recently proposed approaches go beyond these pre-trained embeddings. Recent works have proposed methods that produce different representations for the same word depending on its contextual usage . These methods have shown to be very powerful in the fields of named entity recognition, coreference resolution, part-ofspeech tagging and question answering, especially in combination with classic word embeddings."
"Attack. Based on the above attribution analysis, we can perform adversarial attacks using interpretable perturbation. Recent work shows that DNNs are fragile, where a small perturbation could dramatically change their prediction results <REF>. We change the word ""terribly"" to its synonyms ""extremely"" and ""very"", as shown in Tab. 6. As the result, the prediction for this text shifts from strong negative to strong positive for both conditions, even though the real meaning of the original text is unchanged. Beside this example, such adversarial attack finding applies to other texts in SST2 dataset. We search for texts in test set of SST2, which contain word ""extremely"" and are originally assigned positive predictions by the LSTM classifier. We change the word ""extremely"" in the original text to ""terribly"", as can be seen in Tab. 7. Although the modified texts are semantically identical to our humans, both predictions of these two samples switch from strong positive to strong negative, demonstrating to some extent the vulnerability of this LSTM classifier.  set are below 82%. This big gap indicates that the RNN classifiers have overfitted to the training set and may have memorized the artifacts and biases that widely exist in texts. Through examining the failure reasons using the the proposed attribution method, or through attacking the RNN classifiers using attribution-based adversarial samples, we could identify the problem of the RNN classifier. We further propose three directions to promote the generalization ability of RNN classifiers, from the perspective of the training data, word embedding or recurrent model, respectively, as illustrated in . First, if attribution statistic analysis shows that the RNN has learned lots of bias from the training data, then we can check the training data to reduce the data imbalance or data leaking problem to make sure the training data is less biased. Second, if attribution analysis tell us that trained RNN cannot capture context dependent meanings of words, then we can replace context-free and fixed word embeddings, such as word2vec  and GloVe , to context-aware embeddings, such as the recently proposed ELMo , ULMFiT  and BERT . Third, if the attribution analysis indicates that the RNN heavily relies on superficial correlations in the training data, then we can add a regularizer to regulate the training behavior of the original RNN model. Bridging attribution with improving model generalization is a challenging topic, which is beyond the length of this paper and will be further investigated in our future research."
"To this end, <REF> organized a shared task specifically on suggestion mining called SemEval 2019 Task 9: Suggestion Mining from Online Reviews and Forums. The shared task is composed of two subtasks, Subtask A and B. In Subtask A, systems are tasked to predict whether a sentence of a certain domain (i.e. electronics) entails a suggestion or not given a training data of the same domain. In Subtask B, systems are tasked to do suggestion prediction of a sentence from another domain (i.e. hotels). Organizers observed four main challenges: (a) sparse occurrences of suggestions; (b) figurative expressions; (c) different domains; and (d) complex sentences. While previous attempts  made use of human-engineered features to solve this problem, the goal of the shared task is to leverage the advancements seen on neural networks, by providing a larger dataset to be used on dataintensive models to achieve better performance. This paper describes our system JESSI (Joint Encoders for Stable Suggestion Inference). JESSI is built as a combination of two neural-based encoders using multiple pre-trained word embeddings, including BERT , a pre-trained deep bidirectional transformer that is recently reported to perform exceptionally well across several tasks. The main intuition behind JESSI comes from our finding that although BERT gives exceptional performance gains when applied to in-domain samples, it becomes unstable when applied to out-of-domain samples, even when using a domain adversarial training  module. This problem is mitigated using two tricks: (1) jointly training BERT with a CNNbased encoder, and (2) using an RNN-based encoder on top of BERT before feeding to the classifier."
"In this paper, we provide two contributions: first, we release the first open, large-scale preprocessed unlabeled text corpora in the low-resource Filipino language which we call ""WikiText-TL-39."" Second, we show that transfer learning techniques such as BERT <REF> and ULMFiT  can be used to train robust classifiers in low-resource settings, experiencing at most a 0.0782 increase in error when the number of training examples is reduced from 10K to  We open source all pretrained models and datasets in an open, public repository 1 ."
"This paper describes a language representation model which combines the Bidirectional Encoder Representations from Transformers (BERT) learning mechanism described in <REF> with a generalization of the Universal Transformer model described in . We further improve this model by adding a latent variable that represents the persona and topics of interests of the writer for each training example. We also describe a simple method to improve the usefulness of our language representation for solving problems in a specific domain at the expense of its ability to generalize to other fields. Finally, we release a pre-trained language representation model for social texts that was trained on 100 million tweets."
"Approaches to sentiment analysis have moved from lexicon-based methods <REF>, to machine learning methods based on hand derived features  and finally to neural networks that learn to extract useful features in an end-to-end fashion . While some of these neural architectures have been tailored to suite specific tasks better , two recent end-to-end architectures have shown competitive results on a large number of natural language processing tasks: bidirectional Long Short-term Memory Networks (BiLSTMs)  and Self-Attention Networks (SSANs) . Variants of these two architectures give state-of-the-art results on document-level , sentence-level , and aspect-level ) sentiment analysis tasks."
"Deep Neural Networks (DNN) have demonstrated its amazing feature representation power in various tasks, such as object detection <REF>, natural language processing , speech recognition , face recognition  etc. Neural architecture, which is task specific, plays the most important role in determining the representation ability. To avoid exhaustively explore and exploit the neural architecture by hand, neural architecture search (NAS) has been proposed ."
"Contextualized word embeddings. One of the pioneering contextualized word embedding models is Context2Vec <REF>, which computes the embedding for a word in context using a multi-layer perceptron which is built on top of a bidirectional LSTM  language model. We used the 600-d UkWac pre-trained models 7 . ELMo  is a character-based model which learns dynamic word embeddings that can change depending on the context. ELMo embeddings are essentially the internal states of a deep LSTM-based language model, pre-trained on a large text corpus. We used the 1024-d pre-trained models 8 for two configurations: ELMo 1 , the first LSTM hidden state, and ELMo 3 , the weighted sum of the 3 layers of LSTM. A more recent contextualized model is BERT . The technique is built upon earlier contextual representations, including ELMo, but differs in the fact that, unlike those models which are mainly unidirectional, BERT is bidirectional, i.e., it considers contexts on both sides of the target word during representation. We experimented with two pre-trained BERT models: base (768 dimensions, 12 layer, 110M parameters) and large (1024 dimensions, 24 layer, 340M parameters). 9 Around 22% of the pairs in the test set had at least one of their target words not covered by these models. For such out-of-vocabulary cases, we used BERT's default tokenizer to split the unknown word to subwords and computed its embedding as the centroid of the corresponding subwords' embeddings."
"Attention models have proven useful for many machine learning tasks. Such models are typically applied to a set or sequence of vectors, and the associated weights on the vectors characterize their importance for a given task <REF>. The weights highlight the important parts of the sequence of vectors, and effectively provide an adaptive pooling strategy to obtain a global representation of all observations . A successful example of attention models is its application to natural language processing (NLP) tasks, like question-answering and document comprehension ) -the attention model has been core to advanced NLP modules like the ""transformer"" . Leading language models like GPT  and BERT  also rely on various attention mechanisms. Besides NLP, attention models have recently been applied to other tasks, including recommendation systems , imitation learning , and multiinstance learning . Attention models are also applicable for healthcare, a, e.g., ECG rhythm classification (Goodfellow et al., 2018), admission prediction , and heart failure prediction ."
"The arrival of modern language models like ELMo <REF>, BERT , and GPT , have significantly advanced the state-of-the art in a wide range of NLP problems. All of them have a common theme in that a generative language model is pretrained on a large amount of data, and is subsequently fine-tuned on the target task data. This approach of transfer learning has been very successful. The current work applies the same philosophy and uses BERT as the base model to encode lowlevel features, followed by a task-specific module that is trained from scratch (fine-tuning BERT in the process)."
"We address these limitations in the context of sequence labeling by developing a simple labelagnostic model that explicitly models copying token-level labels from retrieved neighbors. Since the model is not a function of the labels themselves but only of a learned notion of similarity between an input and retrieved neighbor inputs, it can be effortlessly ported (zero shot) to a task with different labels, without any retraining. Such a model can also take advantage of recent advances in representation learning, such as BERT <REF>, in defining this similarity."
"The Transformer <REF> was proposed for natural language processing, specifically machine translation. It uses a novel method to process sequence data using only attention , and is recently showing impressive performance in other tasks such as word representation learning . Graph (convolutional) networks encompass various neural network methods to handle graphs such as molecular structures, social networks, or physical experiments. . In essence, many graph networks can be described as different ways to aggregate a given node's neighbor information, combine it with the given node, and derive the node's latent representation ."
"• We apply transfer learning techniques, finetuning BERT <REF> and XLM ) models in a predictor-estimator architecture."
"We further conduct error analysis on the test set to validate our motivation that GraphIE resolves tagging ambiguity by encouraging consistency among identical entity mentions (cf. <REF> We achieve the best reported performance among methods not using the recently introduced ELMo  and BERT , which are pretrained on extra-large corpora and computationally demanding. 1). Here we examine the word-level tagging accuracy. We define the words that have more than one possible tags in the dataset as ambiguous. We find that among the 1.78% tagging errors of SeqIE, 1.16% are ambiguous and 0.62% are unambiguous. GraphIE reduces the error rate to 1.67%, with 1.06% to be ambiguous and 0.61% unambiguous. We can see that most of the error reduction indeed attributes to the ambiguous words.  shows the results for the social media information extraction task. We first report a simple dictionary-based method as a baseline. Neural IE models achieve much better performance, showing that meaningful patterns are learned by the models rather than simply remembering the entities in the training set. The proposed GraphIE outperforms SeqIE in both the EDUCATION and JOB datasets, and the improvements are more significant for the EDUCATION dataset (3.7% versus 0.3%). The reason for such difference is the variance in the affinity scores  between the two datasets.  underline that affinity value for EDUCATION is 74.3 while for JOB it is only 14.5, which means that in the datasets neighbors are 5 times more likely to have studied in the same university than worked in the same company. We can therefore expect that a model like GraphIE, which exploits neighbors' information, obtains larger advantages in a dataset characterized by higher affinity.  shows the results in the visual information extraction task. GraphIE outperforms the SeqIE baseline in most attributes, and achieves 1.2% improvement in the mirco average F1 score. It confirms that the benefits of using layout graph structure in visual information extraction. The extraction performance varies across the attributes, ranging from 61.4% for Drug Name to   In the ablation test described in , we can see the contribution of: using separate weights for different edge types (+0.8%), horizontal edges (+3.1%), vertical edges (+5.4%), and CRF (+5.7%)."
x : in denver what kind of ground transportation is there from the airport to downtown y : ( _lambda $0 e ( _and ( _ground_transport $0 ) ( _to_city $0 denver : ci ) ( _from_airport $0 den : ap ) ) ) SPIDER x : how many games has each stadium held ? posed architecture by achieving competitive results across 3 semantic parsing tasks. Further improvements are possible by incorporating a pretrained BERT <REF> encoder within the architecture.
"Our experiments build upon recent success in self-supervised pre-training <REF> and multi-task fine-tune BERT  to perform the tasks from the GLUE natural language understanding benchmark . Our training method, which we call Born-Again Multi-tasking (BAM) 2 , consistently outperforms standard single-task and multi-task training. Further analysis shows the multi-task models benefit from both better regu-  We use Single→Multi to indicate distilling single-task ""teacher"" models into a multi-task ""student"" model.  Code is available at https://github.com/ google-research/google-research/tree/ master/bam arXiv:1907.04829v1 [cs.CL] 10 Jul 2019 larization and transfer between related tasks."
"Recently, deep pre-trained language models (LMs) <REF> have shown to be capable of extracting textual representations that contain very rich syntactic and semantic information about the text sequences. By transferring the textual knowledge contained in these deep pre-trained LMs (e.g., by replacing the original text input with the extracted text features), a simple downstream model is able to achieve state-of-the-art performance on a wide range of natural language processing tasks such as natural language inference, sentiment analysis, and question answering, to name a few. These deep LMs are first trained on large amounts of unlabeled text data using selfsupervised objectives, and then fine-tuned with task-specific losses along with models for the downstream tasks."
"In this paper, we argue that the centrality measure can be improved in two important respects. Firstly, to better capture sentential meaning and compute sentence similarity, we employ BERT <REF>, a neural representation learning model which has obtained state-of-the-art results on various natural language processing tasks including textual inference, question answering, and sentiment analysis. Secondly, we advocate that edges should be directed, since the contribution induced by two nodes' connection to their respective centrality can be in many cases unequal. For example, the two sentences below are semantically related:"
"• First, we propose a training technique called TSA that effectively prevents overfitting when much more unlabeled data is available than labeled data. • Second, we show that targeted data augmentation methods (such as AutoAugment <REF>) give a significant improvements over other untargeted augmentations. • Third, we combine a set of data augmentations for NLPs, and show that our method works well and complements representation learning methods, such as BERT . • Fourth, our paper show significant leaps in performance compared to previous methods in a range of vision and language tasks. • Finally, we develop a method so that UDA can be applied even the class distributions of labeled and unlabeled data mismatch."
"Recurrent neural networks (RNN) can solve this problem in certain aspects. It was shown that contemporary Long-Short Term Memory (LSTM) or Gated Recurrent Units (GRU) models can take into account the longterm dependencies between words <REF>. Nowadays RNN-based models are implemented in various practical tasks of natural language processing due to their ability to provide high accuracy and can be robustly trained by using the well-established hyperparameters. Though conventional convolutional neural networks  can be also used in these tasks, they are limited by a fixed-length context and cannot directly learn longer-term dependencies. The modern transformer models  based on attention mechanism are not still well-studied due to their difficult training process. Hence, in this paper we decided to focus on modern RNN-based language models."
"Sentiment analysis is a well-studied task in the field of natural language processing and information retrieval <REF>. In the past few years, researchers have made significant progress from models that make use of deep learning techniques. . However, while there has been significant progress in sentiment analysis for English, not much effort has been invested in analyzing Japanese due to its sparse nature and the dependency on large datasets required by deep learning. Japanese script contains no whitespace, and sentences may be ambiguous such that there are multiple ways to split characters into words, each with a completely different meaning. To see if existing research can make progress in Japanese, we make use of recent transfer learning models such as ELMo , ULMFiT , and BERT  to each pretrain a language model which can then be used to perform downstream tasks. We test the models on binary and multi-class classification. The training process involves three stages as illustrated in . The basic idea is similar to how fine-tuning ImageNet  helps many computer vision tasks . However, this model does not require labeled data for pre-training. Instead, we pre-train a language model in unsupervised manner and then fine-tune it on a domain-specific dataset to efficiently classify using much less data. This is highly desired since there is a lack of large labeled datasets in practice."
"Deep, contextualized language models provide powerful, general-purpose linguistic representations that have enabled significant advances among a wide range of natural language processing tasks <REF>. These models can be pre-trained on large corpora of readily available unannotated text, and then fine-tuned for specific tasks on smaller amounts of supervised data, relying on the induced language model structure to facilitate generalization beyond the annotations. Previous work on model probing has shown that these representations are able to encode, among other things, syntactic and named entity information, but they have heretofore focused on what models trained on English capture about English . * Google AI Resident."
"Recent works go beyond string matching: these works have tried to view the problem of matching a one-or multi-word expression against a knowledge base as a supervised sequence labeling problem. <REF> utilized convolutional neural networks (CNNs) for phrase normalization in user reviews, while ,  applied recurrent neural networks (RNNs) to UGTs, achieving similar results. These works were among the first applications of deep learning techniques to medical concept normalization. The goal of this work is to study the use of deep neural models, i.e., contextualized word representation model BERT  and Gated Recurrent Units (GRU)  with an attention mechanism, paired with word2vec word embeddings and contextualized ELMo embeddings . We investigate if a joint architecture with special provisions for domain knowledge can further improve the mapping of entity mentions from UGTs to medical concepts. We combine the representation of an entity mention constructed by a neural model and distance-like similarity features using vectors of an entity mention and concepts from the UMLS. We experimentally demonstrate the effectiveness of the neural models for medical concept normalization on three real-life datasets of tweets and user reviews about medications with two evaluation procedures."
"Pre-trained contextualized embeddings such as ELMo <REF>, GPTs  and BERT  give improvements on a range of natural language processing tasks by offering rich language model information. We choose ELMo 4 as our contextual feature encoder, which manipulates unknown words by using character representations."
"Self-attention networks <REF> have shown promising empirical results in a variety of natural language processing (NLP) tasks, such as machine translation , semantic role labelling , and language representations . The popularity of SAN lies in its high parallelization in computation, and flexibility in modeling dependencies regardless of distance by explicitly attending to all the signals. Position embedding  is generally deployed to capture sequential information for SAN ."
"Self-attention is very expressive and flexible for both long-term and local dependencies, which used to be modeled by recurrent neural networks (RNNs) and convolutional neural networks (CNNs) <REF>. Moreover, the self-attention mechanism has fewer parameters and faster convergence than RNNs. Recently, a variety of Natural Language Processing (NLP) tasks have experienced large improvements thanks to the self-attention mechanism ."
"For representing the utterance into the fixed length vector representation, e k c , we can use 1) mean of one-hot word vectors, mean(e k−1 w + · · · + e k−1 w ), or 2) mean of external word embeddings, (i.e. Word2Vec <REF>, GloVe , fastText , etc), or 3) external sentence embedding resource (i.e. ELMo , BERT , etc). In this work, we are focusing on learning interactions of two speaker conversation rather than focusing on exploring the method of conversational-context representation, we thus use 3) BERT representation method in all of our experiments."
"Self-attention is an attention mechanism that learns a representation by relating different positions in the sequence. It facilitates the model to learn long-term context by relating each pair of positions directly. The transformer <REF>, which is a sequence model solely based on self-attention, and its variants  showed compelling results on extensive NLP tasks. Inspired by this, we propose to adopt the successful architecture to the back-end of music tagging models. By this means, one can expect not only the performance but also the interpretability."
"We investigate the recently developed Bidirectional Encoder Representations from Transformers (BERT) model <REF> for the hyperpartisan news detection task. Using a subset of hand-labeled articles from SemEval as a validation set, we test the performance of different parameters for BERT models. We find that accuracy from two different BERT models using different proportions of the articles is consistently high, with our bestperforming model on the validation set achieving 85% accuracy and the best-performing model on the test set achieving 77%. We further determined that our model exhibits strong consistency, labeling independent slices of the same article identically. Finally, we find that randomizing the order of word pieces dramatically reduces validation accuracy (to approximately 60%), but that shuffling groups of four or more word pieces maintains an accuracy of about 80%, indicating the model mainly gains value from local context."
"Most participants, including all top systems, treated gap resolution and full annotation tasks as sequence labeling tasks. The most popular approaches were to enhance the standard BLSTM-CRF architecture <REF>, to pretrain an LSTM-based language model or to use transformer-based solutions ."
"Furthermore, another benefit of the framework is high flexibility. As the modules in <REF> has less coupling to one another, it is flexible to improve or customise each of them. For example, we can deploy an advanced language understanding model, e.g., BERT , as a traditional classifier. Moreover, we may replace ConceptNet with a domain-specific knowledge graph to deal with medical texts."
"Traditional word embeddings, like Word2Vec and GloVe, merge different meanings of a word in a single vector representation <REF>. These pre-trained embeddings are fixed, and stay the same independently of the context of use. Current contextualized sense representations, like ELMo and BERT, go to the other extreme and model meaning as word usage . They provide a dynamic representation of word meaning adapted to every new context of use."
"Context prediction is also a popular representation learning approach in natural language processing, with wellknown examples such as word2vec <REF>, ELMo  and BERT . Other related work includes PixelNet , which uses loss functions defined on subsets of image pixels (much like MSP) to tackle dense prediction tasks such as edge detection and semantic segmentation."
"Transfer learning is another solution to deal with data sparsity in such tasks. It is based on the assumption that the knowledge extracted from other well-resourced tasks can be transferred to the new tasks/domains. Recently, large models pre-trained on multiple tasks with vast amounts of data, for instance BERT and MT-DNN <REF>, have obtained stateof-the-art results when fine-tuned over a small set of training samples. Following , in this paper we use BERT  within the encoder-decoder framework ( §2.1) and formulate the task of Automatic Post Editing as generating pe which is (possibly) the modified version of mt given the original source sentence src. As discussed in §2.1, instead of using multi-encoder architecture, in this work we concatenate the src and mt with the BERT special token (i.e.  and feed them to our single encoder."
"where D H is the number of hidden units in each LSTM. In this work, the parameters of WordCaps are trained with the whole model, while sophisticated pretrained models such as ELMo <REF> or BERT  may also be integrated."
The Transformer is one of the most commonly used neural network architectures in natural language processing. Variants of the Transformer have achieved state-of-the-art performance in many tasks including language modeling <REF> and machine translation . Transformer-based unsupervised pre-trained models also show impressive performance in many downstream tasks .
"Contextualized encoders such as GPT <REF> and BERT  have led to improvements on various structurally similar Natural Language Understanding (NLU) tasks such as variants of Natural Language Inference (NLI). Such tasks model the conditional interpretation of a sentence (e.g., an NLI hypothesis) based on some other context (usually some other sentence, e.g., an NLI premise). The structural similarity of these tasks points to a structurally similar modeling approach: (1) concatenate the conditioning context (premise) to a sentence to be interpreted,  D h N I stopped running right were I was h N I stopped running right were I was D h C I continued on my way read this pair using a contextualized encoder, then (3) employ the resultant representation to support classification under the label set of the task. NLI datasets employ a categorical label scheme (Entailment, Neutral, Contradiction) which has led to the use of a cross-entropy log-loss objective at training time: learn to maximize the probability of the correct label, and thereby minimize the probability of the competing labels."
"Although our proposed MirrorGAN shows superiority in generating visually realistic and semantically consistent images, some limitations must be taken into consideration in future studies. First, STREAM and other MirrorGAN modules are not jointly optimized with complete end-toend training due to limited computational resources. Second, we only utilize a basic method for text embedding in STEM and image captioning in STREAM, which could be further improved, for example, by using the recently proposed BERT model <REF> and state-of-the-art image captioning models . Third, although MirrorGAN is initially designed for the T2I generation by aligning cross-media semantics, we believe that its complementarity to the stateof-the-art CycleGAN can be further exploited to enhance model capacity for jointly modeling cross-media content."
"As yet another extension, one could combine the approaches of multilingual CoVe embeddings and monolingual ELMo (or BERT, <REF> embeddings and jointly train an encoder with a language model and an MT objective, which would potentially combine the benefit of training a model on large monolingual corpora while at the same time aligning the vector spaces of the two languages. A similar approach worked well for cross-lingual NLI inference on the XNLI data set  as well as for unsupervised machine translation ."
"Another significant advancement in neural network-based representation learning in NLP tasks is word embeddings (also called distributed representation of words). By representing each word in a given vocabulary with a real-valued vector of a fixed dimension, word embeddings enable capturing of lexical, semantic or even syntactic similarities between words. Typically, these vector representations are learned from large corpora and can be used to enhance the performance of numerous NLP tasks such as document classification, question answering and machine translation. Most frequently used word embeddings are word2vec <REF> and GloVe (Global Vectors for Word Representation) . Both of these are extracted in an unsupervised manner and are based on the distributional hypothesis , i.e., the assumption that words that occur in the same contexts tend to have similar meanings. Both word2vec and GloVe treat a word as a smallest entity to train on. A shift in this paradigm was introduced by fastText , which treats each word as a bag of character n-grams. Consequently, fastText embeddings are shown to have better representations for rare words . In addition, one can still construct a vector representation for an out-of-vocabulary word which is not possible with word2vec or GloVe embeddings . Enhanced methods for deducting better word and/or sentence representations were recently introduced as well by Peters et al. with the name ELMo (Embeddings from Language Models)  and by Devlin et al. with the name BERT (Bidirectional Encoder Representations from Transformers) . All of these word embedding models are trained on large corpora such as Wikipedia, in an unsupervised manner. For analyzing tweets, word2vec and GloVe word embeddings have been employed for topical clustering of tweets , topic modeling ,  and extracting depression symptoms from tweets ."
"Although neural networks have made strong empirical progress in a diverse set of domains (e.g., computer vision <REF>, speech recognition , natural language processing , and games ), a number of fundamental questions still remain unsolved. How can Stochastic Gradient Descent (SGD) find good solutions to a complicated non-convex optimization problem? Why do neural networks generalize? How can networks trained with SGD fit both random noise and structured data , but prioritize structured models, even in the presence of massive noise ? Why are flat minima related to good generalization? Why does over-parameterization lead to better generalization ? Why do lottery tickets exist ?"
"For future work, we plan to run similar experiments using recently introduced contextual embeddings, (e.g., <REF>, BERT , OpenAI GPT-2 (Radford et al., 2019)), which are expected to implicitly capture more syntax than context-free embeddings used in the current paper. We plan also to investigate the contribution of multi-context term embeddings to other tasks in computational semantics."
"In the general domain, we have recently observed that the General Language Understanding Evaluation (GLUE) benchmark <REF> has been successfully promoting the development of language representations of general purpose . To the best of our knowledge, however, there is no publicly available benchmarking in the biomedicine domain."
"Recent approaches to transfer learning in NLP have a empted to improve classifier scalability by first training an initial model to perform some task using a large training set, and subsequently fine-tuning the model for different tasks with smaller additional training sets <REF>. However, the transferability of other text classifiers to author identification is yet to be studied."
"The large success of deep neural networks in NLP is perplexing when considering that unlike most other NLP approaches, neural networks are typically not informed by explicit language rules. Yet, neural networks are constantly breaking records in various NLP tasks from machine translation to sentiment analysis. Even more interestingly, it has been shown that word embeddings and language models trained on a large generic corpus and then optimized for downstream NLP tasks produce even better results than training the entire model only to solve this one task <REF>. These models seem to capture something generic about language. What representations do these models capture of their language input? Different approaches have been proposed to probe the representations in the network layers through NLP tasks designed to detect specific linguistic information   Step 1: Acquire brain activity of people reading or listening to natural text"
"Radford et al. <REF> proposed similar pre-trained model, the OpenAI-GPT, by adapting the Transformer (see section IV-E). Recently, Devlin et al.  proposed BERT which utilizes a transformer network to pre-train a language model for extracting contextual word embeddings. Unlike ELMo and OpenAI-GPT, BERT uses different pre-training tasks for language modeling. In one of the tasks, BERT randomly masks a percentage of words in the sentences and only predicts those masked words. In the other task, BERT predicts the next sentence given a sentence. This task in particular tries to model the relationship among two sentences which is supposedly not captured by traditional bidirectional language models. Consequently, this particular pre-training scheme helps BERT to outperform state-of-the-art techniques by a large margin on key NLP tasks such as QA, Natural Language Inference (NLI) where understanding relation among two sentences is very important. We discuss the impact of these proposed models and the performance achieved by them in section VIII-I."
"Sentence representations pre-trained from a large amount of data have been shown to be effective when transferred to a wide range of downstream tasks. Prior work along this line can be roughly divided into two categories: i) pre-trained models that require fine-tuning on the specific transferring task <REF>; ii) methods that extract general-purpose sentence embeddings, which can be effectively applied to downstream NLP tasks without finetuning the encoder parameters . Our proposed methods belong to the second category and provide a generic and easy-to-use encoder to extract highly informative sentence representations. However, our work is unique since the embeddings inferred from our models are binarized and compact, and thus possess the advantages of small memory footprint and much faster sentence retrieval."
"Many recent advances in NLP have been driven by new approaches to representation learningi.e., the design of models whose primary aim is to yield representations of words or sentences that are useful for a range of downstream applications <REF>. Approaches to representation learning typically differ in the architecture of the model used to learn the representations, the objective used to train that network, or both. Varying these factors can significantly impact performance on a broad range of NLP tasks ."
"Common approaches address the ID and SF tasks in joint Deep Learning architectures (e.g., <REF>). In particular, encoder-decoder models  and/or recurrent neural networks (RNN) with attention ) are trained on the task of predicting at the same time intents and slots. Recently, recurrence-less models  shifted the attention on a neural-based computation for natural language which is not based on the typical recurrent processing happening in RNNs. Based on this idea, BERT  models multiple tasks with a unique deep attention-based architecture."
"Ensemble learning is an effective approach to improve model generalization, and has been used to achieve new state-of-the-art results in a wide range of natural language understanding (NLU) tasks, including question answering and machine reading comprehension <REF>. A recent survey is included in . However, these ensemble models typically consist of tens or hundreds of different deep neural network (DNN) models and are prohibitively expensive to deploy due to the computational cost 1 Based on the GLUE leaderboard at https://gluebenchmark.com/leaderboard as of April 1, 2019. of runtime inference. Recently, large-scale pretrained models, such as BERT  and GPT , have been used effectively as the base models for building taskspecific NLU models via fine-tuning. The pretrained models by themselves are already expensive to serve at runtime (e.g. BERT contains 24 transformer layers with 344 million parameters, and GPT-2 contains 48 transformer layers with 1.5 billion parameters), the ensemble versions of these models multiplying the extreme for online deployment."
"To tackle benchmark tasks, many computational models have been developed from earlier symbolic and statistical approaches to more recent approaches that learn deep neural models from training data. These models are often augmented with external data or knowledge resources, or pre-trained word embeddings, e.g., BERT <REF>. Section 4 summarizes the state-of-the-art approaches, and discusses their performance and limitations."
"Recurrent Neural Networks (RNNs) especially its variants such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) have achieved great success in a wide range of sequence learning tasks including language modeling, speech recognition, recommendation, etc <REF>. Despite their success, however, the recurrent structure is often troubled by two notorious issues. First, it easily suffers from gradient vanishing and exploding problems, which largely limits their ability to learn very long-term dependencies . Second, the sequential nature of both forward and backward passes makes it extremely difficult, if not impossible, to parallelize the computation, which dramatically increases the time complexity in both training and testing procedure. Therefore, many recently developed sequence learning models have completely jettisoned the recurrent structure and only rely on convolution operation or attention mechanism that are easy to parallelize and allow the information flow at an arbitrary length. Two representative models that have drawn great attention are Temporal Convolution Networks(TCN)  and Transformer . In a variety of sequence learning tasks, they have demonstrated comparable or even better performance than that of RNNs .  : The illustration of one layer of R-Transformer. There are three different networks that are arranged hierarchically. In particular, the lower-level is localRNNs that process positions in a local window sequentially (This figure shows an example of local window of size 3); The middle-level is multi-head attention networks which capture the global long-term dependencies; The upper-level is Position-wise feedforward networks that conduct non-linear feature transformation. These three networks are connected by a residual and layer normalization operation. The circles with dash line are the paddings of the input sequence The remarkable performance achieved by such models largely comes from their ability to capture long-term dependencies in sequences. In particular, the multi-head attention mechanism in Transformer allows every position to be directly connected to any other positions in a sequence. Thus, the information can flow across positions without any intermediate loss. Nevertheless, there are two issues that can harm the effectiveness of multi-head attention mechanism for sequence learning. The first comes from the loss of sequential information of positions as it treats every position identically. To mitigate this problem, Transformer introduces position embeddings, whose effects, however, have been shown to be limited . In addition, it requires considerable amount of efforts to design more effective position embeddings or different ways to incorporate them in the learning process . Second, while multi-head attention mechanism is able to learn the global dependencies, we argue that it ignores the local structures that are inherently important in sequences such as natural languages. Even with the help of position embeddings, the signals at local positions can still be very weak as the number of other positions is significantly more."
"As the book is too small to train any models, we leverage recent advances in large-scale language model-based representations <REF> to compute a representation of each city. We feed these representa-tions into a clustering algorithm that produces exactly eleven clusters of five cities each and evaluate them against both Calvino's original labels and crowdsourced human judgments. While the overall correlation with Calvino's labels is low, both computers and humans can reliably identify some thematic groups associated with concrete objects."
"We introduce a novel method of generating synthetic question answering corpora by combining models of question generation and answer extraction, and by filtering the results to ensure roundtrip consistency. By pretraining on the resulting corpora we obtain significant improvements on SQuAD2 (Rajpurkar et al., 2018) and NQ (Kwiatkowski et al., 2019), establishing a new state-of-the-art on the latter. Our synthetic data generation models, for both question generation and answer extraction, can be fully reproduced by finetuning a publicly available BERT model <REF> on the extractive subsets of SQuAD2 and NQ. We also describe a more powerful variant that does full sequence-to-sequence pretraining for question generation, obtaining exact match and F1 at less than 0.1% and 0.4% from human performance on SQuAD2."
"We use the BERT model (Bidirectional Encoder Representations from Transformers, <REF> as our base pre-trained model. Pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, including the GLUE benchmark. However, the entire model is finetuned, meaning we need a separate model for each task. The arXiv:1902.02671v2 LG] 15 May 2019 transformer architecture that BERT is based on is powerful and popular, so finding the best way to adapt the parameters of this architecture for multi-task learning may be useful in other contexts, such as multilingual machine translation."
"An emerging trend in natural language processing is to train a language model in an unsupervised fashion on a large corpus of text, and then to fine-tune the model for a specific task <REF>]. The language model often takes the form of an LSTM  or a Transformer network ]."
"Denoising auto-encoders <REF> are commonly used for model initialization to extract and select features from inputs. BERT ) used a pre-trained bi-directional transformer model and outperformed existing systems by a wide margin on many NLP tasks. In contrast to denoising auto-encoders, BERT only predicts the 15% masked words rather than reconstructing the entire input. BERT denoise the 15% of the tokens at random by replacing 80% of them with [MASK], 10% of them with a random word and 10% of them unchanged."
"Pre-trained embeddings such as word embeddings <REF> and sentence embeddings  have become fundamental NLP tools. Learned with large-scale (e.g., up to 800 billion tokens ) open-domain corpora, such embeddings serve as a good prior for a wide range of downstream tasks by endowing task-specific models with general lexical, syntactic, and semantic knowledge."
"Language representation pre-training <REF> has been shown effective for improving many natural language processing tasks such as named entity recognition, sentiment analysis, and question answering. In order to get reliable word representation, neural language models are designed to learn word cooccurrence and then obtain word embedding with unsupervised learning. The methods in Word2Vec  and Glove  represent words as vectors, where similar words have similar word representations. These word representations provide an initialization for the word vectors in other deep learning models. Recently, lots of works such as Cove , , GPT  and BERT  improved word representation via different strategies, which has been shown to be more effective for down-stream natural language processing tasks."
"In recent years, deep learning technologies have been intensively used for text classification in the presence of sufficient training data. For example, convolutional neural network (CNN) <REF> is capable of capturing key words in the sentences, while recurrent neural network (RNN)  is proven to have better performance at modeling long dependencies among text, and so on. No matter which model is chosen, a large amount of supervised data are needed to learn the numerous model parameters. * Contact Author To tackle the low-resource problem in NLP, a lot of pretrained models are developed for domain adoption. Pretrained word embeddings  are perhaps the most popular techniques which are frequently adopted in many NLP tasks. The generalization of word embeddings, such as sentence embeddings , are also used in downstream models, e.g., natural language inference, etc. More recently, the technique of pre-trained language models trained on large monolingual corpora combined fine-tuning has become dominant in a variety of NLP benchmarks. These models, such as BERT , Open AI GPT , and Transformer-XL , are complex neutral networks with extremely deep depth. The borrowed knowledge from pre-trained models can not only improve predictive performance for downstream models, but also accelerate model training for lowresource problems."
"External Transferable Knowledge and Learning schemas Clearly, the improvement in accuracy and performance is not merely because of the shift from feature engineering to structure engineering, but the flexible ways to incorporate external knowledge <REF> and learning schemas to introduce extra instructive constraints . For this part, we make some first steps toward answers to the following questions: 1) Which type of pre-trained models (supervised or unsupervised pre-training) is more friendly to the summarization task? 2) When architectures are explored exhaustively, can we push the state-of-the-art results to a new level by introducing external transferable knowledge or changing another learning schema?"
"can be acquired directly from the labels. However, representation learning in the unsupervised setting, without hand-specified labels, becomes significantly more challenging: although much more data is available for learning, this data lacks the clear learning signal that would be provided by human-specified semantic labels. Nevertheless, unsupervised representation learning has made significant progress recently, due to a number of different approaches. Representations can be learned via implicit generative methods <REF>, via explicit generative models , and self-supervised learning . Among these, the latter methods are particularly appealing because they remove the need to actually generate full observations (e.g., image pixels or audio waveform). Self-supervised learning techniques have demonstrated state-of-the-art performance in speech and image understanding , reinforcement learning , imitation learning , and natural language processing ."
"The Transformer model using self-attention mechanism has recently achieved the state of the art accuracy in language translation <REF>. Compared to its predecessors, this sequence transduction model circumvents recurrent or long-short memory (LSTM) neural cells and exploits multi-headed attention mechanism to capture global dependencies between input and output word sequences. Since its first use in machine translation, the multi-headed attention mechanism has shown tremendous promise in speech recognition , generative language modeling , machine reading comprehension , Language representation models  and other natural language processing workloads."
"In this work, we introduce a generalization of the standard transformer architecture to accept lattice-structured network topologies. The standard transformer is a transduction model relying entirely on attention modules to compute latent representations, e.g., the self-attention requires to calculate the intra-attention of every two tokens for each sequence example. Latest works such as <REF> empirically find that transformer can outperform LSTMs by a large margin, and the success is mainly attributed to selfattention. In our lattice transformer, we propose a lattice relative positional attention mechanism that can incorporate the probability scores of ASR word lattices. The major difference with the selfattention in transformer encoder is illustrated in ."
"Recently, AN is being actively used in various fields <REF>. For example, AN was used to solve the position dependency problem of recursive neural network (RNN) in natural language processing (NLP). In other words, AN succeeded in deriving even the relation between distant words in a sentence. Multi-head attention (MHA)  which simultaneously calculates various relations using multiple ANs already became the most important technique for NLP. On the other hand, non-local neural network  applied ANs to CNN to solve computer vision tasks such as detection and classification."
"In this paper, we focus instead on a function word form, namely the German reflexive pronoun sich. Traditionally, the context of function words was considered to be too general to be amenable to distributional analysis. The situation has changed with a generation of recently proposed distributional models that learn so-called contextualized embeddings. These models concurrently learn (a) general vectors for word types and (b) specialized vectors for word tokens in their context. This division of labor circumvents the generality problem: even if the representation of the word type sich is too general to be useful, the ability of the model to learn how the meaning of each sich token arises from a combination of basic word meaning and context. The specific embedding model we use is BERT <REF>, a so-called transformer architecture which captures relations among words in an unsupervised fashion with the help of an attention mechanism . In concrete terms, we use the pretrained 'BERT multilingual base' model which provides 768-dimensional contextualized embeddings for all input tokens. To visualize these vectors, we perform principal components analysis, a standard dimensionality reduction method, to represent instances of sich on a two-dimensional plane."
4. BERT 7 <REF> is a state-of-theart supervised sentence embedding approach based on the Transformer architecture.
"Our approach for both task 1 and task 2 is based on the state-of-the-art natural language understanding model MT-DNN <REF>, which combines the strength of multi-task learning (MTL) and language model pre-training. MTL in deep networks has shown performance gains when related tasks are trained together resulting in better generalization to new domains . Recent works such as BERT ,  have shown * equal contribution the efficacy of learning universal language representations in providing a decent warm start to a task-specific model, by leveraging large amounts of unlabeled data. MT-DNN uses BERT as the encoder and uses MTL to fine-tune the multiple taskspecific layers. This model has obtained stateof-the-art results on several natural language understanding tasks such as SNLI , SciTail  and hence forms the basis of our approach. For the task 3, we use a simple model to combine the task 1 and task 2 models as shown in §2.5."
"Several works <REF> adapted the Transformer architecture beyond the task of NMT. Their main idea is to train the Transformer in an unsupervised fashion over large corpora of texts and further refine its parameters on a wide variety of supervised tasks. Motivated by these recent works, we present a model, MATAN (Mutual Attention for Text-Attributed Networks), that derives from the the SDPA to address the task of link prediction in a network of documents."
"To fill this gap, we conduct an intensive investigation with the following directions for obtaining the best performance in the answerselection task. First, we explore the effect of additional information by adopting a pretrained language model (LM) to compute the vector representation of the input text. Recent studies have shown that replacing the word-embedding layer with a pretrained language model helps the model capture the contextual meaning of words in the sentence <REF>. Following this study, we select an ELMo  language model for this study. Furthermore, we investigate the applicability of transfer learning (TL) by using a large-scale corpus that is created for relevant-sentence-selection task (i.e., questionanswering NLI (QNLI) dataset ). Second, we further enhance one of the baseline models, Comp-Clip  (see the discussion in 3.1), for the target QA task by proposing a novel latent clustering (LC) method. The LC method compute latent cluster information for target samples by creating latent memory space and calculating similarity between the sample and the memory. Through an endto-end learning process along with the answer-selection task, LC method assigns true-label question-answer pairs into similar clusters. In this way, a model will have further information in matching sentence pairs that increases the overall model performance. Last, we explore the effect of different objective functions (listwise and pointwise learning). Different from the previous research , we observe that the pointwise learning approach performs better than that of listwise learning when we apply our proposed methods. Extensive experiments are conducted to investigate the efficacy and properties of the proposed methods and show the superiority of our proposed approaches with achieving state-of-the-art performance on the WikiQA and TREC-QA datasets."
"Recently, a novel attention-based network architecture named Transformer was proposed in <REF>. It performs well without any recurrence or convolution and the use of Transformer has become popular in various domains. For example, a bi-directional Transformer model called BERT achieved state-of-the-art results on eleven natural language processing (NLP) tasks . In the domain of music,  applied Transformer to a music generation task and succeeded in creating music with complex and repetitive structure."
"Pre-training and fine-tuning, e.g., BERT <REF>, have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for encoder-decoder based language generation. MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over baselines without pre-training or with other pretraining methods. Specially, we achieve state-ofthe-art accuracy (37.5 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model (Bahdanau et al., 2015b) 1 ."
"Recently, there is a growing interest in applying MTL to representation learning using deep neural networks (DNNs) <REF> for two reasons. First, supervised learning of DNNs requires large amounts of task-specific labeled data, which is not always available. MTL provides an effective way of leveraging supervised data from many related tasks. Second, the use of multi-task learning profits from a regularization effect via alleviating overfitting to a specific task, thus making the learned representations universal across tasks. In contrast to MTL, language model pretraining has shown to be effective for learning universal language representations by leveraging large amounts of unlabeled data. A recent survey is included in . Some of the most prominent examples are ELMo , GPT  and BERT . These are neural network language models trained on text data using unsupervised objectives. For example, BERT is based on a multi-layer bidirectional Transformer, and is trained on plain text for masked word prediction and next sentence prediction tasks. To apply a pre-trained model to specific NLU tasks, we often need to fine-tune, for each task, the model with additional task-specific layers using task-specific training data. For example,  shows that BERT can be fine-tuned this way to create state-of-the-art models for a range of NLU tasks, such as question answering and natural language inference."
"Undirected neural sequence models such as BERT <REF> have recently brought significant improvements to a variety of discriminative language modeling tasks such as questionanswering and natural language inference. Generation of sequences from such models has received relatively little attention. Unlike directed sequence models, each word often depends on the full left and right context around it in undirected sequence models. Thus, a decoding algorithm for an undirected sequence model must specify both how to select positions and what symbols to place in the selected positions. In this paper we formalize this process of selecting positions and replacing symbols as a generalized framework of sequence generation, and unify decoding from both directed and undirected sequence models under this framework. This framing enables us to study generation on its own, independent from the specific parameterization of the sequence models."
"In this paper, we use transfer learning to address the lack of training data. Word2vec <REF>, which extracts knowledge from unannotated texts, has been one of the major advancements in natural language processing. However, in biomedical text mining tasks, Word2vec needs to be modified when applied to biomedical data due to the large differences in vocabulary and expressions between a biomedical corpus and a general domain corpus . Recent development of ELMo  and BERT  has proved the effectiveness of contextualized representations from a deeper structure (e.g., bidirectional language model) for transfer learning. While contextualized representations extracted from a general domain corpus such as Wikipedia have been used in biomedical text mining studies , only few researches have focused on directly extracting contextualized representations from a biomedical corpus ."
". As a next step it predicts the probability that the i th word denotes the start of the answer span as s i = exp (q W1C u i ) n i=k exp (q W1C u k ) , using a bilinear soft-max model. Subsequently, it summarizes the context with the start prediction and produces z = n i=1 s i C u i .  <REF> PyTorch v1.0.0 1 T4 / GCP 0:56:43 DrQA (ParlAI)  PyTorch v1.0.0 1 P4 / GCP 1:00:35 DrQA (ParlAI)  PyTorch v0.4.1 1 V100 1:22:33 BERT-base  (1 epoch fine-tuning) TensorFlow v1.11.0 1 GTX-1080 Ti 7:38:10 BiDAF  TensorFlow v1.2 1 K80 : DAWNBench Training Track It then produces a refined question vectorq with one step of GRU , using the original question vector q as the hidden memory and z as the input, i.e.q = GRUCell(z, q). Similarly, a bi-linear module is applied to get the end predicted"
"Both metrics strongly depend on learned word embeddings. We propose to explore the use of contextualized embeddings, specifically BERT embeddings <REF>, in composing evaluation metrics. Our contributions in this work are as follows:"
"Most of the existing product representation techniques for ecommerce depends largely on customer behavior data. Customer co-view and co-purchase information can be directly used to build product embedding such that products with similar browse or purchase history will share similar vectors <REF>. Product catalog data, on the other hand, is a collection of free-form texts like title, description, brand, etc.. There are supervised and unsupervised ways to transform the catalog data into a vector space. The supervised way originates from ImageNet , where a multi-source neural network is trained to predict certain product categorization labels, and the last hidden layer is used as the instance embedding . The categorization labels require extensive annotation while the embedding vector is found of high quality. The unsupervised way, on the other hand, applies to broader cases, especially when labels are unavailable. With the well established methods of word2vec and sentence2vec , the problem of product representation learning effectively becomes the task of vector aggregation, namely the method of combining embedding vectors from multiple product fields. Though this problem seems fundamental for natural language processing in e-commerce, to the best of our knowledge, there is no related research dedicated to this problem."
"Recently, large-scale pre-training with generative models on language <REF> have achieved stunning success on various tasks including generating coherent text. By conditioning on certain observed text, these pre-trained models can also exert some degree of control on text generation. However, these models are essentially black boxes. The control over the generated text using seed sentences is weak, unpredictable and often uninterpretable."
"More recently, the pre-trained language models, such as ELMo <REF>, OpenAI GPT , and BERT , have shown their effectiveness to alleviate the effort of feature engineering. Especially, BERT has achieved excellent results in QA and NLI. However, there is not much improvement in (T)ABSA task with the direct use of the pretrained BERT model (see ). We think this is due to the inappropriate use of the pre-trained BERT model."
"Deep neural networks (DNNs) have been widely applied in various fields, including computer vision <REF>, , natural language processing , and speech recognition , among many others. Graph neural networks (GNNs) have been proposed for learning node presentations for networked data , and later have been extended with graph convolutions that can better capture the topological information in a network . Since then, GCNs begin to attract wide interests. For example, GraphSage  defines the convolutional neural network (CNN) based graph learning framework as sampling and aggregation. Vast follow-up works further enhance the sampling or aggregation process via various techniques such as the attention mechanism  and the adaptive sampling mechanism ."
"Concurrently, the NLP community continues to build better contextual text representations such as ELMo <REF> and BERT . Both as-is usage and finetuning of these representations showed significant improvements in downstream NLP tasks. In the speech community however, only non-contextualized embeddings  have been proposed thus far, with some success in SR, gender, and emotion recognition. However, the potential gains from contextualization and self-attention  for speech tasks has not previously been explored."
"To investigate these questions, we consider the task of event completion, where one held-out entity is predicted from the remaining entities that participate in an event. An entity is said to participate in an event, if it is named in its description. Thus, predicting one entity from other participating entities is a suitable problem to evaluate relevant associations between words as captured by word embeddings, as well as different combination modes of word vectors to exploit the neighbourhood relationships. Since the task relies on the cooccurrence of entities in a common context, it lends itself to the use of embeddings. However, the fact that an entity may occur in different contexts provides a challenge for learned word vectors (for example, Brazil held the Summer Olympics in 2016, and in the same year impeached its president Dilma Rousseff for breaking fiscal laws). Benchmarks and training data for such an event completion task are provided by Spitz and Gertz <REF>, who used it to evaluate ranking in entity cooccurrence networks. Research by  suggests that there is no universally best embedding method since the performance of embeddings varies by task . We therefore evaluate word2vec-CBOW, word2vec-skip-gram, and GloVe to determine which method is best suited for identifying the participation of entities in events, and discuss the benefits and drawbacks of each tested method. Contributions. We make two primary contributions. (1) We compare six modes of combining embedding vectors for multi-entity queries. (2) Based on a comparison to entity ranking in cooccurrence networks, we discuss the influence of an entity's frequency in the corpus on the performance of its resulting embedding.  , leading to methods such as Latent Dirichlet Allocation  and neural language models . These provide the groundwork for current word embedding models, which usually employ shallow neural networks. Recently, the context-based embeddings ELMo  and BERT  have been introduced, which are generated by data and training intensive deep architectures. These word representations vary with the input sentence(s), and are thus unsuitable for an isolated entity retrieval task, so we rely on more traditional embeddings, which yield only one fixed vector per word. Word2vec is one of the most widely used models and comes in two variants . Continuous bag-of-words (CBOW) averages the word vectors of context words and uses them to classify the focal word. Inversely, continuous skip-gram predicts the context words from the focal word. Thus, word vectors are learned from local context windows. Typically, word2vec is modified to include negative sampling , which distinguishes between the real context words and a sample that is drawn from a noise distribution. GloVe was proposed as global word representation vectors that improve upon the approach of word2vec . To this end, they make use of the global corpus statistics by training a log-bilinear regression model on the word cooccurrence matrix. By employing a more general weighting function, they also gain more control over how strongly frequent words influence the model. Implicit networks, in contrast, were introduced to model latent relationships between entities as well as the remaining terms . The latent relationships are captured by constructing an entity and term coocurrence graph that is weighted by cross-sentence distances within the documents, thus capturing all word cooccurrences in the text. While the model has a variety of applications like event extraction or entity-centric topic extraction , it comes with event completion ground-truth data for news articles , so we use it as a baseline. With the prevalence of machine learning applications, it is also of interest to evaluate how well the implicit network performs in comparison to learned representations. Combining word vectors as a technique is typically employed for creating document embeddings from word embeddings  or for modeling multi-word entities in knowledge bases . Several combination modes exist in the literature, mainly component-wise minimum and maximum, as well as averaging , which are often combined by concatenation into a single vector. Another approach is summing over the similarities between multiple query vectors and document vectors . Mitchell and Lapata compare several composition functions for handcrafted cooccurrence-based word vectors . Iacobacci et al. compare word embedding combinations for the task of word sense disambiguation . However, to the best of our knowledge, no previous publication has compared different combination modes of learned word vectors obtained from several embedding methods on an entity-centric task."
"Given the insurmountable barrier in performance between MLE-based text generation and other methods, we believe the most promising approach is to stick with the MLE-based approach. Even within MLE training paradigm, there are many ways to proceed. As far as we are aware, text generation with the state-of-the-art quality-diversity trade-off can be achieved with vanilla Transformer or Transformer-XL with beam search (for translation and chatbot) or temperature-tuned decoding, depending on the task of interest <REF>. The sampling method is crucial for better text generation.  used LM top-k sampling (k = 10) and temperature of α = 0.8 (both for vanilla LM case and hierarchical LM case), which they found ""substantially more effective than beam search, which tends to produce common phrases and repetitive text from the training set.""  attempted vanilla LM text generation with temperature of α = 0.7. In either case, the quality-diversity trade-off of the generation is substantially better than any previous method. If the vocabulary of dataset is large, adaptive softmax and adaptive embedding can be used to dramatically reduce the number of parameters .  showed that the use of recurrence achieved a better perplexity even on a dataset without long-range dependency such as 1BLM.  shows some of the top 50% of the samples generated by Transformer LM with temperaturetuned decoding. As the figure shows, the vanilla Transformer with the proper decoding generates with local dependency at a satisfactory level, whereas the global dependency is still weak. For example, the last sample is clearly unrealistic, given that a person has been campaigning for more than a million years. Since the local dependency is good even at the later part of a sentence, the weak global coherence is presumably not due to exposure bias. We believe that it is due to shortness of attention, which was identified as a problem for attention mechanism in . This is in contrast with BERT , which does not pose the shortness of attention. We believe this is because the bidirectional problems such as BERT and ELMo are substantially easier than unidirectional LM and therefore long-range attention can be established without special care. As an improvement on 1BLM was achieved with Transformer-XL, an improvement in the recurrent training in the sense of Transformer-XL may extend the span of attention, since it discourages for the gradient to flow in the short span."
"In the natural language processing (NLP) community, neural network language models have been applied successfully to improve various downstream applications <REF>, , including commonsense reasoning. For example, Trinh et al.  applied pre-trained language models to solve the Winograd Schema Challenge, a pronoun disambiguation challenge which involves using common sense to determine the object pronouns are referring to (e.g., the trophy does not fit in the suitcase because it is too big). Recently, a large transformer-based language model BERT  has achieved state-of-the-art results on several NLP benchmarks, including a commonsense reasoning task Situations With Adversarial Generations (SWAG) , by pre-training the language model on the unlabeled textual corpus and then fine-tuning on a specific task. These applications show that neural network language models are well suited to encoding and extracting knowledge that exists in large language corpora. In our current work, we have shown that our language model is able to learn domain-specific knowledge by using raw textual data from the web and further that the learned knowledge matches well with human common sense."
"Natural language processing (NLP) has achieved significant advances in reading comprehension tasks <REF>. These are partially due to embedding methods  and neural networks , but also to the availability of new resources and challenges. For instance, in cloze-form tasks , the goal is to predict the missing word given a short context.  presented baBI, a set of proxy tasks for reading comprenhension. In the SQuAD corpus , the aim is to answer questions given a Wikipedia passage.  introduce NarrativeQA, where answering the questions requires to process entire stories. In a related line,  use fictional crime scene investigation data, from the CSI series, to define a task where the models try to answer the question: 'who committed the crime?'."
"Deep latent variable models, which combine the composability and interpretability of graphical models with the flexible modeling capabilities of deep networks, are an exciting area of research. It is nonetheless worth discussing again why we would want to use latent variable models in the first place, especially given that from a pure performance standpoint, models that do not formally make use of latent variables such as LSTMs and transformer networks are incredibly effective <REF>. Even from the perspective of learning interesting/meaningful structures (e.g. topic modeling, word alignment, unsupervised tagging), one may argue that these meaningful representations can be implicitly captured within the hidden layers of a deep network. Indeed, the recent, astonishing success of pretrained (non-latent variable) language models as generic feature extractors  suggests that deep networks can capture significant amount of linguistic knowledge without the explicit use of latent variables."
"Recently-proposed attention mechanisms in deep neural network models have also shown great promise in learning to predict complex time-series data <REF>, . When people listen to a story, or even a string of words, not every word is equally important: People tend to pay attention to certain key words or phrases that carry relatively more information. Attention mechanisms in deep neural network models attempt to capture the intuition behind such behavior by trying to learn the relative importance of words within a given timewindow. In vanilla implementations, attention layers in a deep model learn sets of weights over their inputs which are then used to upweight certain parts of the input over others. These attention mechanisms have experienced much success, perhaps most exemplified by the recently-proposed Transformer model , which currently represents the stateof-the-art on natural language sequence-prediction tasks. What is perhaps most interesting about the Transformer model is that it is entirely attention-based; It contains no recurrency (i.e., directed connections between hidden states at consecutive time-steps), which has been the de-facto ""standard"" in sequence-prediction models to learn time-dependent information. Since its introduction, Transformer-based models have been used in various NLP tasks, including text comprehension , Question-and-Answering  machine translation , and language modelling . Notably, however, self-attention mechanisms like in the Transformer have not yet been applied to emotion recognition, and the very recent success of other types of attention applied to emotion recognition (e.g., in RNNs  and LSTMs , or the Memory Fusion Network of ) suggest that these may be fruitful approaches that should be further investigated."
"A lot of progress still remains to be done in this challenging task, as the accuracy of state-of-the-art systems has not reached satisfactory levels yet. At the moment, the most exciting developments in mastering language are coming from the frontier of deep learning and unsupervised language models <REF>. The exploitation of such models for keyphrase extraction and/or generation appears as the most interesting future direction."
"This work presents a solution that can resolve the inefficient multiple-passes issue of existing solutions for MRE by encoding the input only once, which significantly increases the efficiency and scalability. Specifically, the proposed solution is built on top of the existing transformer-based, pretrained general-purposed language encoders. In this paper we use Bidirectional Encoder Representations from Transformers (BERT) <REF> as the transformer-based encoder, but this solution is not limited to using BERT alone. The two novel modifications to the original BERT architecture are: (1) we introduce a structured prediction layer for predicting multiple relations for different entity pairs; and (2) we make the selfattention layers aware of the positions of all en-tities in the input paragraph. To the best of our knowledge, this work is the first promising solution that can solve MRE tasks with such high efficiency (encoding the input in one-pass) and effectiveness (achieve a new state-of-the-art performance), as proved on the ACE 2005 benchmark."
"Another successful neural network paradigm is the attention mechanism <REF>, which has been extremely useful in tackling many machine learning tasks , particularly sequence-based tasks . The state-of-the-art attention mechanism is self-attention or intra-attention, which computes the representation of an input (e.g., a set or sequence) by focusing on its most relevant parts. Self-attention has been successfully applied to a variety of tasks including machine translation , video classification  and question answering . Nonetheless, the majority of these efforts target supervised learning tasks, and few efforts  are made to tackle unsupervised learning tasks. In graph representation learning, to our knowledge, the only proposed attention-based method uses supervised learning ."
"Machine representation of words dates back to ASCII. This one-hot representation encodes each character using a mixture of dummy or indicator variables. While this was slowly extended to words, the large vocabulary size of languages made it difficult. Learned, or distributed, word vector representations <REF> replaced one-hot encodings. These word vectors are able to capture semantic information including context from neighboring words. Even today, the community continues to build better contextual word embeddings such as ELMo , ULMFit , and BERT . Word, phoneme, and grapheme embeddings like Speech2Vec  and Char2Wav  have also been proposed for speceh, following techniques from natural language understanding. While word-level embeddings are promising, they are often insufficient for speech-related tasks for several reasons. First, word and phoneme embeddings capture a narrow temporal context, often a few hundred milliseconds at most. As a result, these embeddings cannot capture long-term dependencies required for higher-level reasoning (e.g., paragraph or song-level understanding). Almost all of the systems for speech recognition focus on the correctness of local context (e.g., letters, words, and phonemes) rather than overall semantics. Second, for speech recognition, an external language model is often used to correct character and word-level predictions. This requires the addition of complex, multiple hypothesis generation methods ."
"More recently, pre-trained language models such as BERT have begun to emerge <REF>. BERT involves bidirectional training of transformers on two tasks simultaneously. These tasks are masked language modelling and next sentence prediction. BERT has led to numerous improvements in various areas of natural language processing, such as question answering and natural language inference ."
"• In SLE, an item is represented via the sentence level embedding of BERT <REF>, i.e., the embedding of token [CLS] is used as the item's embedding. Meanwhile, and a user is still represented as the average of her items' embeddings as AVG. Notice that the context feature in recommendation request is not directly comparable with item's text feature, thus they are ignored in AVG and SLE, where user embedding is used as the retrieval key."
"We remark that the DE does not require any training scheme, instead it is inspired by Language Modeling methods <REF> that uses random initialization to map the input to a space embedding distribution. Particularly, we use a simple random embedding, i.e. a fully connected layer to map from style and labels concatenation to the AdaIN parameters. Our rationale is as follows: By always ensuring different z, we guarantee different normalization parameters, which means different fake images. We study the DE behaviour in more detail in Section 5.1."
"Language modeling is a form of unsupervised learning that allows language properties to be learned from large amounts of unlabeled text. As components, language models are useful for many Natural Language Processing (NLP) tasks such as generation <REF> and machine translation . Additionally, the form of predictive learning that language modeling uses is useful to acquire text representations that can be used successfully to improve a number of downstream NLP tasks . In fact, models pre-trained with a predictive objective have provided a new stateof-the-art by a large margin."
"The Situations With Adversarial Generations (SWAG) dataset <REF> introduced a large-scale benchmark for commonsense question answering in the form of multiple choice sentence completion questions describing situations as observed in video. However, while SWAG was constructed to be resistant to certain baseline algorithms, powerful subsequent methods were able to perform very well on the dataset. In particular, the development of the transformer architecture  has led to powerful pre-trained language model representations, including the OpenAI Transformer Language Model  and the Bidirectional Encoder Representations from Transformers (BERT) model . BERT achieved new state-of-the-art performance on SWAG that exceeded even that of a human expert. However, BERT does not possess human-level common sense in general, as our experiments demonstrate. It is instead able to exploit regularities in the SWAG dataset to score high. This motivates the construction of additional datasets that pose new challenges, and serve as more reliable benchmarks for commonsense reasoning systems."
"Contextual word embedding models such as ELMo <REF> and BERT  have dramatically improved performance for many natural language processing (NLP) tasks in recent months. However, these models have been minimally explored on specialty corpora, such as clinical text; moreover, in the clinical domain, no publicly-available pre-trained BERT models yet exist. In this work, we address this need by exploring and releasing BERT models for clinical text: one for generic clinical text and another for discharge summaries specifically. We demonstrate that using a domain-specific model yields performance improvements on three common clinical NLP tasks as compared to nonspecific embeddings. These domainspecific models are not as performant on two clinical de-identification tasks, and argue that this is a natural consequence of the differences between de-identified source text and synthetically non de-identified task text."
"Moreover, BERT <REF> pretrains language models and fine-tunes them on the target task. An auxiliary task (next sentence prediction) is used to enhance the representations of the LM. BERT fine-tunes masked bi-directional LMs. Nevertheless, we are limited to a uni-directional model. Training BERT requires vast computational resources, while our model only requires 1 GPU. We note that our approach is not orthogonal to BERT and could be used to improve it, by adding an auxiliary LM objective and weighing its contribution."
"Self-training methods such as ELMo <REF>, GPT , BERT , XLM , and XLNet  have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most. Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances."
"The input for this encoder is an ordered sequence of object labels, sorted by the confidence score of the model that predicts these labels. This allows the model to learn that labels appearing at the head of the sequence are more reliable. For each separate label, we create learnable segment embeddings (inspired by <REF>) as shown in , using the subtokenization scheme described in . A Transformer-based encoder network takes these object label features, Obj = {obj 1 , obj 2 , ..., obj m }, and embeds them into the features space shared by all input modalities, H obj = f enc (Obj, θ enc obj ), where θ enc obj refers to the parameters for this object encoder. We do not apply positional embeddings because the relative positions of object labels are irrelevant. Web Entity Label Encoder For modeling web entity labels, we experiment with two modeling variants that consider either (i) the web entity type, or (ii) the web entity surface tokens. For (i), we obtain entity types by using the Google Knowledge Graph (KG) Search API to match the web entity names to KG entries. Each of these types is subsequently represented by a trainable embedding vector. The model is trained to predict captions with entity types, which during postprocessing are substituted by the highest scored web entity label of the predicted type. If no such typed label exists, we use the generic name of the type itself (e.g., ""film""). For variant (ii), the model directly attempts to model and generate a caption containing the surface tokens. In this case, the entity type is still provided as an input to the model as additional source of information. These input representations are constructed by summing up a trainable segment embedding with the subtoken embeddings and the type embedding . A Transformer encoder network takes these web entity features, WE = {we 1 , we 2 , ..., we n }, and embeds them into the feature space shared by all input modalities, H we = f enc (WE, θ enc we ), where θ enc we refers to the parameters for the web entity encoder. Similar to object labels, positional embeddings are not applied to the sequence of web entity labels."
"Classifiers: In <REF>, Naive Bayes refers to the classifier from . Crimson Hexagon refers to sentiment labels -Category and Emotion -from Crimson Hexagon. We also benchmarked the pre-trained classifier from Davidson et al. . Perspective API refers to the public toxicity scoring API provided by Jigsaw . We also trained our own model, which combined a pre-trained BERT embedder  and an abuse-specific embedding trained from scratch. For details see F."
"Lack of human-labeled data for NLU and other natural language processing (NLP) tasks results in poor generalization capability. To address the data sparsity challenge, a variety of techniques were proposed for training general purpose language representation models using an enormous amount of unannotated text, such as ELMo <REF> and Generative Pre-trained Transformer (GPT) . Pre-trained models can be fine-tuned on NLP tasks and have achieved significant improvement over training on task-specific annotated data. More recently, a pretraining technique, Bidirectional Encoder Representations from Transformers (BERT) , was proposed and has created stateof-the-art models for a wide variety of NLP tasks, including question answering (SQuAD v1.1), natural language inference, and others."
"In NLP, Neural language model pre-training has shown to be effective for improving many tasks <REF>. Transformer  is based solely on the attention mechanism, and dispensing with recurrent and convolutions entirely. At present, this model has received extensive attentions and plays an key role in many neural language models, such as BERT , GPT  and Universal Transformer . However, in Transformer based model, a lot of model parameters may cause problems in training and deploying these parameters in a limited resource setting. Thus, the compression of large neural pre-training language model has been an essential problem in NLP research."
"Neural networks are known to be best exploited when trained on large-scale data. It has been demonstrated that utilizing language representation models pre-trained with large-scale data is effective for various tasks. For example, recent studies have shown a significant improvement using large-scale data to train large deeper models for natural language understanding tasks <REF>."
"Deep Neural Networks (DNNs) are able to achieve stateof-the-art performance in many cognitive applications, including computer vision <REF>, speech recognition , and natural language processing . The architectures of DNNs are continuously becoming deeper and wider with many new operations and activation function designs being constantly invented. Such dynamics make manual design of DNNs very challenging, especially when extensive domain expertise and experience are needed."
"• semantic features extracted from the pre-trained language model BERT <REF> 12 , • spatial features based on the bounding box coordinates of the semantic entity, • meta features that encode the length of the sequence. The resulting input feature dimension for each entity is 733. Each semantic entity is then independently passed through an MLP with 2 hidden layers and 500 units each with ReLu activation. The last layer is a softmax classifier to derive the class label. Note that we are testing the algorithms by assuming that we know the optimal word grouping, word location and textual content. In this way, we only assess the specific task. Results are shown in TableVI. 3) Entity linking: We re-use the semantic entity input features built for the entity labeling task. We approach the entity linking task as a binary classification task (i.e., whether or not a link exists). We simply concatenate the feature representation of each semantic entity for all the possible pairs in the form. We then pass it through a MLP with 2 hidden layers and 500 hidden units with ReLu activation."
"The important question still remains on extracting highquality and more meaningful representations-how to seize the semantic, syntactic and the different meanings in different context-embedding from Language Models (ELMo),by <REF>, is newly-proposed in order to tackle that question. ELMo extracts representations from a bidirectional Long Short Term Memory (LSTM),by , that is trained with a language model (LM) objective on a very large text corpus. ELMo representations are a function of the internal layers of the bi-directional Language Model (biLM) that outputs good and diverse representations about the words/token (a convolutional neural network over characters). ELMo is also incorporating character n-grams, as in fastText, but there are some constitutional differences between ELMo and its predecessors. Likewise, BERT, by , is a method of pre-training language representations that is trained,using a general-purpose ""language understanding"" model on a large text corpus in an unsupervised manner. Therefore, models, like ELMo and BERT, are contextual uni-or bi-directional models, which generate a representation of each word that is based on the other words in the sentence."
The success of pretrained language models for natural language understanding <REF> has led to a race to train unprecedentedly large language models . These large language models have the potential to generate textual output that is indistinguishable from human-written text to a non-expert reader. That means that the advances in the development of large language models also lower the barrier for abuse.
"Recently, embedding representations have been widely used in almost all AI-related fields, from feature maps <REF> in computer vision, to word embeddings  in natural language processing, to user/item embeddings  in recommender systems. Usually, the embeddings are highdimensional vectors. Take language models for example, in GPT  and Bert-Base model , 768-dimensional vectors are used to represent words. Bert-Large model utilizes 1024-dimensional vectors and GPT-2  may have used even higher dimensions in their unreleased large models. In recommender systems, things are slightly different: the dimension of user/item embeddings are usually set to be reasonably small, 50 or 100. But the number of users and items is on a much bigger scale. Contrast this with the fact that the size of word vocabulary that normally ranges from 50,000 to 150,000, the number of users and items can be millions or even billions in large-scale real-world commercial recommender systems ."
"In this work, we extend this post-decoding clustering idea in three key ways. First, we make use of sentence-level embeddings which leverage the pre-trained language representations from the Bidirectional Encoder Representations from Transformers (BERT) <REF>. 4 Second, after clustering,  took the sentence closest to the centroid of each cluster as the representative candidate; we instead choose the highest-ranked candidate (according to loglikelihood) from each cluster to ensure the best candidates are still selected. Finally, after performing standard K-means clustering, we found that it was often the case that some clusters contained large numbers of good candidates, while others contained very few candidates that are also either ungrammatical or otherwise inferior. Thus, in our implementation, we remove clusters containing two or fewer sentences, and then sample a second candidate from each of the remaining clusters, prioritizing selecting candidates from larger clusters first."
"TTS and ASR have long been hot research topics in the field of artificial intelligence and are typical sequence-tosequence learning problems. Recent successes of deep learning methods have pushed TTS and ASR into end-to-end learning, where both tasks can be modeled in an encoderdecoder framework with attention mechanism 2 . CNN/RNN based models are widely used in TTS and ASR <REF>. Recently, Transformer  has achieved great success and outperformed RNN or CNN based models in many NLP tasks such as neural machine translation and language understanding . Transformer mainly adopts the self-attention mechanism to model interactions between any two elements in the sequence and is more efficient for sequence modeling than RNN and CNN , especially when the sequence is extremely long. Considering the lengths of the speech sequence as well as the character or phoneme sequence are usually long, we use Transformer as the basic encoder-decoder model structure for both TTS and ASR in this paper."
"Alternatively, substantial work has shown that pre-trained models on large corpus are beneficial for text classification and other NLP tasks, which can avoid training a new model from scratch. One kind of pre-trained models is the word embeddings, such as word2vec <REF> and GloVe , or the contextualized word embeddings, such as CoVe  and . These word embeddings are often used * Corresponding author as additional features for the main task. Another kind of pre-training models is sentencelevel.  propose ULMFiT, a fine-tuning method for pre-trained language model that achieves state-of-the-art results on six widely studied text classification datasets. More recently, pre-trained language models have shown to be useful in learning common language representations by utilizing a large amount of unlabeled data: e.g., OpenAI GPT  and BERT . BERT is based on a multi-layer bidirectional Transformer  and is trained on plain text for masked word prediction and next sentence prediction tasks."
"During this exploration, we investigate different methods for tweet classification. In the first task, we label tweets as offensive or not. In the second we predict whether an offensive tweet is targeted at an individual or not. Finally, in the third task, we perform target identification by classifying targetted tweets as targeted at a group or at an individual. All three tasks are classification problems to which we explored two main solution classes. Firstly, we computed feature vectors -using naive methods, and then with the use of Word2Vec <REF>. We later fed those feature representations into simple scikit-learn methods. Secondly, we moved on to end-to-end machine learning methods that were able to classify without extracting vector features beforehand. We compared performance of CNN and LSTM as well as effectiveness of BERT  and FastText  models."
"For the purpose of the task, we extended the Marian toolkit with fp16 training, BERT-models <REF> and multi-task training. Similar to  we use mixed-precision training with fp16, an optimizer delay of 16 before updating the gradients. We train on 8 Voltas with 16GB each. Training of one model takes between 2 and 4 days on a single machine. In terms of words per second we reach about 180K target words per second for 6-layer sentence-level systems and 120K target labels for 6-layer document-level systems with long sequences.  summarizes our experiments with a single transformer model. We also recomputed numbers for a single model from our WMT18 submission, and quoted results from FAIR's submission where available. Our WMT18 model used a combination of data-filtering and about 10M ""clean"" backtranslated sentences. Transformer models are the same. It seems that the data-quality of the EnglishGerman training data (in particular of Paracrawl) improved from WMT18 to WMT19 as we are not seeing the strongly detrimental effects of adding unfiltered Paracrawl data to the training data mix anymore. Data-filtering still improves the results, but apparently only on the originally German side. Since there is barely any loss on the originallyEnglish side we hope this shows a general improvement in fluency or a domain-adaptation effect due the language model scores used in filtering."
"LALE offers three combinators for arranging operators into pipelines: >>, &, and |. The >> combinator behaves like scikit learn's make_pipeline. The & combinator behaves like scikit learn's make_union, except without concatenating the features at the end. Instead, LALE provides a separate ConcatFeatures operator to increase flexibility. The | combinator implements operator choice and does not have an equivalent in scikit learn. Users can also write | as make_choice. <REF> illustrates how to perform lifecycle transitions manually for a text classification example. Line 1 imports BERT , a text embedding based on neural networks, in this case pre-trained and implemented in PyTorch . Line 2 imports the scikit-learn logistic regression as LR. Lines 3-5 configure the hyperparameters of both BERT and LR and arrange them into a pipeline using the >> combinator. Note the argument LR.solver.sag is a Python enumeration, which LALE auto-generates from the same JSON schema used for search spaces. Lines 6-7 visualize the computational graph. LALE reflects the name used in the source code in the visualization, e.g., LR instead of LogisticRegression. Node colors indicate lifecycle states, here white for trained and light blue for trainable. It is a static error to call predict or transform on an operator unless it is trained. Line 8 trains the pipeline, resulting in a new pipeline trained that is distinct from the original trainable. Keeping these in separate Python variables and objects makes it easier to track lifecycle states and should also help parallel or cloud-based execution and prevent accidental overwrite, e.g., during k-fold cross validation. Calling predict in Line 9 is valid. Lines 10-11 show that trained is fully trained (all white) and consistent with trainable (same graph topology).  Lines 1-5 import a few more operators. Line 6 arranges them in a pipeline. But unlike the earlier example, it does not manually bind all operator selections, nor does it manually bind hyperparameters. We refer to the lifecycle state where these properties are still free"
"CloudScan is a learning based invoice analysis system <REF>. Aiming to not rely on invoice layout templates, CloudScan trains a model that could be generalized to unseen invoice layouts, where a model is trained either using Long Short Term Memory (LSTM) or Logistic Regression (LR) with expert designed rules as training features. This turns out to be extremely similar with solving a Named Entity Recognition (NER) or slot filling task. For that reason, several models can be employed, e.g., the Bi-directional Encoder Representations from Transformers (BERT) model is a recently proposed state of the art method and has achieved great success in a wide of range of NLP tasks including NER . However, the NER models were not originally designed to solve the key information extraction problem in SROIE. To employ NER models, text words in the original document are aligned as a long paragraph based on a line-based rule. In fact, documents, such as receipts and invoices, present with various styles of layouts that were designed for different scenarios or from different enterprise entities. The order or word-to-word distance of the texts in the line-base-aligned long paragraph tend to vary greatly due to layout variations, which is difficult to be handled with the natural language oriented methods. Typical Examples of documents with different layouts are illustrated in ."
"Proponents of machine learning claim human parity on tasks like reading comprehension <REF> and commonsense inference . Despite these successes, many evaluations neglect that computers solve natural language processing (NLP) tasks in a fundamentally different way than humans."
"A ention mechanism have recently been successfully applied in the Natural Language Processing eld <REF>[2] . e a ention module can well capture the long-term dependencies at a position which allow models to automatically focus on important parts of inputs. e strong strength of it is the ability to parallel implementation. In NLP eld, there is a recent trend of replacing recurrent neural networks by a ention models. Transformer  proposed multihead a ention module, which outperforms than state-of-the-art methods in Neural Machine Translation. It has the ability to focus on di erent relation from di erent heads at di erent words."
"To tackle the above three challenges, this paper propose Inpatient2Vec, a novel medical RL approach for inpatients. We aim to learn three kinds of representations: (medical) activity, (hospital) day and diagnosis, which can not only cover the core data characteristics of inpatients, but also satisfy the requirements for various analyzing applications of inpatients. Inpatient2Vec is an extension of Bidirectional Encoder Representations from Transformers (BERT) <REF> that contains two learning tasks. The first one is masked activity prediction that using a percentage of activities in a day to predict the other activities in the day. Co-occurrence in the unordered set is utilized in this task. The second one is next day activity prediction that using the pre-days of an inpatient to predict the activities in the next day by a bi-directional LSTM ].This task highlights the temporal relations between ordered days. We design a Transformer-based network , which combined activity, day and diagnosis representations, for the two learning tasks. In this network, the diagnosis plays a guidance role for the days through a time-aware mechanism, and the contributions of different activities in a day are distinguished by an attention mechanism."
"We introduce conditional masked language models (CMLMs), which are encoder-decoder architectures trained with a masked language model objective <REF>. This change allows the model to lean to predict, in parallel, any arbitrary subset of masked words in the target translation. We use transformer CMLMs, where the decoder's self attention  can attend to the entire sequence (left and right context) to predict each masked word. We train with a simple masking scheme where the number of masked target tokens is distributed uniformly, presenting the model with both easy (single mask) and difficult (completely masked) examples. Unlike recently proposed insertion models , which treat each token as a separate training instance, CMLMs can train from the entire sequence in parallel, resulting in much faster training."
"Recently, bidirectional LMs (biLMs) have achieved significant success in many applications of the natural language processing <REF>. In speech recognition, there have also been several studies that use biLMs to capture the full context rather than just the previous words . Even though bidirectional networks are superior to unidirectional ones in many applications from phoneme classification  to acoustic modeling , the biLMs for ASR did not show their excellence compared to the uniLMs when applying the LMs to the rescoring. This is because there is no interaction between the past and future words in their biLMs, although the words on both sides are used to predict the current word. Namely, the forward and backward representations are not be fused in their models, and it may limit the biLM's potential. Furthermore, because their biLM architectures are restricted on one encoding layer, the capacities of the LMs are insufficient to model complex patterns of the human language. Similar issues have been addressed in recent studies on pre-training the language representation models ."
"To capture semantic dependency among words, researchers adopted deep learning models in text classification task due to its strong ability of representation. The popular deep models include CNN <REF>, GRU , RNN , LSTM , Bi-LSTM , BERT  and several combination networks . Even though they have achieved great success in traditional NLP tasks, few work is designed for XMTC."
".@AP I demand you retract the lie that people in #Ferguson were shouting ""kill the police"", local reporting has refuted your ugly racism The approach followed in our work builds on recent advances in language representation models. We fine-tune the pre-trained end-to-end Bidirectional Encoder Representations from Transformers (BERT) model <REF>, while using the discussion's source post, target's previous post and the target post itself as inputs to determine the rumour stance of the target post. Our implementation is available online. 1 2 Related Work Previous SemEval competitions: In recent years, there were two SemEval competitions targeting the stance classification. The first one focused on the setting in which the actual rumour was provided . The organizers of SemEval-2016 Task 6 prepared a benchmarking system based on SVM using hand-made features and word embeddings from their previous system for sentiment analysis , outperfoming all the challenge participants."
"Overall, 8 methods from 7 different participants were submitted for the 3 proposed tasks in the ST-VQA challenge. All the methods followed an encoder-decoder architecture, which now is the de facto choice for Image Captioning and VQA problems. Specifically, the submitted methods are mostly based on the Bottom-Up and Top-Down attention model <REF> architecture. Additionally, most of the methods employ BERT  which is an off-the-shelf embedding method for encoding the questions or the text tokens predicted by an OCR model. A short description of each method can be found in ."
"In this paper, we focus on sentence-level generation evaluation, and introduce BERTSCORE, an evaluation metric based on pre-trained BERT contextual embeddings <REF>. BERTSCORE computes the similarity between two sentences as a weighted aggregation of cosine similarities between their tokens."
"Moving beyond words towards whole sentence embeddings has been more challenging for disentanglement <REF>). Despite the early success of seq2seq embedding models for neural machine translation , only the recent BERT embeddings  have enabled partial sentence level embedding disentanglement . Disentanglement of the BERT sentence level embeddings with the linear transformation (learned in the supervised manner) revealed the full dependency parsing tree encoded by part of the dimensions. This result shows that the disentangled dimensions might have vastly different functions (what we observe also in our results in Section 4) with only a small fraction of the dimensions involved with the encoding of the parsing tree graph. However, contrary to the postprocessing and supervised approach to disentanglement used in"
"The word vectors are not necessarily initialized at random, like the other parameters of the network. It is actually advantageous to pre-train them in an unsupervised way. Then, word vectors can either be fine-tuned or kept frozen during training. When pre-training is conducted on an external, typically large corpus of unannotated raw text, the approach is known as transfer learning. To this purpose, unsupervised, shallow models such as word2vec <REF> or GloVe  can be applied to big corpora like entire Wikipedia dumps or parts of the Internet 1 . ELMo  and BERT  have also made great strides recently, by showing that it was possible to transfer not only the word vectors but the entire model. After pre-training, the model (or simply its internal representations in the case of ELMo) are used in a supervised way to solve some downstream task. ELMo and BERT have brought great improvement to many natural language understanding tasks."
"There is a parallel research thread around learning effective representations for sentences that can capture sentence semantics <REF>. Skip Thought  is an encoder-decoder model that learns to generate the context sentences for a given sentence. It includes sequential generation of words of the target sentences, which limits the target vocabulary and increases training time. Quick Thought  circumvents this problem by replacing the generative objective with a discriminative approximation, where the model attempts to classify the embedding of a correct target sentence given a set of sentence candidates. Recently, BERT  has proposed a model which predicts the bidirectional context to learn sentence representation using tranformer .  proposes the Input-Response model that we have evaluated in this paper. However, unlike these works, we apply a CharCNN  in our encoder to deal with the problem of learning similar representations for phrases that are orthographic variants of each other."
"The best system for Subtask B was by team AUTOHOME-ORCA (Autohome Inc. and Beijing University of Posts and Telecommunications), who used BERT <REF>."
"Our layer allows us to tackle problems where current architectures underfit given the vast amount of available data, or when they are too slow to work in practice. We thus focus on the language modeling task, integrating our memory within the popular transformer architecture <REF>. This choice is motivated by the success of BERT  and GPT-2 , which demonstrated that increasing the capacity of large models directly translates to large improvements in language modeling, which in turn translates to better performance in both language understanding tasks  and text generation . Overall, our paper makes the following contributions:"
"To test our approach on nested NER, we evaluate it on the ACE 2005 corpus (LDC2006T06) where it achieves a state-of-the-art F1 score of 74.6. This is further improved with contextual embeddings <REF> to 82.4, an overall improvement of close to 8 F1 points against the : Trained model's representation of nested entities, after thresholding the merge values, M (see section 2.1). Note that the merging of "", to"" is a mistake by the model. previous best approach trained on the same data, . Our approach is also 60 times faster than its closest competitor. Additionally, we compare it against BiLSTM-CRFs , the dominant flat NER paradigm, on Ontonotes (LDC2013T19) and demonstrate that its ability to predict nested structures does not impact performance in flat NER tasks as it achieves comparable results to the state of the art on this dataset."
"Our framework also provides a seedbed for studying compositionality in natural language. Based on the LS-Tree value, we develop a novel method for quantifying interactions between sibling nodes on a parse tree captured by the target model, by exploiting Cook's distance in linear regression <REF>. We show that the proposed algorithm can be used to analyze several aspects of widely-used NLP models, including nonlinearity, the ability to capture adversative relations, and overfitting. In particular, we carry out a series of experiments studying four models-a linear model with Bag-Of-Word features, a convolutional neural network , an LSTM , and the recently proposed BERT model ."
"For the NLI and RQE tasks, we use transfer learning on prevalent pre-trained models like BERT <REF> and MT-DNN . These models play a pivotal role to gain deeper semantic understanding of the content for the final task (filtering and re-ranking) of the challenge . Besides using usual techniques for candidate answer selection and re-ranking, we use features obtained from NLI and RQE models. We majorly concentrate on the novel multi-task approach in this paper. We also succinctly describe our NLI and RQE models and their performance on the final leaderboard."
) and <REF>) have applied language modeling and transfer to a variety of academic text understanding problems on the GLUE Benchmark.
"Complementary to the technique of adapting individual word embeddings is the design of deeper sequence models for task-to-task transfer. <REF>;  propose multi-granular transfer of sentence and word representations across tasks using Universal Sentence Encoders. ELMo  trains a multi-layer sequence model to build a contextsensitive representation of words in a sentence. ULMFiT ) present additional tricks such as gradual unfreezing of parameters layer-by-layer, and exponentially more aggressive fine-tuning toward output layers.  propose a deep bidirectional language model for generic contextual word embeddings. We show that our topic-sensitive embeddings provide additional benefit even when used with contextual embeddings."
"It is now standard practice in NLP to derive sentence representations using neural sequence models of various kinds <REF>. However, we do not yet have a firm understanding of the precise content of these representations, which poses problems for interpretability, accountability, and controllability of NLP systems. More specifically, the success of neural sequence models has raised the question of whether and how these networks learn robust syntactic generalizations about natural language, which would enable robust performance even on data that differs from the peculiarities of the training set."
"3. We find that a fine-tuned version of the recent large-scale language model, BERT <REF>, performs significantly better than other methods on KNOWREF, although with substantial room for improvement to match human performance."
"In addition to a trained attacker, we also use a conditional language model, BERT <REF>. 4 BERT is based on the Transformer model of , and uses a bidirectional encoder to obtain ""contextual"" embeddings for each word in a given sentence. We use the BERT model by masking out each obfuscated word, and then predicting the masked word similar to the ""masked language task"" that is mentioned by . This means that the embeddings in each position are fed into a softmax function to predict the missing word. We use the bert-base-uncased model among the available BERT models."
"Deep Neural Networks (DNNs) have led to significant advances in the fields of computer vision <REF>, speech processing  and natural language processing . To be effective, supervised DNNs rely on large amounts of carefully labeled training data. However, it is not always realistic to assume that example labels are clean. Humans make mistakes and, depending on the complexity of the task, there may be disagreement even among expert labelers. To support noisy labels in data, we need new training methods that can be used to train DNNs directly from the corrupted labels to significantly reduce human labeling efforts.  perform an extensive study on the effect of label noise on classification performance of a classifier and find that noise in input features is less important than noise in training labels."
"Natural Language Sentence Matching (NLSM) aims at comparing two sentences and identifying the relationships <REF>, and serves as the core of many NLP tasks such as question answering and information retrieval . Natural Language Inference (NLI)  and Semantic Textual Similarity (STS)  are both typical NLSM problems. A large number of publicly available datasets have benefited the research to a great extent  * Equal contributions from both authors. This work was done when Guanhua Zhang was an intern at Tencent.  , etc. These datasets provide resources for both training and evaluation of different algorithms . However, most of the datasets are prepared by conducting procedures involving a sampling process, which can easily introduce a selection bias . It would get even worse when the bias can reveal the label information, resulting in the ""leakage features,"" which are irrelevant to the content/semantic of the sentences but are predictive to the label. One example is the QuoraQP, a dataset on classifying whether two sentences are duplicated (labeled as 1) or not (labeled as 0), which has been widely used to evaluate STS models . In QuoraQP, three leakage features have been identified, including S1 freq, the number of occurrences of the first sentence in the dataset; S2 freq, the number of occurrences of the second sentence; and S1S2 inter, the number of sentences that are paired with both the first and the second sentences in the dataset for comparison.  shows the distributions of normalized (negative) Word Mover's Distance (WMD)  and normalized leakage features versus the labels in QuoraQP. The features are all normalized to their quantiles. As illustrated, the leakage features are more predictive than the WMD, as the differences between the distributions of positive and negative pairs are more significant. Moreover, combining S1 freq and S2 freq can make even more accurate predictions as illustrated in , where we calculate the averages of the labels under different S1 freq and S2 freq. We find that when both features' values are large, the pairs tend to be duplicated (marked in red), while when one is large and the other is small, the pairs tend to be not duplicated (marked in blue)."
"We provide two baselines for DiscEval: prediction of the majority class, and a fastText classifier. The fastText classifier <REF> has randomly initialized embeddings of size 10 and default parameters otherwise. Embeddings size was picked among {10, 100} according to DiscEval development set accuracy. When the input is a sentence pair, words have distinct representations for their occurrences in first and second sentence (e.g. cat s 1 and cat s 2 for the word cat) As another reference, we evaluate BERT  base uncased model; During evaluation fine-tuning phase, we use 2 epochs and HuggingFace script 6 default parameters otherwise. We also perform experiments with Supplementary Training on Intermediate Labeled-data Tasks (STILT) . STILT is a further pretraining step on a datarich task before the final fine-tuning evaluation on the target task. We finetune BERT on three of such tasks:"
"As a special case of attributed networks, graphs of documents bring together the fields of natural language processing (NLP) and network embedding. A wide variety of unsupervised learning algorithms to represent words and documents have been proposed, from the well-known bag-of-word model <REF> to the recently introduced attention-based Transformer  adapted for unsupervised pretraining . However, few works have fully explored the interplay between NLP techniques and network embedding. NetPLSA  adapts a topic modelling algorithm by regularizing a statistical topic model with a harmonic regularizer based on a graph structure. It generates topics that reflect the underlying communities of the network, providing cleaner topics than regular statistical models."
"(2) Char-only: where the input words are encoded using a separate singlelayered BiLSTM over their characters; and (3) Word+Char: where the input words are encoded using a concatenation of (1) and (2) 6 . The second architecture uses the fine-tuned BERT model <REF>, with an input format of word-piece tokenization. This model has recently set a new state-of-the-art on several NLP benchmarks, including the sentiment analysis task we consider here. All models are trained and evaluated on the binary version of the sentence-level Stanford Sentiment Treebank  dataset with only positive and negative reviews. We also consider the task of paraphrase detection. Here too, we make use of the fine-tuned BERT , which is trained and evaluated on the Microsoft Research Paraphrase Corpus (MRPC) ."
"The embedding function takes as input question q i and its associated sequence of k i input tokens [w First we consider unordered composition with deep averaging networks . This is the most similar to the ir and linear model since it is based on bag of words features. Next, we consider sequence based composition functions such as recurrent <REF>, long short-term , and gated recurrent  networks. We compare unidirectional and bidirectional versions of these networks . We leave comparing against other sequence models such as attention based models  and large pre-trained language models  to future work."
"For span scores, we apply a hidden size of 250-dimensional feed-forward networks. For dependency biaffine scores, we employ two 1024-dimensional MLP layers with the ReLU as the activation function and a 1024-dimensional parameter matrix for biaffine attention. In addition, we augment our parser with ELMo <REF> and a larger version of BERT  (24 layers, 16 attention heads per layer, and 1024-dimensional hidden vectors) to compare with other pre-trained or ensemble models. We set 4 layers of self-attention for ELMo and 2 layers of self-attention for BERT as . Training Details we use 0.33 dropout for biaffine attention and MLP layers. All models are trained for up to 150 epochs with batch size 150 on a single NVIDIA GeForce GTX 1080Ti GPU with Intel i7-7800X CPU. We use the same training settings as  and  if use BERT."
"Meanwhile, neural language models, such as fastText <REF>, , OpenAI GPT , and Bidirectional Encoder Representations from Transformers (BERT) , that encode words and sentences in fixed-length dense vectors, embeddings, have achieved impressive results on various natural language processing tasks. Such general word/sentence embeddings learned on large text corpora (i.e., Wikipedia) has been used extensively and plugged in a variety of downstream tasks, such as question-answering and natural language inference, , to drastically improve their performance in the form of transfer learning."
"Building effective representations for singletons and pairs is therefore of utmost importance. We attempt to build a vector representation for each instance. The representation should be invariant to the instance type, i.e., a singleton or pair. In this paper we exploit the BERT architecture <REF> to learn instance representations. The representations are fine-tuned for a classification task predicting whether a given instance contains content used in human-written summary sentences (details for ground-truth creation in §4)."
"More recently, unsupervised representation-learning approaches -such as Word2Vec , Paragraph Vectors <REF>, and BERT ) -have become popular for language processing tasks."
"The same trend towards decentralization is expected to affect the field of artificial intelligence (AI), and in particular machine learning, as well. Complex models such as deep neural networks require large amounts of computational power and resources to train. Yet, these large, complex models are being retrained over and over again by different parties for similar performance objectives, wasting computational power and resources. Currently, only a relatively small number of pretrained models such as pretrained VGG <REF>, ResNet , GoogLeNet , and BERT  are made available for reuse."
"Question answering has been a long-standing research problem. Recently, reading comprehension (RC), a challenge to answer a question given textual evidence provided in a document set, has received much attention. Current mainstream studies have treated RC as a process of extracting an answer span from one passage <REF> or multiple passages , which is usually done by predicting the start and end positions of the answer . * Work done during an internship at NTT. The demand for answering questions in natural language is increasing rapidly, and this has led to the development of smart devices such as Alexa."
"task. In recent years, extensive works <REF> have been done in learning the word or sentence representations, but most of them only use a sentence or a few sentences when learning the representation, and the document context can hardly be included in the representation. Hence, we introduce new pre-training methods that take the whole document into consideration to learn the contextualized sentence representation with self-supervision."
"The recent advances in sequential feed-forward networks are not limited to architecture design. Various methods, such as BERT <REF> and  pre-training technique, have demonstrated how models such as the Transformer can improve over RNN pre-training . For translation specifically, work on scaling up batch size , using relative position representations , and weighting multi-head attention  have all pushed the state-of-the-art for WMT'14 EnDe and En-Fr. However, these methods are orthogonal to this work, as we are only concerned with improving the neural network architecture itself, and not the techniques used for improving overall performance."
"Distributed representations of words in the form of word embeddings <REF> and contextualized word embeddings  have led to huge performance improvement on many NLP tasks. However, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human-produced data . In this work, we extend these analyses to the ELMo contextualized word embeddings."
"We implement our knowledge selection model based on the code 2 by <REF> and that 3 by . We set the maximum reasoning length T as 3. We use TransE  to initialize vertex and relation representations in our augmented KG. The embedding size for TransE is set as 768 for compatibility with the setting of BERT 4  embeddings. Here we use BERT to get the representations of input messages for knowledge selection model. We use BiDAF as our MRC module, shown in Equation , and we train the MRC module on the same training set for our knowledge selection model. α and β in Equation  is set as 0.4 and 0.1 respectively. The embeddings of vertices and input messages is fixed during training process. We use Adam optimizer with a minibatch size of 32. The learning rate is 0.001. The model is ran at most 20 epochs. During preparation of v X for each X, if we cannot find any vertex for X, then we take its movie name vertex as v X . This operation helps improve the recall of correct knowledge for X. We implement the knowl-edge aware generation model based on the code of GTTP 3 released by . The word embedding size is set to 300, the vocabulary size is limited to 30000, and for other parameter settings, we follow them. We will make the augmented KG and our code publicly available soon. For baselines, including Seq2Seq, HRED, MemNet, and CCM, we initialize word embeddings with GloVe. We also follow the parameter setting in their original papers for model training."
"Unfortunately, it is a highly challenging task. Although several pre-training methods have shown their superiority in NLP on tasks such as question answering <REF>, they just exploit the sentence context with homogeneous text. They are infeasible in understanding and representing question materials due to following domain-unique characteristics. First, test questions contain coherent heterogeneous data. For example, typical math questions in  comprise of multiple parts with different forms including text (red), image (green) and side information such as knowledge concept (yellow). All these kinds information are crucial for question understanding, which requires us to find an appropriate way to aggregate them for a comprehensive representation. Second, for a certain question, not only should we extract its basic linguistic context, but we also need to carefully consider the advanced logic information, which is a nontrival problem. As shown in , in addition to linguistic context and relations from its content, a test question also contains high-level logic, taking the information of four options into consideration. The right answers are more related to the question meaning compared with the wrong ones, reflecting the unique mathematical logic and knowledge. E.g., to find the right answer (B) in question example 2, we need to focus more on the expression (""AB = AC, . . ."") in text and the related ∠CBE in the image. Third, in practice, the learned question representations should have great accessibility and be easy to apply to downstream tasks such as difficulty estimation. In actual educational tasks, question representations are often used as part of a complex model, which requires the method to have simple yet powerful structure and easy to mix-in into task-specific models."
"Both SGNS and SVD PPMI have been shown to be adequate for exploring historical semantics <REF>. A general downside of existing embedding algorithms other than SVD PPMI is their inherent stochastic behavior during training which makes the resulting embedding models unreliable . Very recently, contextualized word embeddings, such as ELMo  and BERT , have started to establish themselves as a new family of algorithms for word representation. Those methods achieve enhanced performance on many downstream tasks by taking context into account, both during training and testing, to generate an individual vector representation for each individual token. This makes them unsuitable for our contribution, since we address emotion on the type level by creating emotion lexicons."
"There are several ideas to condition the model on a context <REF>. However, most of them are initially designed for LSTMs. For Transformer, we experimented with all these methods (Cf. Section 4). Our results show that it achieves the best performance by simply adding the attribute embedding to the title embeddings. Specifically, we first embed the aspect attribute a 1 and user category attribute a 2 to obtain the attribute representation e a 1 and e a 2 . We average the embedding of the two attributes to obtain a single representation e at t r . Then, given the product title embeddings e = (e 1 , e 2 , . . . , e n ), we add the attribute embedding to the title word embedding e i at each timestamp i. The fused representation is illustrated in . A similar idea has also been used in BERT ."
"Most research on generative models of dialog has built on the baseline introduced by <REF> by incorporating various forms of inductive bias  into their models, whether it be through the training procedure, the data or through the model architecture.  use Maximum Mutual Information (MMI) as the objective function, as a way of encouraging informative agent responses.  proposes to better capture the semantics of dialog with the use of a hierarchical encoder decoder (HRED), comprised of an utterance encoder, a conversational context encoder, and a decoder.  incorporate a number of heuristics into the reward function, to encourage useful conversational properties such as informativity, coherence and forward-looking.  encodes a speaker's persona as a distributed embedding and uses it to improve dialog generation.  simultaneously learn intent modelling, slot filling and language modelling.  enables task-oriented systems to make slot-value-independent decisions and improves out-of-domain recovery through the use of entity indexing and delexicalization.  present Recurrent Entity Networks which use action templates and reasons about abstract entities in an end-to-end manner.  present the Action Matching algorithm, which maps utterances to a cross-domain embedding space to improve zero-shot generalizability.  explore several dialog specific pre-training objectives that improve performance on dowstrean dialog tasks, including generation.  present a hierarchical self-attention network, conditioned on graph structured dialog acts and pre-trained with BERT ."
"Our baseline approach, as with previous zeroshot and fully supervised approaches for error detection, makes use of word embeddings pretrained on billions of tokens. We also experiment with the recently proposed contextualized embeddings of <REF>. Interestingly, this is sufficient to yield a sequence tagger with F 1 and F 0.5 token labeling effectiveness competitive with a baseline bidirectional LSTM fullysupervised model (using standard pre-trained embeddings), despite only being given access to labels at the sentence-level for training. For perspective, these results are competitive with the state-ofthe-art fully supervised approach in the literature from as recently as 2016."
"We also apply the model to CoQA <REF> for conversational question answering. BERT Pretrained models like BERT  and ELMo  have shown great efficacy in many natural language processing tasks. In our toolkit, we use  as embedding layers to provide a strong contextualized representation. Meanwhile, we also include the BERT model for machine reading comprehension, as well as our modified version. The results of the models in our toolkit are listed in Section 4."
"Distributed representations of words are a key component of many natural language processing (NLP) systems. In particular, deep contextualized representations learned using an unsupervised language modeling objective <REF> have led to large performance gains for a variety of NLP tasks. Very recently, several authors have proposed to not only use language modeling for feature extraction, but to fine-tune entire language models for specific tasks . Taking up this idea,  introduced BERT, a bidirectional language model based on a transformer architecture ) that has achieved a new state-of-the-art for several NLP tasks. As demonstrated by , it is possible for language models to solve a diverse set of tasks to some extent without any form of task-specific fine-tuning. This can be achieved by simply presenting the tasks in form of natural language sentences that are to be completed by the model. The very same idea can also be used to test how well a language model understands a given word: we can ""ask"" it for properties of that word using natural language. For example, a language model that understands the concept of ""guilt"" should be able to correctly complete the sentence ""Guilt is the opposite of ."" with the word ""innocence""."
"Recently, neural models pretrained on a language modeling task, such as ELMo <REF>, OpenAI GPT (Radford et al., 2018), and BERT , have achieved impressive results on various natural language processing tasks such as question-answering and natural language inference. In this paper, we describe a simple re-implementation of BERT for query-based passage re-ranking. Our system is the state of the art on the TREC-CAR dataset and the top entry in the leaderboard of the MS MARCO passage retrieval task, outperforming the previous state of the art by 27% (relative) in MRR@10. The code to reproduce our results is available at https://github.com/nyu-dl/ dl4marco-bert"
"even without explicit output space definition, our results are competitive. We also show that models benefit from pre-trained word embeddings, which opens doors for incorporating dynamic word embeddings to state tracking models. Dynamic word embeddings have been shown to improve multiple natural language processing applications <REF>. We believe this work is a step towards more scalable end-to-end learning of task-oriented dialogue systems, which we discuss in the following sections."
"In this paper, we introduce an attention-based fusion network (CombAtt) for the estimation of PHQ-8 score. Experimental results show that (1) fusing all modalities helps in giving the closest estimation of depression level, (2) the text modality plays an important role in the regression process, and (3) CombAtt outperforms previous state-ofthe-art approaches. However, there is still a great margin for improvement. Indeed, as stated in <REF>, a baseline classifier that constantly predicts the mean score of depression provides an RMSE=5.73 and an MAE=4.74. In our future work, we propose to (1) study recent multitask learning architectures such as  and  dig deeper into high-level text representations such as ."
"BERT <REF> is a recently released sequence model used to achieve state-of-art results on a wide range of natural language understanding tasks, including constituency parsing  and machine translation . Early work probing BERT's linguistic capabilities has found it surprisingly robust ."
"We use the Adam optimizer to minimize the cross entropy of the predicted output with the ground truth labels. Training is performed for 150 epochs with early stopping (patience=10) based on accuracy evaluation on the validation set. We use an encoder dropout probability of .1. Test-and validation-time decoding is performed using beam search with a width of 5. Our experiments are built on top of AllenNLP <REF> which uses the PyTorch deep learning framework.  We provide results using each of ELMo , OpenAI GPT1 , BERT base , BERT large , with a comparison against a model that forgoes a contextual word embedding and simply uses GloVe. We leave the contextual word embedding frozen during training and instead allow tuning of the GloVe weights to avoid catastrophic forgetting."
"We also apply the OSFT to interpret the Bidirectional Encoder Representations from Transformers (BERT) model <REF> for text classification. BERT recently set a new state of the art in text classification performance on the GLUE benchmark . It learns multiple layers of attention instead of a flat attention structure , making visualization of its internals complicated. A post-hoc black box interpretability method is therefore more appropriate to understand its predictions, especially since even flat attention visualizations often fail to correspond to model decision semantics ."
"• Multiple task training. Currently NeuronBlocks supports single task training. We plan to support multi-task training soon. • Pre-training and fine-tuning. Deep pre-training models such as ELMo , GPT <REF>, BERT  are new directions in NLP. We will support these models as well. • AutoML. Currently NeuronBlocks facilitates users to build models on top of Model Zoo and Block Zoo. With the integration of AutoML techniques, the toolkit can further support automatic model architecture design for specific tasks and data."
"The increasing amount of data, paired with the exponential progress in the capabilities of hardware and relentless efforts for better methods, has tremendously advanced the development in the fields of deep learning, such as image classification <REF> and machine translation . However, most applications have been greatly limited to situations where large amounts of supervision is available, as labeling data remains a labor-intensive and cost-inefficient exercise. In the meantime, unlabeled data is generally easier to acquire but its direct utilization is yet a central challenging problem. Deep generative models, an emerging and popular branch of machine learning, aims to address these challenges by modeling the high-dimensional distributions of data without supervision."
"The idea of using data harvested via distant supervision as auxiliary supervision signals is inspired by recent progress on multi-task learning: models for auxiliary tasks share hidden states or parameters with models for the main task and act as regularizers. This idea can be seen in recent work using language models as regularizers, such as BERT <REF>,  and OpenAI GPT . In addition, neural models often celebrate performance boost when jointly trained for multiple tasks . For instance,  use sequence-to-sequence model to jointly train machine translation, parsing and image caption generation models.  adopte an alternating training approach for different language pairs, i.e., they optimize each task objective for a fixed number of parameter updates (or mini-batches) before switching to a different language pair.  propose using syntactic tasks to regulate semantic tasks like semantic role labeling.  improve universal syntactic dependency parsing using a multi-task learning approach."
"Deep models have been explored in the context of neural machine translation since the emergence of RNN-based models. To ease optimization, researchers tried to reduce the number of non-linear transitions <REF>   In their approach, the encoder layers are combined just after the encoding is completed, but not during the encoding process. In contrast, our approach allows the encoder layers to interact earlier, which has been proven to be effective in machine translation  and text match . In addition to machine translation, deep Transformer encoders are also used for language modeling . For example,  trained a character language model with a 64-layer Transformer encoder by resorting to auxiliary losses in intermediate layers. This method is orthogonal to our DLCL method, though it is used for language modeling, which is not a very heavy task."
"We model ARC as a sequence labeling task. The input is a topic t and a sentence S = w 1 ...w n . The goal is to select 0 ≤ k ≤ n spans of words each of which corresponding to an argument unit A = w j ...w m , 1 ≤ j, j ≤ m, m ≤ n. Following <REF>, we distinguish between PRO and CON (t should be supported/opposed, because A) arguments. To measure the difficulty of the task of ARC, we estimate the performance of simple baselines as well as current models in NLP, that achieve state-of-the-art results on other sequence labeling data sets ."
"In the future, we will test different ways of training embeddings for misspellings including the extension of the same technique to multi-lingual embeddings. We are going to test deep architectures to combine the n-grams in misspellings to better capture various interdependencies of n-grams and correct versions of words. Finally, we will assess the robustness of both character-based <REF> and context-dependent embeddings ,  with respect to misspellings."
"We investigate the task of video prediction, a particular instantiation of self-supervision <REF> where generative models learn to predict future frames in a video. Training such models does not require any annotated data, yet the models need to capture a notion of the complex dynamics of real-world phenomena (such as physical interactions) to generate coherent sequences."
"The task of reading comprehension, where systems must understand a single passage of text well enough to answer arbitrary questions about it, has seen significant progress in the last few years, so much that the most popular datasets available for this task have been solved <REF>. We introduce a substantially more challenging English reading comprehension dataset aimed at pushing the field towards more comprehensive analysis of paragraphs of text. In * Work done as an intern at the Allen Institute for Artificial Intelligence in  this new benchmark, which we call DROP, a system is given a paragraph and a question and must perform some kind of Discrete Reasoning Over the text in the Paragraph to obtain the correct answer."
"over-provisioned and costly hardware resources compared to the resources needed for inference <REF>. Recent trends in deep learning suggest that this problem will continue to worsen. Larger network architectures are becoming increasingly popular; these highly overparameterized models have achieved great successes in computer vision , natural language understanding , reinforcement learning , and more. However, the size of neural networks is limited by the available memory on the device(s) used for training. For certain datasets, such as video  and high-resolution medical images , the prohibitive memory requirements of current training algorithms often mean that smaller, less expressive models must be used or the inputs must be downsampled to a lower dimensionality, thus destroying information . As data collection systems continue to advance, the dimensionality of real-world datasets continues to grow, leading to an ever more pressing need for the ability to train larger and more complex models. This necessitates the development of more memory-efficient training algorithms."
"We propose an end-to-end model where the retriever and reader components are jointly learned, which we refer to as the Open-Retrieval Question Answering (ORQA) model. An important aspect of ORQA is its expressivity-it is capable of retrieving any text in an open corpus, rather than being limited to the closed set returned by a blackbox IR system. An illustration of how ORQA scores answer derivations is presented in <REF>. Following recent advances in transfer learning, all scoring components are derived from BERT , a bidirectional transformer that has been pre-trained on unsupervised language-modeling data. We refer the reader to the original paper for details of the architecture. In this work, the relevant abstraction can be described by the following function:"
"Recently, there has been much work designing ranking architectures to effectively score query-document pairs, with encouraging results <REF>. Meanwhile, pretrained contextualized language models (such as ELMo  and BERT ) have shown great promise on various natural language processing tasks . These models work by pre-training LSTM-based or transformer-based  language models on a large corpus, and then by performing minimal task fine-tuning (akin to ImageNet )."
"Pre-trained contextualized word embeddings <REF> have become increasingly common in natural language processing (NLP), improving stateof-the-art results in many standard NLP tasks. However, beyond standard tasks, NLP tools are also vital to more open-ended exploratory tasks, particularly in social science. How these types of tasks can benefit from pre-trained contextualized embeddings has not yet been explored."
"With the advance of neural-network language models, e.g. BERT <REF> or , there are new ways how to estimate reducibilities of words. In this paper, we use English model of BERT and explore how a deletion of one word in a sentence changes representations of other words. Our hypothesis is that removing a reducible word (e.g. an adjective) does not affect representation of other words so much as removing e.g. the main verb, which makes the sentence ungrammatical and of ""high surprise"" for the language model."
"This work adopts BERT <REF>) as the base model as it achieves the state-ofthe-art performance on MRC . Although BERT aims to learn contextualized representations across a wide range of NLP tasks (to be task-agnostic), leveraging BERT alone still leaves the domain challenges un-2 http://alt.qcri.org/semeval2016/ task5/."
"We use the BERT <REF> model to learn these representation functions. BERT is a deep neural network with 12 layers that uses an attention-based network called Transformers . We initialize the BERT parameters with the model that is pre-trained for the language modeling task on Wikipedia and fine-tune the parameters on Qulac with 3 epochs. BERT has recently outperformed state-of-the-art models in a number of language understanding and retrieval tasks . We particularly use BERT in our model to incorporate the knowledge from the vast amount of unlabeled data while learning the representation of queries and questions. In addition, BERT shows promising results in modeling short texts."
"Recent research has tried using one-dimensional embedding and implementing RNNs or one-dimensional CNNs to address the TML (Tabular data Machine Learning) tasks, or tasks that deal with structured data processing <REF>, and also categorical embedding for tabular data with categorical features . One-dimensional embeddings such as word2vec , GLoVe , fastText , ELMO , BERT , and Open AI GPT  are widely used in NLP tasks, and as such data scientists have tried to adapt them to TML tasks. These one-dimensional embeddings project each token into a vector containing numerical values. For example, a word2vec embedding  could be a one-dimensional vector that acts like a 300 by 1 array."
"Natural Language Inference (NLI) has attracted considerable interest in the NLP community and, recently, a large number of neural network-based systems have been proposed to deal with the task. One can attempt a rough categorization of these systems into: a) sentence encoding systems, and b) other neural network systems. Both of them have been very successful, with the state of the art on the SNLI and MultiNLI datasets being 90.4%, which is our baseline with BERT <REF>, and 86.7%  respectively. However, a big question with respect to these systems is their ability to generalize outside the specific datasets they are trained and tested on. Recently,  have shown that state-of-the-art NLI systems break considerably easily when, instead of tested on the original SNLI test set, they are tested on a test set which is constructed by taking premises from the training set and creating several hypotheses from them by changing at most one word within the premise. The results show a very significant drop in accuracy for three of the four systems. The system that was more difficult to break and had the least loss in accuracy was the system by  which utilizes external knowledge taken from WordNet ."
"BERT <REF>. Mean and max poolings are used as the latter two strategies. Results of various combinations of BERT type, layer and pooling strategy are illustrated in . It's reasonable that BERT trained on large model surpasses the smaller one in most cases due to the larger model capacity and richer contextual representation. For the pooling strategy, ""mean"" is considered to capture more comprehensive representations of rare words than ""first"" and ""max"", thus better average performances. Additionally, we hypothesize that the higher layers in BERT encode more abstract and semantic features, while the lower ones prefer general and syntax infor-  : F 1 scores on the CoNLL03 and parameter sizes of various models, where ""GRU-384"" indicates the conventional GRU with hidden size of 384, while ""DT2-128"" refers to deep transition RNN with transition number of 2 and hidden size of 128, similarly for ""DT4-256"". mation, which is more helpful for our NER and Chunking tasks. These hypotheses are consistent with results emerged in ."
"Although conceptually attractive, retrieval-based dialogue systems still suffer from data scarcity, as deployment to a new domain requires a sufficiently large in-domain dataset for training the response selection model. Procuring such data is expensive and labour-intensive, with annotated datasets for task-based dialogue still few and far between, as well as limited in size. <REF> Recent work on language modelling (LM) pretraining  has shown that task-specific architectures are not necessary in a number of NLP tasks. The best results have been achieved by LM pretraining on large unannotated corpora, followed by supervised fine-tuning on the task at hand . Given the compelling benefits of large-scale pretraining, our work poses a revamped question for response selection: can we pretrain a general response selection model and then adapt it to a variety of different dialogue domains?"
"Recently, BERT, the pre-trained deep bidirectional Transformer, has shown strong performances on many language processing tasks <REF>. BERT is a very deep language model that is pre-trained on the surrounding context signals in large corpora. Fine-tuning its pre-trained deep network works well on many downstream sequence to sequence (seq2seq) learning tasks. Different from seq2seq learning, previous Neu-IR research considers such surroundingcontext-trained neural models not as effective in search as relevance modeling . However, on the MS MARCO passage ranking task, fine-tuning BERT and treating ranking as a classification problem outperforms existing Neu-IR models by large margins ."
"More recently, deep learning has been utilized for text classification <REF>,  in addition to its more common usages in image classification. State-of-the-art results in several text-based tasks could, for example, be achieved by transfer learning methods like Universal Language Model Fine-tuning (ULMFit)  and the Google research project Bidirectional Encoder Representations from Transformers (BERT)  for the training of language representations, which includes ULMFit and several other methods. The code of BERT and several pre-trained models are also available on GitHub 3 ."
"Recent scientific achievements in Deep Learning (DL) <REF>-  provide many opportunities for the development of novel methods for efficient cyber defense. One of the major breakthroughs in DL is associated with the usage of contextual embeddings in various Natural Language Processing (NLP) tasks. Several methods for embedding words into vectors have been proposed in recent years - . Generally, these methods leverage large datasets of text documents (such as Wikipedia articles) to obtain representations of words as vectors in the Euclidean space from contexts of their appearances in the document corpus. These embedding methods have gained popularity over traditional one-hot encoding in various NLP tasks, because of their ability to project semantically similar words to proximate vectors in the embedding space. Pretrained embeddings can be used to initialize the first layer of a neural network trained to perform a particular task (for example classification of documents to topics), thereby reducing the volume of data required for training."
"Such models have enabled research on a broad suite of new software engineering tools. For example, the ability to automatically quantify the naturalness of software has enabled new tools for autocompletion <REF>, improving code readability , and program repair . Furthermore, recent work in natural langugage processing (NLP)  has shown that LMs and sequence models in general learn useful word embeddings, which can then be used for downstream tasks in the same way as older, word2vec-style embeddings . Such continuous embeddings have formed the foundation of important software engineering tools. Examples of such are suggesting readable function and class names , summarizing source code , predicting bugs , detecting code clones , comment generation , fixing syntactic errors , and variable de-obfuscation . Therefore, improved LMs for code have the potential to enable improvements in a diverse variety of software engineering tools."
"Sentence Pair Representation To model the likelihood of adjacency between two sentences s u and s v of length |u| and |v| respectively, we first compute a hidden representation of the sentence pair using BERT <REF>. This representation allows us to better capture the finegrained contextual information necessary for understanding the relationship between s u and s v . The initial input to the encoder is the concatenation of these sentences:"
"Recent work has also shown that language representation models pre-trained on large unlabelled corpora and fine-tuned onto specific tasks significantly outperform models trained only on taskspecific data. This method of transfer learning is particularly useful in legal NLP, given the lack of labelled data in the legal domain. We thus evaluated <REF>'s state-of-the-art BERT model using published pre-trained weights from bert base (12-layers; 110M parameters) and bert large (24-layers; 340M parameters). 3 However, as BERT's self-attention transformer archi-tecture  only accepts up to 512 Wordpiece tokens  as input, we used only the first 512 tokens of each j i to fine-tune both models.  We considered splitting the judgment into shorter segments and passing each segment through the BERT model but doing so would require extensive modification to the original fine-tuning method; hence we left this for future experimentation. In this light, we also benchmarked Howard and Ruder (2018)'s ULMFiT model which accepts longer inputs due to its stacked-LSTM architecture."
"In this work, we first show that fine-tuning existing LMs on WSCR is a robust method of improving the capabilities of the LM to tackle WSC273 and WNLI. This is surprising, because previous attempts to generalize from the WSCR dataset to WSC273 did not achieve a major improvement <REF>. Secondly, we introduce a method for generating large-scale WSC-like examples. We use this method to create a 2.4M dataset from English Wikipedia 1 , which we further use together with WSCR for fine-tuning the pre-trained BERT LM . The dataset and the code have been made pub-licly available 2 . We achieve accuracies of 72.5% and 74.7% on WSC273 and WNLI, improving the previous best solutions by 8.8% and 9.6%, respectively."
"Data is being generated at an unprecedented scale. Internet scale companies generate terabytes of data every day which needs to be analyzed effectively to draw meaningful insights <REF>. Deep Learning has emerged as a powerful tool for performing this analysis, these algorithms boast state of the art results on complex tasks for vision , language  and intelligent reasoning . Unfortunately, these algorithms need large amounts of data for effective training which takes a substantial amount of time. The first deep learning algorithms that got state of the art results on the ImageNet classification task took a week to train on a single GPU. This speed is no longer sustainable in today's day and age where models need to be trained on data which dwarfs the ImageNet dataset in size. There is an intrinsic need to scale deep learning training in a horizontal manner while also retaining the accuracy of a single GPU model. The speed of this training should ideally decrease linearly with the increase in the number of machines while also being fault tolerant and able to converge under high latency network conditions."
"In this paper, we showed that MTN, a multi-head attention-based neural network, can generate good conversational responses in multimodal settings. Our MTN models outperform the reported baseline and other submission entries to the DSTC7. We also adapted our approach to a visual dialogue task and achieved excellent performance. A possible improvement to our work is adding pre-trained embedding such as BERT <REF> or image-grounded word embedding  to improve the semantic understanding capability of the models. his = 10 due to memory issues with large input sequences."
"Most deep models for answer selection are constructed with similar frameworks which contain an encoding layer (also called encoder) and a composition layer (also called composition module). Traditional models usually adopt convolutional neural networks (CNN) <REF> or recurrent neural networks (RNN)  as encoders. Recently, complex pre-trained models such as BERT  and GPT-2 , are proposed for NLP tasks. BERT and GPT-2 adopt Transformer  as the key building block, which discards CNN and RNN entirely. BERT and GPT-2 are typically pre-trained on a large-scale language corpus, which can encode abundant common knowledge into model parameters. This common knowledge is helpful when BERT or GPT-2 is fine-tuned on other tasks."
"Transformer <REF> which computes a score similarly to the BiLSTM's scorer, except that each bi-LSTM layer is replaced by a either a bidirectional Transformer layer (BiTransf), or a Transformer with causal self-attention (UniTransf). For unidirectional models we use the same averaging technique as with BiLSTM models. For bidirectional models the score is computed via: f (w 1 , ..., w n ) = u h L,1 + b, where h L,1 is the top layer hidden state at the first position (as common practice also in prior work ). BiTransf uses the BERTBase  configuration: 12 bidirectional transformer layers with 768 units and 12-head attention. It is initialized from a publicly available pretrained bert-base-cased model 3 . UniTransf has instead 12 layers with 1024 units and 16 attention heads per layer. Following  we used the language modeling task for pretraining."
"The majority of NER systems treat the task has sequence labelling and model it using conditional random fields (CRFs) on top of hand-engineered features <REF>   : Actions and stack states when processing sentence ""Obama met Donald Trump"". The predicted types and detected mentions are contained in the Output and the entities the mentions refer to in the Entity. . Recently, NER systems have been achieving state-of-the-art results by using word contextual embeddings, obtained with language models ."
"ConvQA is closely related to machine comprehension. High quality datasets <REF> have boosted research progress, resulting in a wide range of MC models . A major difference between ConvQA and MC is that questions in ConvQA are organized in conversations. Thus, we need to model conversation history to understand the current question. Compared to existing methods that prepend history turns  to the current question or mark history answers in the passage , our method can handle longer conversation history and thus is more robust and effective. In addition, our method is conceptually simple and more efficient than FlowQA  that uses complicated recurrent structures. Our method is specifically tailored for BERT , which pre-trains language representations with bidirectional encoder representations from transformers ."
"Extractive summarization is usually modeled as a sentence ranking problem with length constraints (e.g., max number of words or sentences). Top ranked sentences (under constraints) are selected as summaries. Early attempts mostly leverage manually engineered features <REF>. Based on these sparse features, sentence are selected using a classifier or a regression model. Later, the feature engineering part in this paradigm is replaced with neural networks.  propose a hierarchical long short-term memory network (LSTM;  to encode a document and then use another LSTM to predict binary labels for each sentence in the document. This architecture is widely adopted recently . Our model also employs a hierarchical document encoder, but we adopt a hierarchical transformer ) rather a hierarchical LSTM. Because recent studies  show the transformer model performs better than LSTM in many tasks."
"Recently, contextual word embeddings from ELMo <REF> or BERT  have emerged as the successors to traditional embeddings. With this development, word embeddings have become context-sensitive by design and thus more suitable for representing polysemous words. However, as shown by the experiments of (Pilehvar and , they are still insufficient by themselves to reliably detect meaning shifts."
"Neural networks for language processing have advanced rapidly in recent years. A key breakthrough was the introduction of transformer architectures <REF>. One recent system based on this idea, BERT , has proven to be extremely flexible: a single pretrained model can be fine-tuned to achieve state-of-the-art performance on a wide variety of NLP applications. This suggests the model is extracting a set of generally useful features from raw text. It is natural to ask, which features are extracted? And how is this information represented internally?"
"The Transformer architecture <REF> replaces RNN cells with self-attention and point-wise fully connected layers, which are highly parallelizable and thus cheaper to compute. Together with positional encoding, Transformers are able to capture long-range dependencies with vague relative token positions. This results in a coarse-grained sequence representation at sentence level. Recent works such as GPT (or GPT-2)  and BERT  show that the representations learned on large-scale language modeling datasets are effective for fine-tuning both sentence-level tasks, such as GLUE benchmark , and token-level tasks that do not rely on word order dependency in the context, such as question answering and NER."
"Though discourse marker prediction in itself is an interesting and useful task <REF>, discourse markers have often been used as a training cue in order to improve implicit relation prediction     : Accuracy of various models on linguistic probing tasks using logistic regression on SentEval. BShift is detection of token inversion. CoordInv is detection of clause inversion. ObjNum/SubjNum is prediction of the number of object resp. subject. Tense is prediction of the main verb tense. Depth is prediction of parse tree depth. TC is detection of common sequences of constituents. WC is prediction of words contained in the sentence. OddM is detection of random replacement of verbs/nouns by other verbs/nouns. AVG is the average score of those tasks for each model. For more details see  Our work is the first to automatically discover discourse markers from text. More generally, various automatically extracted training signals have been used for unsupervised learning tasks. Hashtags  have been sucessfully exploited in order to learn sentiment analysis from unlabelled tweets, but their availability is mainly limited to the microblogging domain. Language modeling provides a general training signal for representation learning, even though there is no obvious way to derive sentence representations from language models. BERT  currently holds the best results in transfer learning based on language modeling, but it relies on sentence pair classification in order to compute sentence embeddings, and it makes use of a simple sentence contiguity detection task (like QuickThought); this task does not seem challenging enough since BERT reportedly achieves 98% detection accuracy.  showed that the use of SNLI datasets yields significant gains for the sentence embeddings from , which are based on language modeling."
"Recent progress on CoQA Since we first released the dataset in August 2018, the progress of developing better models on CoQA has been rapid. Instead of simply prepending the current question with its previous questions and answers, Huang et al. <REF> proposed a more sophisticated solution to effectively stack single-turn models along the conversational flow. Others (e.g.,  attempted to incorporate the most recent pretrained language representation model BERT    into CoQA and demonstrated superior results. As of the time we finalized the paper (Jan 8, 2019), the state-of-art F1 score on the test set was 82.8."
"Another further improvement of this work would be to expand this analysis to verbal and nominal expressions of emotion, which we hypothesize as also being frequent. In order to obtain meaningful representations for the phrases we focus on, another natural next step is expanding the postprocessing pipeline and including a comparison to other adaptation methods such as counterfitting  <REF>: Evaluation: Spearman's rank correlation between predicted emotion intensity scores and annotated scores on our dataset (T) or the EmoInt dataset (EI). We report results only for the 4 emotions annotated in the EmoInt data. . Presumably, this will also generate additional insights into the aspect that we were only able to show a limited improvement based on retrofitting. Given the recent advances in representing contextualized word embeddings as functions computing dynamically the embeddings for words given their context, we hypothesize and intend to further verify that these embeddings would be a better choice for input to systems that predict intensity scores. It would be interesting to compare models such as word embeddings from language models (Elmo) , bidirectional encoder representations from transformers (BERT) , and generative pre-training OpenAI (GPT)  to the ones we already discussed, since the contextualized embeddings assign a different vector for a word in each given context. These approaches presumably produce a different vector for ""happy"" in the context of ""not"" than in the content of ""very"" or ""completely""."
"In the field of NLP, text is usually converted into Bag of Words (BoW), Term FrequencyInverse Document Frequency (TF-IDF) and, more recently, highly complex vector representations. In fact in the last few years, Word Embeddings have become an essential part of any DeepLearning-based Natural Language Processing system representing the state-of-the-art for a large variety of classification task models. Word Embeddings are pre-trained on a large corpus and can be fine-tuned to automatically improve their performance by incorporating some general representations. The Word2Vec method based on skipgram <REF>, had a large impact and enabled efficient training of dense word representations and a straightforward integration into downstream models.  added subword-level information and augmented word embeddings with character information for their relative applications. Later works  showed that incorporating pretrained embeddings character n-grams features provides more powerful results than composition functions over individual characters for several NLP tasks. Character ngrams are in particular efficient and also form the basis of Facebook's fastText classifier . The most recent approaches  exploit contextual information extracted from bidirectional Deep Neural Models for which a different representation is assigned to each word and it is a function of the part of text to which the word belongs to, gaining state-of-the-art results for most NLP tasks.  achieves relevant results and is essentially a method to enable transfer learning for any NLP task without having to train models from scratch. Regarding the prediction models for text classification, LSTM and RNN including all possible model Attention-based variants  have represented for years the milestone for solving sequence learning, text classification  and machine translation ; other works show that CNN can be a good approach to solve NLP task too . In the last few years Transformer  outperformed both recurrent and convolutional approaches for language understanding, in particular on machine translation and language modeling."
"Our work offers a new perspective on why the heuristic of next sentence prediction used in previous works <REF> are useful auxiliary tasks, while revealing missing ingredients, which we complete in the proposed algorithm. We demonstrate improved perplexity on two established benchmarks, reflecting the positive regularizing effect. We also show that our proposed method can help the model generate higher-quality samples with more diversity measured by reversed perplexity  and more dependency measured by an empirical lower bound of mutual information."
"supervision. In particular, we present a simple way to model the relationship between the visual domain and the linguistic domain by combining three off-the-shelf methods: an automatic speech recognition (ASR) system to convert speech into text; vector quantization (VQ) applied to low-level spatio-temporal visual features derived from pretrained video classfication models; and the recently proposed BERT model <REF> for learning joint distributions over sequences of discrete tokens."
"Thus, Newman argues that the z-score can indeed identify short-and mid-term research trends. <REF> To find promising papers, we calculated the z-score for the papers in our datasets, using a time-window of ±10 days. We ignored papers with less than 4 citations, because we deemed such small numbers unreliable.  shows the distribution of tasks in the top-100 list of cs.CL, with topic labels given in . There are few very interesting patterns: for example, traditional NLP tasks like sequence tagging and parsing are marginalized and make up only 5% of the top papers, while the most prominent tasks dealt with in the top-100 papers are almost all about natural language generation: machine translation, language modeling (where the goal is to predict the next word given the previous text sequence) and generation proper (consisting of subcategories dialogue, question answering, text2SQL generation and summarization). The task speech mostly also deals with generation, namely, generation of a speech signal from written text (text2speech). The only major category not dealing with text generation is 'text representations' which deals with either word embeddings  or sentence embeddings , that is, vector representations of either words or sentences that summarize their semantic and syntactic properties. One speculation is that this latter field, text representations, will now be gradually declining in importance, after having dominated most of the 2010s. The more high-level topic text generation appears to replace it in its leadership role.  shows the method distribution in the top-100 list of cs."
"Despite the impressive success stories behind word representation learning <REF>, further investigations into the learnt representations have revealed several worrying issues. The semantic representations learnt, in particular from social media, have shown to encode significant levels of racist, offensive and discriminative language usage . For example,  showed that word representations learnt from a large (300GB) news corpus to amplify unfair gender biases. Microsoft's AI chat bot Tay learnt abusive language from Twitter within the first 24 hours of its release, which forced Microsoft to shutdown the bot (The Telegraph, 2016).  conducted an implicit association test (IAT)  using the cosine similarity measured from word representations, and showed that word representations computed from a large Web crawl contain human-like biases with respect to gender, profession and ethnicity."
"For language task, we verify our algorithm on Masked Language Model(MLM) task. In each sequence, 15% of the words are replaced with a [MASK] token and MLM model attempts to predict the original value of the masked words from a given context. We choose BERT <REF> as our backbone and adapt hyperparameters from the official implementation * ."
"Our method for TDMS identification resembles some approaches used for textual entailment <REF> or natural language inference (NLI) . We follow the example of  and  who reframe different NLP tasks, including extraction tasks, as NLI problems.  and  have both used NLI approaches for relation extraction. Our work differs in the information extracted and consequently in what context and hypothesis information we model. Currently, one of the best performing NLI models (e.g., on the SNLI dataset) for three way classification is . The authors apply deep neural networks and make use of BERT , a novel language representation model. They reach an accuracy of 91.1%.  exploit denselyconnected co-attentive recurrent neural network, and reach 90% accuracy. In our scenario, we generate pseudo premises and hypotheses, then apply the standard transformer encoder  to train two NLI models."
"Language models are the simplest fully unsupervised generative models for natural language text which are actively used to improve state-of-the-art results in various natural language processing tasks <REF>. Formally, assume we are given a vocabulary of words in a language V = {x 1 , . . . , x M } and a text corpus represented as a sequence of words X = (x j1 , . . . , x j N ), where typically N >> M . We assume that this corpus is generated sequentially from a true conditional next-token distribution P * (X i |X i−1 , . . . , X 1 ) = P * (X i |C i ), where the context random variable is denoted by C i = X <i and its outcome by c i = x j<i . The chain rule formula then gives the full corpus likelihood P * (X 1 , . . . , X N ) = Π N i=1 P * (X i |C i ). Therefore, we view natural language as a set of conditional next-token distributions S = {(c 1 , P * (X|c 1 )), . . . , (c N , P * (X|c N ))}."
"However, more complex scenarios can also be used, such as masking some words in the sentence and predicting them from the context or predicting next sentences from the previous ones <REF>."
"We use a ResNet-152 network <REF> as the image encoder φ I . We use the features (D I = 2048) from a network pretrained on ImageNet classification task with the last layer removed and do not fine-tune the network during learning. We encode a sentence into a stream of vectors by using 300 word-based embeddings that were learned separately for the cleaned sentences for each corpus using the Gensim library 2 . We restrict the length of sentences to 20 words and pad sentences who length is less than 20 words with a pad token. For the text encoder φ T , we use a CNN, on the word embedding, with three parallel convolutional blocks with different stride lengths  and number of filter (512, 256, 256) . The outputs from these blocks are concatenated and used as text features. Recently, there has been a significant improvement in pre-trained approaches for several NLP tasks that encode sentences using deeper and more efficient pipelines e.g. BERT and ELMo . However, since our primary focus was on jointly embedding multimodal content and users in a common geometric space, we opted to keep our architecture simple and focus more on the core learning method. We set the dimensionality of the common embedding space as D = 1024. We use 300 dimensional embeddings for users, which are initialized using the average of word2vec embeddings (learned previously) of all the tweets posted by a user. We found that such initialization helps with faster convergence. For training our model, we use an Adam optimizer with a learning rate of 0.0005, which is dropped by a factor of 10 after every 10 epochs. We use a batch size of 1000 posts and the margin was set to m = 0.2 in all of our experiments. We select model checkpoints for evaluation on the test set based on the performance on the validation set. Since we evaluate our model on 3 different cross-modal retrieval tasks simultaneously, we use a cumulative metric based on the addition of normalized mean median ranks on the three tasks for selecting the best checkpoint. We add the normalized ranks for those retrieval tasks where the regularization parameter is non-zero (λ > 0)."
"We have recently seen great successes in using pretrained language models as encoders for a range of difficult natural language processing tasks <REF>, often with little or no fine-tuning: Language models learn useful representations that allow them to serve as general-purpose encoders. A hypothetical general-purpose decoder would offer similar benefits: making it possible to both train models for text generation tasks with little annotated data and share parameters extensively across applications in environments where memory is limited. Is it possible to use a pretrained language model as a general-purpose decoder in a similar fashion?"
"Transfer learning. Transfer learning could help have better results on small dataset. Upstream unsupervised pre-training can help use less source and time to do the downstream tasks. There are two methods to apply the pretrained language representations to downstream tasks: feature-based approach (eg, ELMO <REF>) and fine-tuning approach (eg, GPT , GPT2 ,BERT ). Feature-based approach includes pre-trained representations as additional features into embeddings. Fine-tuning approach fine-tunes the pre-trained param-eters in the specific downstream tasks. In our work, we use BERT in upstream to do pre-training and CRF in downstream to fine-tune with the task-specific data."
"We perform a comparison of Ctrl with two groups of baselines. The results of the first group are non-CNN based methods. CRF is conditional random fields. IHS RD <REF> and NLANGP  are the best systems from the original challenges . WDEmb  is enhanced CRF with multiple embeddings. LSTM ) is a BiLSTM implementation. BiLSTM-CNN-CRF  is the state-of-the-art named entity recognition system. BERT  fine-tunes pre-trained language model on aspect extraction tasks. The following methods use multi-task learning and opinion lexicon or human annotation are adopted for opinion supervision: RNCRF ) is a recursive neural network and CRF jointed model for aspect and opinion terms co-extraction. CMLA  solves the co-extraction through a multi-layer coupled-attention network. MIN  solves co-extraction, and discriminate sentimental/non-sentimental sentences."
"Commonsense reasoning Datasets that require models to learn to predict relations between situations or events in natural language have been introduced in the recent past. The Story Cloze (also referred to as ROC Stories) involves predicting the correct story ending from a set of plausible endings <REF> while the Situations with Adversarial Generations (SWAG) involves predicting the next scene based on an initial event . Language Modeling based techniques such as the GPT and BERT models get human-level performance on these datasets . They have been less successful on tasks that require clear understanding of how pronouns resolve between sentences and how that interacts with world knowledge. For example, the Winograd Schemas  and challenges derived from that format  have proven difficult for even the most modern machine learning methods (Trinh and Le, 2018) to achieve near-human performance, but the emphasis on pronoun resolution in those challenges leaves room for exploration of other kinds of commonsense reasoning. CQA is a new dataset that consists of 9500 questions with one correct answer and two distractor answers . The authors claim that because all the answer choices are drawn from the same source concept, the dataset requires models to actually infer from the question rather than take advantage of distributional biases. We, however, observed that the current state of this dataset has gender disparity with higher proportion of feminine pronouns used in negative context."
"Notably, it has been reported that these large pretrained language models are difficult to finetune and that many random restarts can be required to achieve optimal performance <REF> ."
"The matching score measured by our knowledge-aware response ranker is defined as p(l = 1|Y, X, K, G). As shown in <REF>(a), our knowledge-aware response ranker consists of four major parts, i.e., the context-response representation module (Encoder), the knowledge representation module (Knowledge Encoder), the knowledge reasoning module (Knowledge Reasoner) as well as the matching module (Matcher). The Encoder module has the same architecture as BERT , it takes the context X and candidate response Y as segment a and segment b in BERT, and leverages a stacked selfattention to produce the joint representation of X and Y , denoted as xy. Each related knowledge knowledge i is also encoded as vector representations in the Knowledge Encoder module using a bi-directional GRU , which can be formulated as k"
"In NLP, BERT pre-trained models have achieved impressive performance on a wide range of downstream tasks <REF>. At the core of BERT is the masked language model, which is a self-supervised prediction task to encourage the model to capture useful domain knowledge about language."
"Does visually situated language learning benefit from the specific architecture of the ∆-RNN, or does the proposal work with state-ofthe-art language models? We applied the same architecture to Gated Recurrent Units (GRU, <REF>, Long Short Term Memory (LSTM, , and BERT . We train these models on text alone and compare to the two variations of the multi-modal ∆-RNN, as described in the previous section. The multimodal GRU, with context information directly integrated, is defined as follows:"
"In order to evaluate our sense vocabulary compression methods, we applied it on a neural WSD system based on a softmax classifier capable of classifying a word in all possible synsets of WordNet (see subsection 2.2). We implemented a system similar to <REF>'s BiLSTM but with a few differences: our output vocabulary only consists of sense tags seen during training (mapped according to the compression method used), we used Transformer encoders instead of LSTM, as described by , with the same parameters as their ""base"" model, and we used BERT contextualized word vectors  in input of our network. We used the model named ""bert-large-cased"" of the PyTorch implementation of BERT 3 , which consists of vectors of dimension 1024, trained on BooksCorpus and English Wikipedia. We trained with mini-batches of 100 sentences, truncated to 80 words, and we used Adam  with a learning rate of 0.0001 as the optimization method."
"Settings Natural language inference reasons about the entailment relationship between a premise sentence and a hypothesis sentence. We use the Stanford Natural Language Inference (SNLI) corpus <REF> and treat the task as a three-way classification task. This dataset contains 549,367 premise-hypothesis pairs for training, 9,842 pairs for developing and 9,824 pairs for testing. We employ accuracy for evaluation. We implement a variant of the word-by-word attention model  using Tensorflow for this task, where we stack two additional bidirectional RNNs upon the final sequence representation and incorporate character embedding for word-level representation. The pretrained GloVe  word vectors are used to initialize word embedding. We also integrate the base BERT    We set the character embedding size and the RNN hidden size to 64 and 300 respectively. Dropout is applied between consecutive layers with a rate of 0.3. We train models within 10 epochs using the Adam optimizer (Kingma and Ba, 2014) with a batch size of 128 and gradient norm limit of 5.0. We set the learning rate to 1e −3 , and apply an exponential moving average to all model parameters with a decay rate of 0.9999. These hyperparameters are tuned according to development performance. Results  shows the test accuracy and training time of different models. Our implementation outperforms the original model where  report an accuracy of 83.50. Overall results show that LRN achieves competitive performance but consumes the least training time. Although LSTM and GRU outperform LRN by 0.3∼0.9 in terms of accuracy, these recurrent units sacrifice running efficiency (about 7%∼48%) depending on whether LN and BERT are applied. No significant performance difference is observed between SRU and LRN, but LRN has fewer model parameters and shows a speedup over SRU of 8%∼21%."
"To generate sentiment-preserved fake reviews, we use a pre-trained GPT-2 NLM <REF>, which is able to generate length variable, fluent, meaningful sentences, to generate reviews and then use a fine-tuned text classifier based on BERT  to filter out undesired-sentiment reviews. Since GPT-2 training data differs from the data used in our experiment (i.e., Amazon reviews  and Yelp reviews ), it may generate reviews with irrelevant topic. We solved this problem by adapting the original GPT-2 model to the two databases we used. Subjective evaluation with 80 participants demonstrated that the fake reviews generated by our method had the same fluency as those written by people. It also demonstrated that it was difficult for the participants to identify fake reviews given that they tended to randomly identify fake reviews as the one most likely to be the real review. However, the use of two countermeasures, the Grover  and the GLTR for detecting text generated by an LM , enabled fake reviews to be accurately identified."
"We introduce BERT-DST 1 , a scalable end-to-end dialogue state tracker, based on the BERT model <REF>, that directly predicts slot values from the dialogue context with no dependency on candidate generation. In our framework, BERT is adopted to produce contextualized representations of dialogue context (Section 3.1), which are used to by the classification and span prediction modules to predict the slot value as none, dontcare or a text span in the dialogue context (Section 3.2, 3.3). The advantages of using BERT as dialogue context encoder include: (1) The contextualized word representations are suitable for extracting slot values from semantic context. (2) Pre-trained on large-scale language modeling datasets, BERT's word representations are good initialization to be fine-tuned to our DST problem (Section 2). Moreover, we employ parameter sharing in the BERT dialogue encoder across all slots, which reduces the number of model parameters. Contextualized language representation can also benefit from more training examples of other slots (Section 3.5). To prevent overfitting, we apply the slot value dropout technique, originally introduced in ]. This step is critical for extracting unseen slot values from their contextual patterns (Section 3.6). Through empirical evaluation, we show the effectiveness of BERT-DST with parameter sharing and slot value dropout. BERT-DST achieves state-ofthe-art performance of 80.1% and 89.6% joint goal accuracy on the benchmark scalable DST datasets Sim-M and Sim-R , and competitive results on the standard DSTC2  and WOZ 2.0  datasets (Section 5)."
"We now carry out empirical studies to validate the efficacy, as well as showcase some problems, of several GORAL strategies. For this we use three datasets, i.e. synth2 <REF>, which is a binary classification dataset crafted to highlight issues with those AL strategies that focus on exploiting ""informative"" samples only (e.g. uncertainty sampling), rt-polarity , a binary sentence classification dataset, and letter , a multi-class image classification dataset. For rt-polarity, we encode every sentence by taking its ""[CLS]"" embedding from BERT .  The full dataset We focus on Multinomial Logistic Regression (MLR) for all the experiments. There has been a lot of research specifically concentrated on AL for logistic regression . Furthermore, with the recent advent of powerful pre-trained models , it is becoming ever more promising that, by simply stacking an additional final layer (typically MLR for classification) on top of those pre-trained networks and fine-tuning that layer's parameters to the given task one can readily obtain well performing models with little work. We include intercepts in the model and select the hyperparameter λ with cross validation. 7"
"In this paper, we propose SDNet, a contextual attention-based deep neural network for the task of conversational question answering. Our network stems from machine reading comprehension models, but has several unique characteristics to tackle contextual understanding during conversation. Firstly, we apply both inter-attention and self-attention on passage and question to obtain a more effective understanding of the passage and dialogue history. Secondly, SDNet leverages the latest breakthrough in NLP: BERT contextual embedding <REF>. Different from the canonical way of appending a thin layer after BERT structure according to , we innovatively employed a weighted sum of BERT layer outputs, with locked BERT parameters. Thirdly, we prepend previous rounds of questions and answers to the current question to incorporate contextual information. Empirical results show that each of these components has substantial gains in prediction accuracy."
"BERT <REF> differs from ELMo and BiLSTM in that it is designed to pre-train deep bidirection representations by jointly conditioning on both left and right context in all layers. The pre-trained BERT representations can be fine-tuned with just one additional output layer to create state of the art models for a wide range of NLP tasks, including MRC."
"In this paper, we address the above concerns of disentangled models for style transfer. Different from them, we propose Style Transformer, which takes Transformer <REF> as the basic block. Transformer is a fully-connected selfattention neural architecture, which has achieved many exciting results on natural language processing (NLP) tasks, such as machine translation , language modeling , text classification . Different from RNNs, Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. Moreover, Transformer decoder fetches the information from the encoder part via attention mechanism, compared to a fixed size vector used by RNNs. With the strong ability of Transformer, our model can transfer the style of a sentence while better preserving its meaning. The difference between our model and the previous model is shown in ."
"Pre-trained language representation models, including feature-based <REF> and fine-tuning  approaches, can capture rich language information from text and then benefit many NLP applications. BERT , as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition , question answering , natural language inference , and text classification ."
"We address the first two challenges and make the following contributions in this paper: (a) We improve on knowledge extraction from the OpenBook present in the dataset. We use semantic textual similarity models that are trained with different datasets for this task; (b) We propose natural language abduction to generate queries for retrieving missing knowledge; (c) We show how to use Information Gain based Re-ranking to reduce distractions and remove redundant information; (d) We provide an analysis of the dataset and the limitations of BERT Large model for such a question answering task. The current best model on the leaderboard of OpenBookQA is the BERT Large model <REF>. It has an accuracy of 60.4% and does not use external knowledge. Our knowledge selection and retrieval techniques achieves an accuracy of 72%, with a margin of 11.6% on the current state of the art. We study how the accuracy of the BERT Large model varies with varying number of knowledge facts extracted from the OpenBook and through IR."
"Pre-trained word embeddings are crucial to many NLP tasks. Traditional methods such as Skipgram, Cbow, and Glove <REF>) take use of large text corpora to train an upsupervised prediction model based on contexts and learn a high dimensional distributed representation of each token. On the contrary, recently proposed pre-training methods integrate attention-based techniques with deep neural architectures, aiming to learn higher quality token representation that incorporates syntactic and semantic information from the surrounding contexts. and then the model is fine-tuned to adapt to a downstream supervised task. BERT ) is a bi-directional pre-training model backboned by the Transformer Encoder , a deep hybrid neural network with feed forward layers and self-attention layers which we have briefly discussed in section 3.3. During pre-training, one task is to learn a bidirectional masked language model, meaning that a small percent of tokens in a sentence are masked and the goal is to predict these tokens based on their context. The other task is binary next sentence prediction, where two spans of texts are sampled from the corpora and the model is trained to predict whether they are contiguous. As discussed in Section 3.3, each token redistributes attention across the sequence and reconstruct its interaction with other tokens in a selfadaptive manner as the training proceeds, aiming to learn its contextual representation based on the entire sequence."
"Proposed Method: Since this environment poses a high level of difficulty, we argue that incorporating the shortest path information (shortest path from each room to the nearest exit) in the DQN model(s) by transfer learning and pretraining the DQN neural network function approximator is necessary. Transfer learning has been used extensively in computer vision tasks for many years, recently vastly expanded for many computer vision problems in <REF>. Lately, it has been utilized in Natural Language models , . In reinforcement learning, pretrained models have started to appear as well , . In fact, we use the convergence analysis of , which provides a general theoretical perspective of task transfer learning, to prove that our method guarantees convergence. In this paper, we present a new class of pretrained DQN models called Q-matrix Pretrained Deep Q-Networks (QMP-DQN). We employ Q-learning to learn a Q-matrix representing the shortest paths from each room to the exit. We perform multiple random episodic starts and -greedy exploration of the building model graph environment. Q-learning is applied on a pretraining instance of the environment that consists of only the building model graph. Then, we transfer the Q-matrix to a DQN model, by pretraining the DQN to reproduce the Q-matrix. Finally, we train the pretrained DQN agent on the complete fire evacuation task. We compare our proposed pretrained DQN models (QMP-DQN) against regular DQN models and show that pretraining for our fire evacuation environment is necessary. We also compare our QMP-DQN models with state-of-the-art Reinforcement Learning algorithms and show that off-policy Q-learning techniques perform better than other policy based methods as well as actor-critic models. Finally, in Section 5, we show that our method can perform optimal evacuation planning on a large and complex real world building model by dealing with the large discrete action space in a new and simple way by using an attention based mechanism."
"We take BERT <REF> as the default backbone network, and explore two research questions. First, we make an elaborate comparison between tagging-based models and span-based models. Second, following previous works , we compare the pipeline, joint, and collapsed models under the span-based labeling scheme. Extensive experiments on three benchmark datasets show that our models consistently outperform sequence tagging baselines. In addition, the pipeline model firmly improves over both the joint and collapsed models. Source code is released to facilitate future research in this field 1 . 1 https://github.com/huminghao16/SpanABSA"
"The architecture of our transformer model is identical to the base model described in <REF>, which has 6 layers for both encoder and decoder, 512 hidden units in each layer, 8 attention heads and 2048 hidden units in the feedforward layers. We train with a batch size of 4096 tokens for a maximum of 30 epochs. We use Adam (Kingma and Ba, 2014) with a learning rate of 10 −3 , β 1 = 0.9, β 2 = 0.998, L2 weight decay of 0.01, learning rate warmup over the first 8,000 steps, and linear decay of the learning rate. We use a dropout probability of 0.1 in all layers. Our implementation uses the OpenNMT framework ; training takes place on four V100 GPUs. To avoid overfitting, we monitor the BLEU scores of the training and develop- ment sets and stop training when their difference is greater than four points. Once the model is trained, for each document in the corpus, we predict 10 queries using top-k random sampling  and append them to the document. We do not put any special markup to distinguish the original document text from the predicted queries. The expanded documents are indexed, and, for each query, we retrieve a ranked list of documents using BM25 . We optionally re-rank these retrieved documents using BERT  as described in ."
"Three types of neural networks (NN) comprise the most prominent DL workloads by representing 95% of the data-centers's demands <REF>: i) Recurrent Neural Networks (RNN)  with the socalled Long Short-Term Memory (LSTM)  networks being the most popular variation, ii) Convolution Neural Networks (CNN) , and iii) Multi-Layer Perceptrons (MLP) . Additionally, the contemporary Transformer  and BERT  workloads computationally involve fully-connected layers which also lie in the heart of MLP. All these neural networks can be further associated with two use-cases: training of the underlying NN models (i.e. learning via back-propagation ), and inference (i.e. yielding predictions) based on trained models. Due to the increase of the involved datasets' size and complexity in deep neural networks (DNN), the training and inference tasks require vast amount of computation. Therefore, academia and industry have invested into the development of DL libraries targeting all the aforementioned workloads on various architectures."
"The first step is to convert each word in the sentences into an embedding vector. In this work, we employ the contextualized word representations BERT in <REF> for this purpose. BERT is a pre-trained language representation model with multiple computation layers that has been shown to improve many NLP tasks. In particular, the sentence (x 1 , x 2 , ..., x n ) would be first fed into the pre-trained BERT model from which the contextualized embeddings of the words in the last layer are used for further computation. We denote such word embeddings for the words in (x 1 , x 2 , . . . , x n ) as (e 1 , e 2 , . . . , e n ) respectively."
"To integrate and reason over information from multiple pieces of evidence, we propose a graph-based evidence aggregating and reasoning (GEAR) framework. Specifically, we first build a fully-connected evidence graph and encourage information propagation among the evidence. Then, we aggregate the pieces of evidence and adopt a classifier to decide whether the evidence can support, refute, or is not sufficient for the claim. Intuitively, by sufficiently exchanging and reasoning over evidence information on the evidence graph, the proposed model can make the best of the information for verifying claims. For example, by delivering the information ""Los Angeles County is the most populous county in the USA"" to ""the Rodney King riots occurred in Los Angeles County"" through the evidence graph, the synthetic information can support ""The Rodney King riots took place in the most populous county in the USA"". Furthermore, we adopt an effective pretrained language representation model BERT <REF> to better grasp both evidence and claim semantics."
"The success of an MRC model hinges on its ability to represent both the structures of the questions and contexts, and the relationship between the questions and the contexts. The two most prominent methods in the literature to represent the structures of such kinds of sequential data are attention and recurrence, thus it is not surprising that the best performing models on SQuAD 1.0 leaderboard are attention-based models, e.g. BERT <REF>, and RNN-based models, e.g. RNet, . One prominent attention-based candidate on the leaderboard is QANet, , upon which our work is built. We will now provide a brief introduction to QANet and motivate our decision to work with this model. QANet consists of five functional blocks: a context processing block, a question processing block, a context-query block, a start-probability block and a end-probability block. See figure 2 for a high level representation of the model. Within the context, question and context-query blocks, an embedding encoder of the form shown in  is used repeatedly. This is very similar to the transformer encoder block introduced in , however possesses an additional convolutional layer after positional encoding and before the layernorm and self-attention layer. These additional separable convolutional layers enable the model to capture local structure of the input data. Having passed through the contextquery block, the data is then passed into the two probability blocks, which are both standard feedforward layers with softmax, to calculate the probability of each word being a start-or end-word. For a detailed description of each portion of the model, the reader is referred to the original paper , however further discussion of the components most relevant to our architecture design and experiments can be found in section 4.1."
"• improving model quality with advanced model architectures and techniques at minimum latency cost; • taking advantage of the latest hardware accelerators and continuously optimizing inference and serving. Specifically, we plan to look into an adapted version of the Transformer model where self-attention is applied locally over a fixedsized window of steps during decoding, instead of over all previous decoding steps. In this way, we are hoping to maintain the quality improvement of the Transformer model while mitigating the increase in inference latency. Also, inspired by recent work in language model pre-training <REF>, we are interested in exploring how similar pre-training techniques would benefit our global English model as well as the multilingual model."
"We have repeated these experiments for NER in several different settings, including using only static embeddings, using a non-neural truecaser, and using BERT uncased embeddings <REF>. While the absolute performance of the experiments varied (by about 1 point F1), the conclusion was the same: training on cased and uncased data produces the best results.  comprised of tweets gathered from a broad variety of genres, and including many noisy and informal examples. Since we are testing the robustness of our approach, we use a model trained on CoNLL 2003 data. Naturally, in any cross-domain experiment, one will obtain higher scores by training on in-domain data. However, our goal is to show that our methods produce a more robust model on outof-domain data, not to maximize performance on this test set. We use the recommended test split consisting of section F, containing 3580 tweets of varying length and capitalization quality."
"Node weights. Node weights are derived from similarity scores with regard to tokens in the input question. For entity nodes, we use thresholded similarities from entity-mention dictionaries as explained in Sec. 5.3. For type nodes and relation nodes, the similarity of the node label is with regard to the highest-scoring question token (after stopword removal). In QUEST, this similarity is computed using word2vec <REF>, GloVe , or BERT  embeddings."
"Recently, <REF> have demonstrated the capacities of contextualized word embeddings across a wide variety of tasks, including SPRL. However, for SPRL labeling they proceed similar to  in the sense that they extract the gold heads of arguments in their dependency-based SPRL approach. Instead of using an LSTM to convert the input sentence to a sequence of vectors they make use of large language models such as ELMo  or BERT . The contextual vectors corresponding to predicate and the (gold) argument head are processed by a projection layer, self-attention pooling  and a two-layer feed forward neural network with sigmoid output activation functions. To compare with , our basic model uses standard GloVe embeddings. When our model is fed with contextual embeddings a further observable performance gain can be achieved."
"However, most state-of-art deep-learning-based resolvers utilize one-directional Transformers <REF>, limiting the ability to handle long-range inferences and the use of cataphors. Bidirectional Encoder Representations from Transformers, or BERT  learns a bidirectional contextual embedding and has the potential to overcome these problems using both the previous and next context. However, fine-tuning BERT for a specific task is computationally expensive and time-consuming. Syntax information has always been a strong tool for semantic tasks. Most heuristics-based methods use syntax-based rules ). Many of learning based models also rely on syntactic parsing for mention or entity extraction algorithms and compute hand-crafted features as input ."
"We establish baseline performance on SOCIAL IQA, using large pretrained language models based on the Transformer architecture <REF>. Namely, we finetune OpenAI-GPT  and BERT , which have both shown remarkable improvements on a variety of tasks. OpenAI-GPT is a uni-directional language model trained on the BookCorpus , whereas BERT is a bidirectional language model trained on both the BookCorpus and English Wikipedia. As per previous work, we finetune the language model representations but fully learn the classifier specific parameters described below."
"For images, a proof of existence for broadly useful representations is the output of the penultimate layer (the one before the softmax) of a powerful deep net trained on ImageNet. In natural language processing (NLP), low-dimensional representations of text -called text embeddings -have been computed with unlabeled data <REF>. Often the embedding function is trained by using the embedding of a piece of text to predict the surrounding text . Similar methods that leverage similarity in nearby frames in a video clip have had some success for images as well ."
"These large conversational datasets may support modeling across a large spectrum of natural conversational domains. Similar to the recent work on language model pretraining for diverse NLP applications <REF>, we believe that these datasets can be used in future work to pretrain large general-domain conversational models that are then fine-tuned towards specific tasks using much smaller amounts of task-specific conversational data. We hope that the presented repository, containing a set of strong baseline models and standardised modes of evaluation, will provide means and guidance to the development of nextgeneration conversational systems."
"Pre-trained word representations such as word2vec <REF> have been widely used in neural IR. They are learned from word cooccurrence in a large corpus, providing hints about synonyms and Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR '19, July 21-25, 2019, Paris, France © 2019 Association for Computing Machinery. ACM ISBN 978-1-4503-6172-9/19/07. . . $15.00 https://doi.org/10.1145/3331184.3331303 related words. But word co-occurrence is only a shallow bag-ofwords understanding of the text. Recently, we have seen rapid progress in text understanding with the introduction of pre-trained neural language models such as ELMo  and BERT . Different from traditional word embeddings, they are contextual -the representation of a word is a function of the entire input text, with word dependencies and sentence structures taken into consideration. The models are pre-trained on a large number of documents so that the contextual representations encode general language patterns. Contextual neural language models have outperformed traditional word embeddings on a variety of NLP tasks ."
"Finetune-based approaches such as Generative Pretrained Transformer (GPT) <REF> and BERT , however, pretrain a model on unannotated data and then finetune the same architecture and use it on different downstream tasks."
"Related to the above class of methods but without constraining input and output to be of the same modality, another popular class of unsupervised learning for classification is to perform pre-training without labels <REF>. Subsequently fine-tuning the pre-trained parameters is carried out with limited labels. This class of methods has accounted for the early success of deep learning in speech recognition . Very recently since October of 2018, they have also shown large success in natural language processing . However, to solve classification problems helped by the stage of unsupervised pre-training still requires labeled data in the fine-tuning stage. When labeled data is not so costly, this problem is not very serious. To be completely label free, it is desirable to carry out end-to-end learning via direct optimization. A class of new methods with this motivation have been developed in recent years, and are the focus of this paper."
"b) Predict answer candidates. In order to predict possible answer spans for each passage, we train a candidate answer prediction model on SQuAD 1.1 <REF>. We use the question answering (QA) model described in Section 3.  . The QA model predicts a answer span for each generated example. We discard the instance if the F1 score of the answer span is less than 0.6. Then we replace the answer of the triple with the one predicted by the QA model. e) Generate unanswerable questions. Because SQuAD 2.0 contains unanswerable questions. We use two rules to convert generated answerable questions to unanswerable ones. First, we substitute the question entities with the entities of the same type in the passage. Second, we insert the negative word ""not"" behind the verbs ""be"" , ""do"", ""have"", and modal verbs (such as ""can"", ""must"")."
"For each question, we assume that N p paragraphs are given (e.g. N p = 10 in HotpotQA). Since not every piece of text is relevant to the question, we train a sub-network to select relevant paragraphs. The sub-network is based on a pre-trained BERT model <REF> followed by a sentence classification layer with sigmoid prediction. The selector network takes a query Q and a paragraph as input and outputs a relevance score between 0 and 1. Training labels are constructed by assigning 1's to the paragraphs with at least one supporting sentence for each Q&A pair. During inference, paragraphs with predicted scores greater than η (= 0.1 in experiments) are selected and concate-  nated together as the context C. η is properly chosen to ensure the selector reaches a significantly high recall of relevant paragraphs. Q and C are further processed by upper layers."
"Our approach consists of following steps: (1) parse original protocol into a collection of protocol phrases together with their procedural relations, using a deterministic finite automation (DFA); (2) Match the protocol phrases back to the text spans in transcripts using fuzzy matching <REF>; (3) Generate text span extraction dataset and train a sequence labeling model  for text span extraction; (4) Generate text spanpair relation extraction (span-pair RE) dataset and fine-tune pre-trained context-aware span-pair RE model . With the trained models, we can automatically extract text spans summarizing actions from transcripts along with the procedural relations among them. Finally, we assemble the results into protocol knowledge, which lays the foundation for CTA."
"Features. We encode q and c (a candidate answer) with averaged word vectors <REF> and i with CNN features , following . We obtain the scene graph of each image either via human annotation  or via automatic scene graph generation . Ideally, every node in a graph will be provided with a node name (e.g., car) and a set of attributes (e.g., red, hatchback) and we represent each of them by the averaged word vectors. We then concatenate them to be the node features v n . For edges that are provided with the relationship name (e.g., on the top of), we again encode each e m by the average word vectors. We note that more advanced visual and natural language features  can be applied to further improve the performance."
"Attention mechanism: Attention mechanism of neural networks has been extensively utilized in various fields of deep learning, yielding state-of-the-art results. It facilitates neural networks to focus on valuable information of inputs, aiming to avoid interference from redundant messages. Firstly, proposed in machine translation, attention mechanism aims to align the words in the source language and target language <REF>. Then, it has been applied in other application of natural language processing , evolving to an indispensable module in neural networks architectures. Motivated by its success in NLP, attention mechanism has also been employed in computer vision tasks. Conspicuous performance gain has also been observed in images recognition , images caption , and fine-grained classification ."
"This section introduces the main experimental results. We also conducted some experiments related to BERT <REF>, which are included in the appendix."
"We evaluate our model following <REF>. We chiefly focus on their ultra-fine en-  tity typing scenario and use the same two distant supervision sources as them, based on entity linking and head words. On top of an adapted model from  incorporating ELMo , naïvely adding distant data actually hurts performance. However, when our learned denoising model is applied to the data, performance improves, and it improves more than heuristic denoising approaches tailored to this dataset. Our strongest denoising model gives a gain of 3 F 1 absolute over the ELMo baseline, and a 4.4 F 1 improvement over naive incorporation of distant data. This establishes a new state-ofthe-art on the test set, outperforming concurrently published work  and matching the performance of a BERT model  on this task. Finally, we show that denoising helps even when the label set is projected onto the OntoNotes label set , outperforming the method of  in that setting as well."
"We first evaluate our system on the   <REF>, we further evaluate our EPAr system (and its smaller-sized and ablated versions) on the ""follows + multiple"", ""follows + single"", and the full development set. First, note that on the full development set, our smaller system (""DE+AP+EA"") achieves statistically significant (p-value < 0.01) 9 improvements over the BiDAF baseline and is also comparable to  on the development set (64.7 vs. 64.8). 10 8 Note that there also exists a recent anonymous unpublished entry on the leaderboard with 70.9% accuracy, which is concurrent to our work. Also note that our system achieves these strong accuracies even without using pretrained language model representations like ELMo  or BERT , which have been known to give significant improvements in machine comprehension and QA tasks. We leave these gains for future work. 9 All stat. signif. is based on bootstrapped randomization test with 100K samples .  For time and memory feasibility, we use this smaller Moreover, we see that EPAr is able to achieve high accuracy in both the examples that require multi-hop reasoning (""follows + multiple""), and other cases where a single document suffices for correctly answering the question (""follows + single""), suggesting that our system is able to adjust to examples of different reasoning requirements. The evaluation results further demonstrate that our Document Explorer combined with TF-IDF-based retrieval (row 'DE+AP+EA') consistently outperforms TF-IDF alone (row 'AP+EA') or the Document Explorer without TF-IDF (row 'DE+AP+EA ' in ), showing that our 2-hop TF-IDF document retrieval procedure is able to broadly identify relevant documents and further aid our Document Explorer by reducing its search space. Finally, comparing the last two rows in Table 2 shows that using self-attention  to compute the document representation can further improve the full-sized system. We show an example of the 'reasoning tree' constructed by the Document Explorer and the correct answer predicted by the Evidence Assembler in . We report our system's accuracy on the MedHop dataset in . Our best system achieves 60.3 on the hidden test set 11 , outperforming all current models on the leaderboard. However, as reported by , the original MedHop dataset suffers from a candidate frequency imbalance issue that can be exploited by certain strong model with 100-d word embeddings and 20-d LSTM-RNN hidden size (similar to baselines in ) in all our analysis/ablation results (including Sec. 4).  The masked MedHop test set results use the smaller size model, because this performed better on the masked dev set."
"There has been a line of attempts to learn compositional phrase representations (e.g. <REF>, but many of these are tailored to a specific type of phrase or to a fixed number of constituent words, and they all disregard the surrounding context. Recently, contextualized word representations boasted dramatic performance improvements on a range of NLP tasks . Such models serve as a function for computing word representations in a given context, making them potentially more capable to address meaning shift. These models were shown to capture some world knowledge (e.g , which may potentially help with uncovering implicit information."
"There are 512 cells in each GRU layer. The vocabulary size in all experiments is 20,000, separately built for each dataset. All the word embeddings in our model come from the pre-trained BERT embeddings provided by <REF>, which has a dimension of 768 for each embedding. The maximum input length of our model is 50, the learning rate is 0.001, the k for embedding reward loss function is 5, the λ recon is 1, the λ embed is 0.5, and the batch size is 128. ER-AE is implemented in TensorFlow , and it uses the tokenizer in the NLTK library. All the models are evaluated from three aspects: semantic preservation, privacy protection, and stylometric changes:"
"Recent work has investigated this question by examining the outputs of language models on carefully chosen input sentences <REF> or examining the internal vector representations of the model through methods such as probing classifiers . Complementary to these approaches, we study 1 the attention maps of a pre-trained model. Attention  has been a highly successful neural network component. It is naturally interpretable because an attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word. Our analysis focuses on the 144 attention heads in BERT 2 , a large pre-trained Transformer  network that has demonstrated excellent performance on many tasks."
"Given the recent advances in transfer learning for natural language processing, we plan to experiment with pre-trained neural language models for feature extraction and fine-tuning using stateof-the-art approaches such as ELMO <REF>), ULMFIT  and   ."
"Given that NLMs (and Neural approaches in general) are the state-of-the-art in the field of NLP, finding ways to scale them to a larger software corpus is very desirable. An additional reason to scale them is that recent results in NLP <REF> show that NLMs can be used as upstream tasks in a transfer learning scenario, leading to state-of-the-art improvement in downstream tasks. Section 3 presents our first contribution: a detailed explanation of the possible modeling choices for source code that we identified. A variety of modeling choices for vocabulary are available, including: which source code elements to include or exclude; whether to filter out unfrequent tokens or not; how to handle different natural languages; and how to handle compound tokens. Some of these choices may have a large impact on vocabulary size, directly impacting the feasibility of training neural approaches."
"Unsupervised NMT As the foundation of unsupervised sentence translation, unsupervised word alignment has been investigated by <REF>, where linear embedding mapping and adversarial training are used to ensure the distribution-level matching, achieving considerable good accuracy or even surpasses the supervised counterpart for similar languages. ;  propose unsupervised NMT that leverages word translation for the initialization of the bilingual word embeddings.  partially share the parameter of the encoder and decoder to enhance the semantic alignment between source and target language.  further share the vocabulary of source and target languages and jointly learned the word embeddings to improve the quality of word alignment, and achieve large improvements on similar language pairs. Recently, inspired by the success of BERT  and MASS ,  leverage the MASS pre-training in the unsupervised NMT model and achieve state-of-the-art performance on some popular language pairs. Previous works on unsupervised NMT can indeed achieve good accuracy on similar language pairs, especially on the closely related languages such as English and German that are in the same language branch. In this circumstance, they can simply share the vocabulary and learn joint BPE for source and target languages, and share the encoder and decoder, which is extremely helpful for word embedding and latent representation alignment. However, they usually achieve poor accuracy on distant languages that are not in the same language branch or do not share same alphabets. In this paper, we propose pivot translation for distant languages, and leverage the basic unsupervised NMT model in  on similar languages as the building blocks for the unsupervised pivot translation."
"To cope up with this problem, RNN free architectures like QANet <REF>, which combines local convolution over words with a global self-attention mechanism, has been developed. In this study, BERT model  has been used with the intention of building a computationally faster neural architecture which performs reasonably well on SQuAD task."
"Self-Supervised Learning Self-supervised learning, which aims to train a network on an auxiliary task where ground-truth is obtained automatically, has been successfully applied in computer vision. Many self-supervised tasks have been introduced to use non-visual but intrinsically correlated features to guide the visual feature learning <REF>. As for natural language processing, predicting nearby words  is a self-supervised task to learn word embeddings. The language modeling is another line of self-supervision where a language model learns to predict the next word given the previous sequence . Recently,  further proposes two self-supervised tasks, the masked language model and next sentence prediction, to learn sentence embeddings. ;  further extend these two tasks into multi-lingual and multi-task paradigms.  consider them at the sentence-level for extractive summarization. Our work is the first to consider the sequential order as the self-supervised signal in dialogue and we propose the self-supervised task of inconsistent order detection towards more coherent and relevant dialogue learning."
"(1) what is the relationship between sentence representations learned by deep learning networks and those encoded by the brain; (2) is there any correspondence between hidden layer activations in these deep models and brain regions; and (3) is it possible for deep recurrent models to synthesize brain data so that they can effectively be used for brain data augmentation. In order to evaluate these questions, we focus on representations of simple sentences. We employ various deep network architectures, including recently proposed ELMo <REF> and BERT  networks. We use MagnetoEncephaloGraphy (MEG) brain recording data of simple sentences as the target reference. We then correlate the representations learned by these various networks with the MEG recordings. Overall, we observe that BERT representations are the most predictive of MEG data. We also observe that the deep network models are effective at synthesizing brain data which are useful in overcoming data sparsity in stimuli decoding tasks involving brain data."
"Implicit Context Inference in Pre-training. In the pre-training stage, the pre-training dataset comes from positive user-item interaction data, such as clicks and buys. We want to nd some latent pa erns of implicit context from these observational data. Pretraining method is widely used in many deep-learning tasks such as NLP and Computer Vision. In NLP, language models are pretrained over large corpus to learn good embedding representation of words and sentences <REF>. In computer vision, researchers nd it s bene cial to pre-train image with large dataset and ne-tune the last few layers in application speci c tasks . ere are two main approaches to apply the pre-trained representation to the following supervised tasks: feature-based approach and ne-tuning approach . Feature-based approach will include the representation as additional features and ne-tuning approach tunes the same model architecture in the downstream supervised learning tasks."
"The success of neural network models in the modern paradigm of deep learning often requires the construction of complicated networks with a large number of parameters (see, e.g., <REF>). Such network models thus often have overwhelmingly high capacities. Although it is still unclear to date what makes a neural network generalize well , the high capacities of these models are observably prone to overfitting and effective regularization techniques are highly demanded in the training of these models."
"ing the importance of semantic information <REF>. Among the few attempts of text-based models, word embeddings are usually trained from scratch, which might be suboptimal due to the lack of large quantities of data . Recently, general purpose text-embeddings such as ELMo  and BERT , which are pretrained on large datasets, become popular due to their performance on many natural language processing benchmarks. Therefore, the use of pretrained contextual sentence embeddings, namely ELMo and BERT are investigated in the current work for their usage in depression detection."
"For the open tracks, we use the contextualized word representations produced by BERT <REF> as extra input features. 2 Following previous works, we use the weighted summation of the last four transformer layers and then multiply a task-specific weight parameter following ."
"Word embedding is one of the most popular representations of document vocabulary in lowdimensional vector. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc. For this work, word embedding was created with a model similar to Bidirectional Encoder Representations from Transformers (BERT), <REF>. BERT is a language representation model designed to pre-train deep bidirectional representations by jointly conditioning on both left and right context in all layers."
"Although many neural models have been proposed for extractive summarization recently <REF>, the improvement on automatic metrics like ROUGE has reached a bottleneck due to the complexity of the task. In this paper, we argue that, BERT , with its pre-training on a huge dataset and the powerful architecture for learning complex features, can further boost the performance of extractive summarization ."
"Therefore, in this work, to address the aforementioned problems facing WDL and DIN, we try to incorporate sequential signal of users' behavior sequences into RS at Taobao. Inspired by the great success of the Transformer for machine translation task in natural language processing (NLP) <REF>, we apply the self-attention mechanism to learn a better representation for each item in a user's behavior sequence by considering the sequential information in embedding stage, and then feed them into MLPs to predict users' responses to candidate items. The key advantage of the Transformer is that it can better capture the dependency among words in sentences by the self-attention mechanism, and intuitively speaking, the ""dependency"" among items in users' behavior sequences can also be extracted by the Transformer. Therefore, we propose the user behavior sequence transformer (BST) for e-commerce recommendation at Taobao. Offline experiments and online A/B test show the superiority of BST comparing to existing methods. The BST has been deployed in rank stage for Taobao recommendation, which provides recommending service for hundreds of millions of consumers everyday."
"Our work relates to the field of model pretraining in NLP and computer vision fields <REF>. In the NLP community, works on model pretraining can be divided into unstructured text-based and structured knowledge-based ones. Both word embedding learning algorithms  and contextual embedding learning algorithms  belong to the textbased direction. Compared with these methods, which aim to learn a representation for a continuous sequence of words, our goal is to model the concept relatedness with graph structure in the knowledge base. Previous works on knowledgebased pretraining are typically validated on knowledge base completion or link prediction task . Our work belongs to the second line. We pre-train models from the commonsense knowledge base and apply the approach to the question answering task. We believe that combining both structured knowledge graphs and unstructured texts to do model pretraining is very attractive, and we leave this for future work."
"Recent developments in NLP, namely the Transformer architecture <REF> have led to significant performance improvements for various tasks such as translation , text generation , and language understanding . In , the Transformer was applied to the task of image captioning. The authors explored extracting a single global image feature from the image as well as uniformly sampling features by dividing the image into 8x8 partitions. In the latter case, the feature vectors were fed in a sequence to the Transformer encoder. In this paper we propose to improve upon this uniform sampling by adopting the bottom-up approach of . The Transformer architecture is particularly well suited as a bottom-up visual encoder for captioning since it does not have a notion of order for its inputs, unlike an RNN. It can, however, successfully model sequential data with the use of positional encoding, which we apply to the decoded tokens in the caption text. Rather than encode an order to objects, our Object Relation Transformer seeks to encode how two objects are spatially related to each other and weight them accordingly."
"We compare our results with the state-of-the-art in <REF>. Our model achieves the best performance on SciTai and Quora Question tasks. For instance, SAN obtains 89.4 (vs 89.1) and 88.4 (88.3) on the Quora Question and SciTail test set, respectively and set the new state-of-the-art. On SNLI and MultiNLI dataset, , GPT  and BERT  use a large amount of external knowledge or a large scale pretrained contextual embeddings. However, SAN is still competitive these models. On SciTail dataset, SAN even outperforms GPT. Due to the space limitation, we only list two top models.  We further utilize BERT as a feature extractor 6 and use the SAN answer module on top of it. Comparing with Single-step baseline, the proposed model obtains +2.8 improvement on the SciTail test set (94.0 vs 91.2) and +2.1 improvement on the SciTail dev set (96.1 vs 93.9). This shows the generalization of the proposed model which can be easily adapted on other models 7 . Analysis: How many steps it needs? We search the number of steps t from 1 to 10. We observe that when t increases, our model obtains a better improvement (e.g., 86.7 (t = 2)); however when t = 5 or t = 6, it achieves best results (89.4) on SciTail dev set and then begins to downgrade 4 For direct comparison, this has the same three lower layers as     We run BERT (the base model) to extract embeddings of both premise and hypothesis and then feed it to answer models for a fair comparison. 7 Due to highly time consumption and space limitation, we omit the results using BERT on SNLI/MNLI/Quora Question dataset."
"Learning useful representations from unlabeled data is a challenging problem and improvements over existing methods can have wide-reaching benefits. For example, consider the ubiquitous use of pre-trained model components, such as word vectors <REF> and context-sensitive encoders , for achieving state-of-the-art results on hard NLP tasks. Similarly, large convolutional networks pre-trained on large supervised corpora have been widely used to improve performance across the spectrum of computer vision tasks . Though, the necessity of pre-trained networks for many vision tasks has been convincingly questioned in recent work . Nonetheless, the core motivations for unsupervised learning -namely minimizing dependence on potentially costly corpora of manually annotated data -remain strong."
"Comparing with other commonsense reasoning tasks, such as COPA <REF>, Story Cloze Test , , SWAG , ReCoRD , and so on, WSC and PDP better approximate real human reasoning, can be easily solved by native English-speaker , and yet are challenging for machines. For example, the WNLI task, which is derived from WSC, is considered the most challenging NLU task in the General Language Understanding Evaluation (GLUE) benchmark . Most machine learning models can hardly outperform the naive baseline of majority voting (scored at 65.1) 1 , including BERT  and Distilled MT-DNN ."
"In this paper, we focus on developing a ""scalable"" and ""universal"" belief tracker, whereby only a single belief tracker serves to handle any domain and slot-type. To tackle this problem, we propose a new approach, called slot-utterance matching belief tracker (SUMBT), which is a domainand slot-independent belief tracker as shown in <REF>. Inspired by machine reading comprehension techniques , SUMBT considers a domain-slot-type (e.g., 'restaurant-food') as a question and finds the corresponding slot-value in a pair of user and system utterances, assuming the desirable answer exists in the utterances. SUMBT encodes system and user utterances using recently proposed BERT  which provides the contextualized semantic representation of sentences. Moreover, the domain-slot-types and slotvalues are also literally encoded by BERT. Then SUMBT learns the way where to attend that is related to the domain-slot-type information among the utterance words based on their contextual semantic vectors. The model predicts the slot-value label in a non-parametric way based on a certain metric, which enables the model architecture not to structurally depend on domains and slot-types. Consequently, a single SUMBT can deal with any pair of domain-slot-type and slot-value, and also can utilize shared knowledge among multiple domains and slots."
"Word Representations To represent words from samples, we used deep contextualized word representations <REF> also known as ELMo along with its available pre-trained model 1 . We also experimented with transformers model word representations known as BERT  and its pre-trained model 2 ."
"We trained a BERT-Large language model from <REF> on the combined Wikipedia and BooksCorpus . BERT-Large is a large bidirectional transformer model containing 24 transformer blocks with 1024 hidden dimensions and 16 self attention heads. It has 340M parameters (1.297 GiB), and is setup to optimize two losses jointly: (a) masked language model (Masked-LM) loss where the task is to predict masked tokens based on surrounding context, and (b) next sentence prediction (NSP) loss where the task is to predict if a sentence follows another sentence where negatives sentences are randomly selected from the corpus. We ran all our experiments using the open sourced code from  on an 8×8 Cloud TPU-V2 configuration which has 128 cores. The baseline used was the Adam optimizer with learning rate η = 10 −4 , β 1 = 0.9, and β 2 = 0.999. The learning rate was warmed-up over the first 10,000 steps, followed by a linear decay. SM3 used the same warmup as a safety mechanism, with no further tinkering. Momentum was set to 0.9. We trained all models for 500K steps. We split the dataset into a 90 − 10 train-test split."
"In addition to the human-annotated data, we also collect large-scale distantly supervised data to promote weakly supervised RE scenarios. We remove the 5, 053 human-annotated documents from the 106, 926 documents, and use the rest 101, 873 documents as the corpus of distantly supervised data. To ensure that the distantly supervised data and human-annotated data share the same entity distribution, named entity mentions are reidentified using Bidirectional Encoder Representations from Transformers (BERT) <REF> that is fine-tuned on the human-annotated data collected in Sec. 2.1 and achieves 90.5% F1 score. We link each named entity mention to one Wikidata item by a heuristic-based method, which jointly considers the frequency of a target Wikidata item and its relevance to the current document. Then we merge the named entity mentions with identical KB IDs. Finally, relations between each merged entity pair are labeled via distant supervision."
"But I prefer Bill Gates if we want to talk about hero worship myself. [...] MEMNET What if you don't want to see this movie? SEQ2SEQ No ... the whole point is that this is a good way to make a difference. CMR+W So he's the CEO of Apple. <REF>: Sample output comparing our best system (CMR+W) against Memory Networks and a SEQ2SEQ baseline. The source documents were manually shortened to fit in the table, without significantly affecting meaning. from a given document for a given question . These models differ in how they fuse information between questions and documents. We chose SAN  because of its representative architecture and competitive performance on existing MRC tasks. We note that other off-theshelf MRC models, such as BERT , can also be plugged in. We leave the study of different MRC architectures for future work. Questions are treated as entirely independent in these ""single-turn"" MRC models, so recent work (e.g., CoQA  and QuAC ) focuses on multi-turn MRC, modeling sequences of questions and answers in a conversation. While multi-turn MRC aims to answer complex questions, that body of work is restricted to factual questions, whereas our work-like much of the prior work in end-to-end dialogue-models free-form dialogue, which also encompasses chitchat and non-factual responses."
"To represent complex characteristics of words and word usage across different linguistic contexts effectively, a new model for deep contextualized word representation was introduced in <REF>. First, an Embeddings from Language Models (ELMo) representation is generated with a function that takes an entire sentence as the input. The function is generated by a bidirectional LSTM network that is trained with a coupled language model. Existing embedding models can be improved by incorporating the ELMo representation as it is effective in incorporating the sentence information. By following ELMo, a series of pre-trained neural network models for language tasks are proposed such as BERT  and OpenAI GPT . Their effectiveness is proved in lots of language tasks."
"This paper contributes a new publicly available English legal judgment prediction dataset of cases from the European Court of Human Rights (ECHR). 1 Unlike <REF>, who provide only features from approx. 600 ECHR cases, our dataset is substantially larger (∼11.5k cases) and provides access to the raw text. As a second contribution, we evaluate several neural models in legal judgment prediction for the first time in English. We consider three tasks: (1) binary classification (i.e., violation of a human rights article or not), the only task considered by ; (2) multi-label classification (type of violation, if any); (3) case importance detection. In all tasks, neural models outperform an SVM with bag-of-words , the only method tested in English legal judgment prediction so far. As a third contribution, we use an approach based on data anonymization to study, for the first time, whether the legal predictive models are biased towards demographic information or factual information relevant to human rights. Finally, as a side-product, we propose a hierarchical version of BERT , which bypasses BERT's length limitation and leads to the best results."
"Since large scale pretraining systems using language models, such as BERT <REF>,  and GPT , have proved to be effective in a wide range of NLP tasks, we explore the possibility of combining glyph embeddings with BERT embeddings. Such a strategy will potentially endow the model with the advantage of both glyph evidence and large-scale pretraining. The overview of the combination is shown in . The model consists of four layers: the BERT layer, the glyph layer, the Glyce-BERT layer and the task-specific output layer. BERT Layer Each input sentence S is concatenated with a special CLS token denoting the start of the sentence, and a SEP token, denoting the end of the sentence. Given a pre-trained BERT model, the embedding for each token of S is computed using BERT. We use to output from the last layer of the BERT transformer to represent the current token."
"That day the office manager, who was drinking, hit the problem sales worker with a bottle, but it was not serious.  <REF> is from . Both a simple BOW and the state-of-the-art BERT   Wikipedia. Examples are generated from controlled language models and back translation, and given five human ratings each in both phases. A final rule recombines annotated examples and balances the labels. Our final PAWS dataset will be released publicly with 108,463 pairs at https: //g.co/dataset/paws. We show that existing state-of-the-art models fail miserably on PAWS when trained on existing resources, but some perform well when given PAWS training examples. BERT  fine-tuned on QQP achieves over 90% accuracy on QQP, but only 33% accuracy on PAWS data in the same domain. However, the accuracy on PAWS boosts to 85% by including 12k PAWS training pairs (without reducing QQP performance).  also shows that the new model is able to correctly classify challenging pairs. Annotation scale is also important: our learning curves show strong models like BERT improve with tens of thousands of training examples."
"In the future, it is worth exploring how different architectures and model choices affect the quality, complexity and readability of the uncovered evidence. For instance, one direction would be to to train the classifier on top of a pretrained language model <REF> which could improve the classification performance. Furthermore, other explainability methods should also be tested."
"Recently, transformer networks have been shown to perform well for neural machine translation <REF> and many other NLP tasks . A Transformer layer distinguishes itself from a regular recurrent network by entirely relying on a key-value ""self""-attention mechanism for learning relationships between distant concepts, rather than relying on recurrent connections and memory cells to preserve information, as in LSTMs, that can fade over time steps. Transformer layers can be seen as bagof-concept layers because they don't preserve location information in the weighted sum self-attention operation. To model word order, sinusoidal positional embeddings are used ."
We include a short summary of the model types used for some of the top competitors in <REF>. Some of the authors of these models plan to write detailed papers describing their models. Please also refer to the slides at the website written by the model's authors  . The winner's (Lost in Conversation's) code is also publicly available 12 .
"Transformer networks <REF> are sequence models that rely on the attention mechanism  to capture long term dependencies. Since their introduction in the context of machine translation, they have been applied to many natural language processing tasks, such as language modeling  or sentence representation . On most of them, they are now surpassing the former state-of-the-art models based on recurrent  or convolutional networks . At their core, transformers use a self-attention layer that forms a representation of the current input by gathering the most relevant information from its context. This layer is repeated along the network depth, allowing for information to flow for long distances and to form rich sequence representations. The self-attention mechanism is often considered as the key component of their success and many have worked on improving transformers by increasing the size of the context captured by those layers ."
"In this paper, we argue that such left-to-right unidirectional architectures restrict the power of the historical sequence representations. Both MCs and RNN based models are originally introduced for sequential data with a natural order, e.g., text and time series data. They often assume a rigidly ordered sequence over data which is not always true for user behaviors in real-world applications. The choices of items in a user's historical interactions may not follow a rigid order assumption <REF>. For example in , the order in which three lipsticks (B, C, and D) are clicked makes no difference for recommendation systems. Moreover, user behaviors on websites like YouTube or Amazon are often noisy due to a variety of unobservable external factors . Thus, we argue that bidirectional model is a more reasonable choice for modeling user behavior sequences. Furthermore, even on data with rigid order like text, deep bidirectional self-attention model BERT  has significantly outperformed other state-of-the-art unidirectional models (e.g., OpenAI GPT ) on eleven tasks of General Language Understanding Evaluation (GLUE) benchmark 1 ."
"c) Conversational context modeling: Context is at the core of the NLP research. According to several recent studies <REF>, contextual sentence and word embeddings can improve the performance of the state-of-the-art NLP systems by a significant margin."
"Unsupervised representation learning has been highly successful in the domain of natural language processing <REF>. Typically, these methods first pretrain neural networks on large-scale unlabeled text corpora, and then finetune the models or representations on downstream tasks. Under this shared high-level idea, different unsupervised pretraining objectives have been explored in literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been the two most successful pretraining objectives."
"Since the release of such large data sets, many advanced deep learning architectures have been developed <REF>. Although many of these deep learning models achieve close to human level performance on SNLI and MultiNLI datasets, these models can be easily deceived by simple adversarial examples.  shows how simple linguistic variations such as negation or re-ordering of words deceives the DecAtt Model.  goes on to show that this failure is attributed to the bias created as a result of crowd sourcing. They observe that crowd sourcing generates hypothesis that contain certain patterns that could help a classifier learn without the need to observe the premise at all."
"GPT <REF> 100 SPN  Not reported BERT  40 Mesh Transformer  10 Transformer-XL  Not reported GPT-2  Not reported (20 or 100) Sparse Transformer  70 -120 dataset for many epochs to make the comparison with the previous state-of-the-art fair. Also, there are many papers that do not report the number of epochs used for the training. Since the total computational resources spent on the training is proportional to not only the number of parameters but also the number of epochs, the current practice should be reconsidered. For example, we need to create larger standard datasets, and the models have to be trained for only one epoch for a fair comparison."
"Extractive Question Answering (EQA) is the task of answering questions given a context document under the assumption that answers are spans of tokens within the given document. There has been substantial progress in this task in English. For SQuAD <REF>, a common EQA benchmark dataset, current models beat human : A schematic of our approach. The right side (dotted arrows) represents traditional EQA. We introduce unsupervised data generation (left side, solid arrows), which we use to train standard EQA models performance; For SQuAD 2.0 , ensembles based on BERT  now match human performance. Even for the recently introduced Natural Questions corpus , human performance is already in reach. In all these cases, very large amounts of training data are available. But, for new domains (or languages), collecting such training data is not trivial and can require significant resources. What if no training data was available at all?"
"Fine-tuned BERT. We compare our baselines with a fine-tuned BERT model <REF>. BERT is a pre-trained deep bidirectional  transformer model that can encode sentences into dense vector representations. It is trained on large un-annotated corpora such as Wikipedia and the BooksCorpus  using two different learning objectives, namely masked language model and next sentence prediction. These learning objectives together allow the model to learn representations that can be easily fine-tuned to achieve state-of-the-art performance for a wide range of natural language processing tasks. For relative specificity detection, we feed the pair of claims as a single sequence with the special [SEP] token between the claims, and a [CLS] token at the beginning of the sequence, as shown in , into a pre-trained BERT model 6 . In addition, we indicate each token in the first claim (as well as the [CLS] and [SEP] tokens) as belonging to sentence A, and each token in the second claim as belonging to sentence B, which is used by the BERT model to add the appropriate learned sentence embedding to each token. Note that this approach of packing a pair of claims into a single sequence is consistent with the input representation from , for tasks where the input is a pair of sequences. We then take the output of the [CLS] token from the final layer of the BERT model, and feed it into a classification layer. We fine-tune 7 this architecture for relative specificity detection."
"Story-Cloze: Since it is difficult to do human evaluation on all the stories, we use the StoryCloze task <REF> to create a metric in order to pick our best model and also to evaluate the efficacy of our model against Seq2Seq and its variants. This new proposed metric measures the semantic relevance of the generated ending with respect to the context. In  : Model comparison based on automatic metrics DIST-1, DIST-2 and DIST-3. the Story-Cloze task, given two endings to a story the task is to pick the correct ending. We can use this task to identify the better of two endings. In order to do so, we fine-tune BERT  to identify the true ending between two story candidates. The dataset for this task was obtained using the Story-Cloze task. Positive examples to BERT are obtained from the StoryCloze dataset while the negative examples are obtained by randomly sampling from other story endings to get false ending for the story. We fine tune BERT in the two sentence setting by providing the context as the first sentence and the final sentence as the second. We pick the ending with a greater probability (from BERT's output head) of being a true ending as the winner. With this approach we were able to get a Story-Cloze test accuracy of 72%. We now use this pre-trained model to compare the IE + GA model with our models. We select the winner based on the probability given by the pre-trained Bert model."
"As modern neural networks evolve, dynamic neural network structures such as variable-length sequences <REF>, trees , and graphs  become increasingly important. For instance, social networks have complex graph structures, and natural language processing (NLP) problems often have variable-length sequences, each accompanied by a different parse tree . While most deep learning frameworks (e.g. Tensorflow , PyTorch , dyNet , and Apache MXNet ) can express these dynamic neural networks as computation graphs, training with dynamic graphs can be much slower compared to static ones. This is because for each sample, the framework needs to construct different computation graphs, move data across the memory hierarchy, which limits parallelism and efficient utilization of hardware resources. Also, optimization techniques such as memory planning might not be effectively reused due to the variations in the computation graphs. Batch processing amortizes these overhead, provides better data locality, and enjoys better parallelism. For example, many vector-vector multiplication operations that share the first operand can be batched into one matrix-vector multiplication."
"Mention detection (MD) is the task of identifying mentions of entities in text. It is an important preprocessing step for downstream applications such as coreference resolution . As such, the quality of mention detection affects very deeply both the quality of an annotation and the performance of a model for such applications <REF>. Comparing to the simplified version that focuses on classifying named entity mentions for named entity recognition (NER), the full MD task for coreference resolution is more complex in two respects: firstly, it identifies more mention types, such as nominal mentions and pronouns; secondly, the mentions can be nested, so the task cannot be treated as a simple sequence labelling task, as is the norm in NER systems. The most recent neural network approaches such as ELMo  and BERT , have achieved substantial improvements in the NER benchmark CONLL 2003 data set. However, most of the MD system used by the state-of-the-art coreference systems do not take advantage of these advances and still heavily rely on parse trees . They either use all the NPs as candidate mentions  or use the rule-based mention detector from the Stanford deterministic system  to extract mentions from NPs, named entity mentions and pronouns . There are only very few studies that attempt to apply neural network approaches to the MD task.  first introduced a neural mention detector as a part of their end-to-end coreference system; however, the system does not output intermediate mentions, hence the mention detector cannot be used by other coreference systems directly. To the best of our knowledge,  introduced the only standalone neural mention detector. By using a modified version of the NER system of , they showed substantial performance gains at mention detection on the benchmark CONLL 2012 data set and on the CRAC 2018 data set when compared with the Stanford deterministic system . To build a high accuracy standalone MD system is not only important for the downstream applications, but also beneficial for annotation tasks that require mentions . In this paper, we compare three neural architectures for MD. The first system is a slightly modified version of the mention detection part of the  system. The second system employs a bi-directional LSTM on the sentence level and uses biaffine attention  over the LSTM outputs to predict the mentions. The third system takes the outputs from BERT  and feeds them into a feed-forward neural network to classify candidates into mentions and non mentions. We evaluate these three models on both the CONLL and the CRAC data sets, with the following results. Firstly, we show that better mention performance of up to 1.5 percentage points 1 can be achieved by training the mention detector alone. Secondly, our best system achieves improvements of 5.3 and 6.5 percentage points when compared with 's neural MD system on CONLL and CRAC respectively. Thirdly, by using better mentions from our mention detector, we can improve the end-to-end  system and the  pipeline system by up to 0.7% and 1.7% respectively."
"The introduction of pre-trained language models, such as BERT <REF> and Open-GPT , among many others, has brought tremendous progress to the NLP research and industrial communities. The contribution of these models can be categorized into two aspects. First, pre-trained language models allow modelers to achieve reasonable accuracy without the need an excessive amount of manually labeled data. This strategy is in contrast with the classical deep learning methods, which requires a multitude more data to reach comparable results. Second, for many NLP tasks, including but not limited to, SQuAD , CoQA , named entity recognition , Glue , machine translation , pre-trained model allows the creation of new state-of-art, given a reasonable amount of labelled data."
"For knowledge transfer between different languages, it is intuitive to use machine translation (MT) to translate the source language into the target language, or vice versa. Using MT to achieve knowledge transfer between different languages has been studied on sentiment analysis <REF>, spoken language understanding  and question answer . It is also possible to train language model to obtain crosslingual text representations and further improve QA performance in different languages using parallel data  or even without parallel data  "
"Large neural networks are ubiquitous in modern deep learning applications, including computer vision <REF>, speech recognition (van den  and natural language understanding . While their size allows learning from big datasets, it is a limitation for users without the appropriate hardware, or for internet-of-things applications. As such, the deep learning community has seen a focus on model compression techniques, including knowledge distillation , network pruning  and quantization ."
"In this paper, we propose the multimodal target-source classifier model (MTCM) for MLU-FI tasks. MTCM is an extension of the multimodal similarity-based integration approach. Multimodal region-wise classification is used instead of cosine similarity. To handle linguistic information, we adopt sub-word embedding using BERT <REF> and a multilayer bidirectional LSTM (Bi-LSTM). In addition, we propose MTCM-GAN that extends MTCM with generative adversarial nets (GANs ). By taking advantage of the generation ability of GANs, we show that MTCM-GAN can successfully augment data and improve on the classification performance of the MTCM. The high level architecture of our method is shown in ."
"Nogueira et al. <REF> first showed the substantial effectiveness gains for the MS MARCO passage re-ranking using BERT , a large pre-trained transformer based model. However, they note the stark trade-off with respect to performance. MacAvaney et al.  show that by combining BERT 's classification label with the output of various neural models exhibits good results for low-trainingresource collections. They also show that this comes at a substantial performance cost -BERT taking two orders of magnitude longer than a simple word embedding."
"The last step towards the fully integrated CSI:Summarization is the backward inference, i.e. the identification of what content a summary actually used, or p(t|x, y). The result of this is shown with red highlights in the interface in <REF>. The backwards model is a separate model we specifically developed for the interface. It uses a contextualized representation of words in both input and summary that represents them as vectors of size d hid . Given the representation for a word x i , the model computes an attention over y, such that each summary word y k is assigned an attention weight a k . We use these weights to derive a context for the word x i which we denote c i , by computing"
"For a proof of concept, we chose the word2vec embedding <REF> as the word vector space to interpret. Recently, contextualized representations, like ELMo  and BERT , received increased attention. Nevertheless, well-established global representations, such as word2vec remain highly relevant: ELMo still benefits from using global embeddings as additional input and BERT trains its own global token embedding space."
"We claim that several existing models <REF> boil down to SRL models where the sentence embeddings (h 1 , h 2 ) act as entity embeddings (e 1 , e 2 ). This framework is depicted in figure 1. In this article we focus on sentence embeddings, although our framework can straightforwardly be applied to other levels of language granularity (such as words, clauses, or documents). Some models  do not rely on explicit sentence encodings to perform relation prediction. They combine information of input sentences at earlier stages, using conditional encoding or cross-attention. There is however no straightforward way to derive transferable sentence representations in this setting, and so these models are out of the scope of this paper. They sometimes make use of composition functions, so our work could still be relevant to them in some respect."
"Recent advancement in NLP such as BERT <REF> has facilitated great improvements in many Natural Language Understanding (NLU) tasks . BERT first trains a language model on an unsupervised large-scale corpus, and then the pretrained model is fine-tuned to adapt to downstream NLU tasks. This finetuning process can be seen as a form of transfer learning, where BERT learns knowledge from the large-scale corpus and transfer it to downstream tasks."
"Recently, significant progress has been achieved on many natural language processing tasks including MRC by fine-tuning a pre-trained generalpurpose language model <REF>. However, similar to the process of knowledge accumulation for human readers, it is time-consuming and resource-demanding to impart massive amounts of general domain knowledge from external corpora into a deep language model via pre-training. For example, it takes a month to pre-train a 12-layer transformer on eight P100 GPUs over the BooksCorpus ;  pre-train a 24-layer transformer using 64 TPUs for four days on the BooksCorpus plus English Wikipedia, a feat not easily reproducible considering the tremendous computational resources (≈ one year to train on eight P100 GPUs)."
"Read Rerank The input documents are pruned and splitted into multiple segments of text, which are then fed into the model 2 . Few top-ranked segments are retrieved and the rest are early stopped. Multiple candidate answers are proposed for each segment, which are later pruned and reranked. RE 3 QA has three outputs per candidate answer: the retrieving, reading, and reranking scores. The network is trained end-to-end with a multi-task objective. ""T-Block"" refers to pre-trained Transformer block <REF>. and reranker components into a unified network instead of a pipeline of separate models, (b) we share contextualized representation across different components while pipelined approaches reencode inputs for each model, and (c) we propose an end-to-end training strategy so that the context inconsistency problem can be alleviated."
"Textual Question Answering (also known as reading comprehension) aims to answer questions based on given paragraphs. It is a typical cornerstone in the NLP domain, which assesses the ability of algorithms in understanding human language. Significant progress has been made over the past years due to the using of end-to-end neural network models and attention mechanism, such as DMN <REF>, r-net , DrQA , QANet , and most recently BERT . Many techniques in QA have been inherited in solving VQA problem, such as the attention mechanism, DMN, etc. In this work, we try to solve the VQA problem built upon QANet."
"Contextual word embedding models, such as ELMo and <REF> have become increasingly common, replacing traditional type-level embeddings and attaining new state of the art results in the majority of NLP tasks. In these models, every word has a different embedding, depending on the context and the language model state; in these settings, the analogy task used to reveal biases in uncontextualized embeddings is not applicable. Recently,  showed that traditional cosine-based methods for exposing bias in sentence embeddings fail to produce consistent results for embeddings generated using contextual methods. We find similar inconsistent results with cosine-based methods of exposing bias; this is a motivation to the development of a novel bias test that we propose."
"Understanding cross-linguistic variation has for long been one of the foci of linguistics, addressed by researchers in comparative linguistics, linguistic typology and others, who are motivated by comparison of languages for genetic or typological classification, as well as many other theoretical or applied tasks. Comparative linguistics seeks to identify and elucidate genetic relationships between languages and hence to identify language families <REF>. From a different angle, linguistic typology compares languages to learn how different languages are, to see how far these differences may go, and to find out what generalizations can be made regarding cross-linguistic variation on different levels of language structure and aims at mapping the languages into types . The availability of large electronic text collections, and especially large parallel corpora, have offered new possibilities for computational methodologies that are developed to capture cross-linguistic variation. This work falls under computational typology , an emerging field with the goal of understanding of the differences between languages via computational (quantitative) measures. Recent studies already offer novel insights into the inner structure of languages with respect to various sequence fingerprint comparison metrics, such as for example the Jaccard measure, the intra edit distance and many other boolean distances . Such comparisons represent e.g., sentences as vectors, and evaluate their similarity using plethora of possible metrics. Albeit useful, vector-based representation of words, sentences or broader context does not necessarily capture the context relevant to the task at hand and the overall structure of a text collection. Word or sentence embeddings, which recently serve as the language representation workhorse, are not trivial to compare across languages, and can be expensive to train for new languages and language pairs (e.g., BERT ). Further, such embeddings can be very general, possibly problematic for use on smaller data sets and are dependent on input sequence length."
"All teams found it useful to use external resources beyond those provided by the Shared Task. Four submissions used external embeddings, MUSE <REF> in the case of MaskParse@Deskiñ and XLangMo, ELMo (Peters et al., 2018) in the case of TüPa, 14 and BERT  in the case of HLT@SUDA.  Other resources included additional unlabeled data (TüPa and CUNY-PekingU), a list of multi-word expressions (MaskParse@Deskiñ), and the Stanford parser in the case of DANGNT@UIT.VNU-HCM. Only CUNY-PKU used the 20K unlabeled parallel data in English and French. A common trend for many of the systems was the use of cross-lingual projection or transfer (âȂŃMaskParse@Deskiñ, HLT@SUDA, TüPa, GCN-Sem, CUNY-PKU and XLangMo). This was necessary for French, and was found helpful for German as well (CUNY-PKU).  shows the labeled and unlabeled F1 for primary and remote edges, for each system in each track. Overall F1 (All) is the F1 calculated over both primary and remote edges. Full results are available online. 15  shows the fine-grained evaluation by 15 http://bit.ly/semeval2019task1results labeled F1 per UCCA category, for each system in each track. While Ground edges were uniformly difficult to parse due to their sparsity in the training data, Relators were the easiest for all systems, as they are both common and predictable. The Process/State distinction proved challenging, and most main relations were identified as the more common Process category. The winning system in most tracks (HLT@SUDA) performed better on almost all categories. Its largest advantage was on Parallel Scenes and Linkers, showing was especially successful at identifying Scene boundaries relative to the other systems, which requires a good understanding of syntax."
"We formulate the answer recycling problem as a constrained optimization based on the relevance and entailment scores between each candidate negative answer and the gold answer, as measured by state-of-the-art natural language inference models <REF>. A neat feature of our recycling algorithm is a knob that can control the tradeoff between human and machine difficulty: we want the problems to be hard for machines while easy for humans."
"Transfer learning has a long history in the field of machine learning <REF>. More recently, deep neural network models have been shown to be extremely effective for learning representations of data with a high degree of re-usability across many different tasks and domains. Perhaps the most well-known example of this is the use of the ImageNet  image classification database to pre-train convolutional neural network models for other downstream computer vision tasks . Other sub-fields have also developed similarly techniques. For example, in natural language processing, dense word vector models such as word2vec  and GloVe , or more advanced ones like ELMo  and BERT  have quickly replaced one-hot word representations in many tasks and pushed the state-of-theart forward on a variety of language understanding tasks. More recently, there is also an increasing interest in learning from multimodal data  and transfer learned representations from such tasks  In the field of speech recognition, low-resource speech recognition is a scenario which heavily benefits from transfer learning, for example in the form of training on multilingual datasets . Other models capable of disentangling phonetic and domain information have recently been shown to learn acoustic features with a greater degree of domain invariance than traditional acoustic features . Another line of work has studied the use of the visual modality as a form of weak supervision using semantic information for acoustic modeling , followed up with analysis on representations learned from such models . In this paper, we build upon this prior work and quantify the degree to which these representations can be used to build robust ASR."
"BERT <REF> represents the latest refinement in a series of neural models that take advantage of pretraining on a language modeling task . Researchers have demonstrated impressive gains in a broad range of NLP tasks, from sentence classification to sequence labeling. Recently,  showed that combining a BERT-based reader with passage retrieval using the Anserini IR toolkit yields a large improvement in question answering directly from a Wikipedia corpus, measured in terms of exact match on a standard benchmark ."
This model is similar to the large Transformer model recently used in several works leading to impressive results on several down-stream NLP tasks <REF>. Our model is based on a recently published PyTorch adaptation by the HuggingFace team which can be found at: https://github.com/huggingface/ pytorch-openai-transformer-lm.
"We look at an interesting application of our technique in this section. Our previous experiments looked at cases where the data, specifically the feature vector representation, was identical for the oracle and the interpretable model. This is also what Algorithm 3 implicitly assumes. Here, we explore the possibility of going a step further and changing the feature vectors between the oracle and the interpretable model. We consider the task of document classification, where our 'raw data' comprises of text documents from the ag-news dataset <REF>. However, the feature vectors used by the models are different: 1) Oracle: Documents are converted to Universal Sentence Encoder Embeddings . The oracle model is a RF built on these embeddings. 2) Interpretable model: We use a bag-of-words (BoW) feature vector, using term frequencies (tf) as feature values. A LPM model of size = 5 is used as the classifier. We use 2 (of 4) labels from the dataset. The accuracy on our task is measured with the F1 score.  shows the improvements we obtain at various sizes of the dataset, with a LPM of size = 5. Clearly, the USE embeddings influence the effectiveness of the simpler tf based BoW representation. We believe this is a particular powerful and exciting application of our technique since: 1) Embeddings are an active and impactful area of research today , , , , , and our technique allows interpretable models to benefit from them even if they may not directly use them. 2) This can be extended to more general applications."
"Coreference resolution is a task that aims to identify spans in a text that refer to the same entity. This is central to Natural Language Understanding. We focus on a specific aspect of the coreference resolution that caters to resolving ambiguous pronouns in English. Recent studies have shown that state-of-the-art coreference resolution systems exhibit gender bias <REF>    .  released a dataset that contained an equal number of male and female examples to encourage gender-fair modeling on the pronoun resolution task. A shared task for this dataset was then published on Kaggle 2 . The task involves classifying a specific ambiguous pronoun in a given Wikipedia passage as coreferring with one of the three classes: first candidate antecedent (hereby referred to as A), second candidate antecedent (hereby referred to as B) or neither of them (hereby referred to as N). The authors show that even the best of the baselines such as , ,  achieve an F1 score of just 66.9% on this dataset. The limited number of annotated labels available in this unbiased setting makes the modeling a challenging task. To that end, we propose an extractive question answering formulation of the task that leverages BERT  pre-trained representations and significantly improves (22.2% absolute improvement in F1 score) upon the best baseline . In this formulation, the task is similar to a SQUAD  style question answering (QA) problem where the question is the context window (neighboring words) surrounding the pronoun to be resolved and the answer is the antecedent of the pronoun. The answer is contained in the provided Wikipedia passage. The intuition behind using the pronoun's context window as a question is that it allows the model to rightly identify the pronoun to be resolved as there can be multiple tokens that match the given pronoun in a passage. There has been previous work that cast the coreference resolution as a Question Answering problem . But the questions used in their approach take the form ""Who does ""she"" refer to?"". This would necessitate including additional information such as an indicator vector to identify the exact pronoun to be resolved when there are multiple of them in a given passage. Furthermore, their approach doesn't impose that the answer should be contained within the passage or the question text. ) model the pronoun resolution task of the Winograd schema challenge  as a question answering problem by including the candidate antecedents as part of the question. An unique feature of the question answering framework (referred to as CorefQA) we propose is that it doesn't require the knowledge of the candidate antecedents in order to produce an answer for the pronoun resolution task. The model ""learns"", from training on the QA version of the shared task dataset, the specific task of extracting the appropriate antecedent of the pronoun given just the Wikipedia passage and the pronoun's context window. We also demonstrate other modeling variants for the shared task that use the knowledge of the candidate antecedents A and B. The first variant (CorefQAExt) is an extension of the CorefQA model that uses its predictions to produce probabilities over A, B and N. The second variant (CorefMulti) takes the formulation of a SWAG  style multiple choice classification and the final variant (CorefSeq) takes the standard sequence classification formulation. An ensemble of CorefQAExt, CorefMulti and CorefSeq models shows further performance gains (23.3% absolute improvement in F1 score)."
"While machine learning is promising, it can't solve everything. An algorithm that achieves human-level performance on image recognition in the ImageNet challenge can identify categories labeled in the ImageNet dataset, but cannot necessarily do anything beyond that. Current systems are incapable of reasoning the way humans do about language and images. As a result, for these domains, machine learning, and especially deep learning, is data hungry. The famed ImageNet database started with one million images, and the dataset is routinely augmented by additional labeled examples to train more and more complicated networks. In natural language processing, current networks are pretrained on a corpus of 3.3 billion words <REF>. In contrast, in an early stage clinical trial, we rarely even have a hundred outcomes. Data likewise are rare in many other scientific domains. Blindly applying large-scale deep networks or other complex machine learning techniques to these applications does not generally help us predict or understand the data better over classical techniques."
"Meanwhile, neural network-based representations continue to advance nearly all areas of NLP, from question answering <REF> to named entity recognition ) (a close analog of concept extraction). Recent advances in contextualized representations, including  and BERT , have pushed performance even further. These have demonstrated that relatively simple downstream models using contextualized embeddings can outperform complex models  using embeddings such as word2vec  and GloVe ."
"Second, modern open-domain QA systems are generally composed of two parts: a retriever that obtains relevant segments of text, and a machine reading comprehension (MRC) model that extracts the answer from the text <REF>. For our retriever, we propose the use of a hierarchical TF-IDF retriever that is efficiently able to trade off between n-gram features and the number of documents retrieved. We chose raw Wikipedia text as our information source instead of knowledge bases  which are commonly used for open-ended QA as it enables our approach to tackle other domains and settings with little adaptation. Now there has been remarkable progress in designing neural MRC models that read and extract answers from short paragraphs; we selected two of the best performing models on the SQuAD dataset  as our document readers. The first is QANet , an efficient convolution and selfattention-based neural network, and the second is BERT , a transformer-based pre-trained model. From the document retriever and reader we build an open domain QA system named SOQAL by combining confidence scores from each. We evaluated our system components on the crowdsoured ARCD dataset: Our hierarchical TF-IDF retriever is competitive with Google Search, and our BERT reader is the current state-of-the-art for reading comprehension. Finally, our open domain system SOQAL achieves a respectable 27.6 F1 on ARCD."
"Following the common practice in deep-learning feature transfer <REF>, we use the SGAE pre-trained D as the initialization for the D in our overall encoder-decoder for image captioning. In particular, we intentionally use a very small learning rate (e.g., 10 −5 ) for fine-tuning D to impose the sharing purpose. The overall training loss is hybrid: we use the cross-entropy loss in Eq. (2) for 20 epochs and then use the RL-based reward in Eq. (3) for another 40 epochs."
"Natural language processing methods where representations include global, long-range information can yield a boost in performance on various tasks <REF>. Clinical notes require capturing interactions between distant words. The need to model this long-range structure makes clinical notes suitable for contextual representations like in the bidirectional encoder representations from transformers (bert) model . We develop ClinicalBert by applying bert to clinical notes. Concurrent to our work,  apply bert to biomedical literature and  apply bert on clinical notes and discharge summaries."
"In building NLP models, a word embedding layer that transforms a sequence of tokens in text into a vector representation is considered as one of the fundamental components. In recent studies, it has been shown that the pre-trained language models by using a huge diversity of corpus (i.e. BERT <REF> and ELMo ) generate deep contextualized word representations. These methods have shown to be very effective for improving the performance of a wide range of NLP tasks by enabling better text understanding and have become a crucial part of the tasks since they have published."
"We follow the data collection method used by Natural Questions (NQ) <REF> to gather 16,000 naturally occurring yes/no questions into a dataset we call BoolQ (for Boolean Questions). Each question is paired with a paragraph from Wikipedia that an independent annotator has marked as containing the answer. The task is then to take a question and passage as input, and to return ""yes"" or ""no"" as output. Following recent work , we focus on using transfer learning to establish baselines for our dataset. Yes/No QA is closely related to many other NLP tasks, including other forms of question answering, entailment, and paraphrasing. Therefore, it is not clear what the best data sources to transfer from are, or if it will be sufficient to just transfer from powerful pretrained language models such as BERT  or . We experiment with state-of-the-art unsupervised approaches, using existing entailment datasets, three methods of leveraging extractive QA data, and using a few other supervised datasets."
"In 2018, the BERT (Bidirectional Encoder Representations from Transformers) language representation model achieved state-of-the-art performance across NLP tasks ranging from sentiment analysis to question answering <REF>. Recently, the OpenAI GPT-2 (Generative Pretrained Transformer-2) model outperformed other models on several language modeling benchmarks in a zero-shot setting ."
"In this study, we propose a method that combines the q-Q similarity obtained by unsupervised model and the q-A relevance learned from the collected QA pairs. <REF> shows the proposed model. Previous studies show that neural methods (e.g., LSTM and CNN) work effectively in learning q-A relevance. Here we use the recentlyproposed model, BERT . BERT is a powerful model that applies to a wide range of tasks and obtains the state-of-the-art results on many tasks including GLUE  and SQuAD . An unsupervised retrieval system achieves high precision, but it is difficult to deal with a gap between the expressions of q and Q. By contrast, since BERT validates the relevance between q and A, it can retrieve an appropriate QA pair even if there is a lexical gap between q and Q. By combining the characteristics of two models, we achieve a robust and high-performance retrieval system."
"Learning meaningful representations of multi-turn dialog contexts is the cornerstone of dialog systems. In order to generate an appropriate response, a system must be able to aggregate information over multiple turns, such as estimating a belief state over user goals <REF> and resolving anaphora co-references . In the past, significant effort has gone into developing better neural dialog architectures to improve context modeling given the same in-domain training data . Recent advances in pretraining on massive amounts of text data have led to state-of-theart results on a range of natural language processing (NLP) tasks  including natural language inference, question answering and text classification. These promising results suggest a new direction for improving context modeling by creating general purpose natural language representations that are useful for many different downstream tasks. * * Equal contribution."
"Baseline Systems: We implement one basic method and four state-of-the-art models as the baseline. we use EternalFeather project 2 to process wikidump data 3 , translate the text into traditional Chinese using OpenCC 4 . We use EternalFeather project to train a CBow word embedding provided by <REF> with 300 dimensions on the processed wiki text. We use this pre-trained word embedding in RNet, QANet and BiDAF model.We use TF-IDF as basic traditional method.We count TF-IDF score of every sentence in paragraph and question. For each question, we find the most similar sentence in the related paragraph using cosine similarity and consider it as answer. The TF-IDF method achieve F1 score 17% but get 0.05% on exact match score.  proposed R-net, which is a widely used MRC model. We adjust Yereval Project 5 to process Chinese character and achieve F1 score 38% and exact match score 23.8%. We use BiDAF which is implemented by . We use pre-trained word embedding instead of randomly initialize. The other hyper-parameters remain the same as . The result of BiDAF model is F1 score 51.18% and exact match score 28.08%.  propose QANet. We adapt non-official implementation 6 and we change word tokenizer to jieba 7 in order to tokenize Chinese word. We get F1 score 78.03% and exact match score 65.56% without changing the hyperparameter setting of QANet model. BERT is released by . We use the pre-trained Chinese representation model 8 and fine-tune on DRCD using released code 9 . The final result of BERT model is F1 score 89.59% and Exact Match score 82.34%."
"Modern neural architectures for NLP are highly effective when provided a large amount of labelled training data <REF>. However, a large labelled data set is not always readily accessible due to the high cost of expertise needed for labelling or even due to legal barriers. Researchers working on such tasks usually spend a considerable amount of effort and resources on collecting useful external data sources and investigating how to transfer knowledge to their target tasks . Recent transfer learning techniques make the most of limited labelled data by incorporating word vectors or LMs pretrained on a large amount of unlabelled data. This produces dramatic improvements over a range of NLP tasks where appropriate unlabelled data is available ."
"Pretrained Language Models (LM) like ULMfit <REF>, ELMo , OpenAI GPT  and BERT , proposed different neural language model architectures and made their pre-trained weights available to ease the application of transfer learning to downstream tasks, where they have pushed the state-of-the-art for several benchmarks including question answering on SQuAD, NLI, cross-lingual NLI and named identity recognition (NER)."
"information between input and response) or is too expensive to use in decoding (e.g."" a large pre-trained language model such as BERT <REF>) to pick the final response. Li et al.  proposed to use Maximum Mutual Information (MMI)"
"We validate both techniques on neural models which process a synthetic language for arithmetic expressions with a simple syntax and semantics and show that they behave as expected in this controlled setting. We further apply our techniques to two neural models trained on English text, Infersent <REF> and BERT , and show that both models encode a substantial amount of syntactic information compared to random models and simple bag-of-words representations; we also show that according to our metrics syntax is most salient in the intermediate layers of BERT. The dominance of deep learning models in NLP has brought an increasing interest in techniques to analyze these models and gain insight into how they encode linguistic information. For an overview of analysis techniques, see . The most widespread family of techniques are diagnostic models, which use the internal activations of neural networks trained on a particular task as input to another predictive model. The success of such a predictive model is then interpreted as evidence that the predicted information has been encoded by the original neural model. The approach has also been called auxiliary task , decoding , diagnostic classifier  or probing ."
"End-to-end models have been made possible by deep learning, which automatically learns hierarchical representations of the input signal <REF>. Speech is natural to represent in a hierarchical way: waveform → phonemes → morphemes → words → concepts → meaning. However, because speech signals are high-dimensional and highly variable even for a single speaker, training deep models and learning these hierarchical representations without a large amount of training data is difficult. The computer vision , natural language processing , and ASR  communities have attacked the problem of limited supervised training data with great success by pre-training deep models on related tasks for which there is more training data. Following their lead, we propose an efficient ASR-based pre-training methodology in this paper and show that it may be used to improve the performance of end-toend SLU models, especially when the amount of training data is very small."
"Motivated by the forward-looking structure, we propose to adopt neural-network language models for learning the reward functionr for credit assignment. In nature language processing, a language model assigns probability to a given sequence in a language <REF>. A more tangible and related model is to assign probability of an upcoming word given a sequence of prior words. More formally, a language model predicts the probability of word w t by parameterizing the conditional distribution p(w t |w 1 , w 2 , . . . , w t−1 ). Such models would be very useful in many applications, especially those generating sequences as output. Notable examples including the n-gram models and the recurrent neural networks, which attempt to capture medium-to long-range dependencies in the sentence. Very recently, a model entirely based on attention mechanisms was proposed for language modeling and has achieved state-of-the-art performance on neural machine translation and other NLP tasks . The model, called Transformer, has an encoder-decoder structure and is composed of stacked selfattention and fully connected layers, without using any recurrence or convolution. To attend multiple parts of the input sequence simultaneously, instead of using a single large attention ""head"", the Transformer uses multiple small attention heads to project the input sequence into multiple subspaces and combines the attention outputs by concatenation. Note that in Transformer, the self-attention for constructing an latent vector at a word is based on all other words prior to the current one, which highly resembles the desideratum for the reward function, as discussed above. Furthermore, it is also possible to use recurrent neural networks or convolutional neural networks to model the reward function."
"Meanwhile, recent advances in neural network learning have shown that training regimens that jointly consider evidence from multiple sources can improve performance -both multi-task learning <REF> and fine tuning . However, existing SCL-based methods treat the representation learning and task learning as separate tasks, so the parameters of the representation learning machinery are fixed before training for the downstream task. Jointly learning the representation-and task-specific parameters can potentially allow a learning algorithm to find representations that are better suited for the task."
"Contextual word representations have recently been used to achieve state-of-the-art performance across a range of language understanding tasks <REF>. These representations are obtained by optimizing a language modeling (or similar) objective on large amounts of text. The underlying architecture may be recurrent, as in ELMo , or based on multi-head self-attention, as in OpenAI's GPT  and BERT , which are based on the Transformer . Recently, the GPT-2 model  outperformed other language models in a zeroshot setting, again based on self-attention."
"The models on each classification task are trained and examined using the open-source dataset provided by <REF>, where each task is assigned 100k sentences for training and 10k sentences for validating and testing. Each of our probing model consists of 3 encoding layers followed by a MLP classifier. For each encoding layer, we employ a multihead self-attention block and a feed-forward block as in TRANSFORMER-BASE, which have achieved promising results on several NLP tasks . The mean of the top encoding layer is served as the sentence  : Classification accuracies on 10 probing tasks of evaluating the linguistic properties (""Surface"", ""Syntectic"", and ""Semantic"") learned by sentence encoder. ""BASE"" denotes the standard linear transformation, ""SIMPLE"" is the simple routing algorithm, and ""EM"" is the EM routing algorithm."
"Recent work on language understanding has demonstrated the effectiveness of pretraining neural networks on large corpora using unsupervised objectives such as language modelling, and then fine-tuning the resulting models on downstream target tasks <REF>. This approach has produced new state-of-the-art results on a variety of popular benchmark datasets, such as the SQuAD question answering dataset , and the General Language Understanding Evaluation (GLUE) benchmark for sentence (and sentence pair) classification . Notably, these approaches typically fine-tune a full copy of the pretrained model on each target task individually, effectively multiplying the number of parameters that must be trained and stored by the number of tasks, and ruling out any potential performance improvements that may arise from sharing information between related tasks."
"We demonstrate that carefully chosen auxiliary tasks that are inherently relevant to a main task can be leveraged to improve the performance on the main task. An interesting line of future work is to explore the design of such tasks or explore the properties or similarities between the auxiliary and the main tasks. Another relevant line of work is adapting our model to other domains containing documents with similar linked structured such as Wikipedia articles. Future work may benefit from replacing ELMo with other types of contextualized representations such as BERT in our scaffold model. For example, at the time of finalizing the camera ready version of this paper, <REF> showed that a BERT contextualized representation model  trained on scientific text can achieve promising results on the SciCite dataset."
"For the purpose of this paper, we consider neural network solution for multilingual named entity recognition for Bulgarian, Czech, Polish and Russian languages for the BSNLP 2019 Shared Task <REF>. Our solution is based on BERT language model , use bidirectional LSTM , Multi-Head attention , NCRFpp ) (being neural network version of CRF++framework for sequence labelling) and Pooling Classifier (for language classification) on the top as additional information."
"Our strategy is to draw on text embedding methods to reduce the dimension of the text [e.g., <REF>. Informally, a text embedding method distills the text of each document to a real-valued vector, and these embeddings can be used as features for prediction problems. Black-box embedding methods are state-of-the-art for a range of natural language understanding tasks . Here, we will adapt embedding methods in the service of causal inference."
"We propose an ensemble approach composed of two deep learning models, the Hierarchical LSTMs for Contextual Emotion Detection (HRLCE) model and the BERT model <REF>. The BERT is a pre-trained language model that has shown great success in many NLP classification tasks. Our main contribution consists in devising the HRLCE model.  illustrates the main components of the HRLCE model. We examine a transfer learning approach with several pre-trained models in order to encode each user utterance semantically and emotionally at the word-level. The proposed model uses Hierarchical LSTMs  followed by a multi-head self attention mechanism ) for a contextual encoding at the utterances level."
"There has recently been rapid progress in developing contextual word representations that improve accuracy across a range of natural language tasks <REF>. While we have shown in previous work ) that such representations are beneficial for constituency parsing, our earlier results only consider the LSTM-based ELMo representations , and only for the English language. In this work, we study a broader range of pre-training conditions and experiment over a variety of languages, both jointly and individually."
"Recent breakthroughs of language models (LM) pre-trained on large corpora like ELMo <REF> and BERT  clearly show that unsupervised LM pre-training can vastly improve performance of downstream models. To fully utilize the knowledge encoded in PubMed abstracts, DECBAE uses BioELMo , a domain adapation verison of ELMo, to embed the words. After the embedding layer, DECBAE applies abbreviation-specific bidirectional LSTM (biLSTM) classifiers to do the abbreviation expansion, where the biLSTM parameters are trained separately for each abbrevi-ation. We train DECBAE from the automatically collected dataset of 950 ambiguous abbreviations."
"Previous attempts at using language for RL tasks in these ways have mostly been limited to relatively small corpora <REF>, or synthetic language . We argue that recent advances in representation learning  make it worth revisiting this research agenda with a much more ambitious scope. While the problem of grounding (i.e. learning the correspondence between language and environment features) remains a significant research challenge, past work has already shown that high-quality linguistic representations can assist cross-modal transfer outside the context of RL (e.g. using semantic relationships between labels to enable zero-shot transfer in image classification )."
"Extractive open-domain question answering (QA) is usually referred to the task of answering an arbitrary factoid question (such as ""Where was Barack Obama born?"") from a general web text (such as Wikipedia). This is an extension of the reading comprehension task <REF> of selecting an answer phrase to a question given an evidence document. To make a scalable opendomain QA system, One can leverage a search engine to filter the web-scale evidence to a few documents, in which the answer span can be extracted using a reading comprehension model . However, the accuracy of the final QA system is bounded by the performance of the search * Equal contribution.  Visit nlp.cs.washington.edu/denspi for code & demo. engine due to the pipeline nature of the search process. What is more, running a neural reading comprehension model ) on a few documents is still computationally costly, since it needs to process the evidence document for every new question at inference time. This often requires multi-GPU-seconds or tens to hundreds of CPU-seconds-BERT  can process only a few thousand words per second on an Nvidia P40 GPU."
"Transfer/Multitask Learning: transfer learning has already been established as the tool to enable learning of new tasks by leveraging already learned ones <REF>. The transferability of tasks to related ones has also been explored, . Recently, the work of  has shown a method of predicting which feature extractors will perform well on a given task. In multi-task learning, a single network is adapted to multiple tasks . The shared representation is more compact than using an exclusive network for each task independently. We propose not to adapt one net to multiple representations, but to adapt multiple representations to a single task. Related approaches exist in NLP where pretrained representations via multiple tasks turn out useful for many downstream ones ."
"After 100 epochs, we select the model that fared best on the development set. We use GloVe embeddings <REF> for our English models and zzgiga embeddings  for the Chinese models, for a more homogeneous comparison against other parsers . ELMo  or BERT  could be used to improve the precision, but in this paper we focus on keeping a good speed-accuracy tradeoff. For SPMRL, no pretrained embeddings are used, following . As a side note, if we wanted to improve the performance on these languages we could rely on the CoNLL 2018 shared task pretrained word embeddings  or even the multilingual BERT model 6 . Our models are run on a single CPU 7 (and optionally on a consumer-grade GPU for further comparison) using a batch size of 128 for testing. Additional hyperparameters can be found in Appendix A.  : Results on the PTB dev set, compared against . DE refers to dynamic encoding and MTL to a model that additionally casts the problem as multi-task learning. Each auxiliary task is added separately to the baseline with DE and MTL. Policy gradient fine-tunes the model that includes the best auxiliary task."
"Our dialogue agent's models are built on the Transformer architecture <REF>, which has been shown to perform well on a variety of NLP tasks , including multiple persona-based chat applications . For the SATISFACTION task, the context x is encoded with a Transformer and converted to the scalar satisfaction predictionŝ by a final linear layer in the task head. The DIALOGUE and FEEDBACK tasks are set up as ranking problems, as in , where the model ranks a collection of candidate responses and returns the top-ranked one as its response. The context x is encoded with one Transformer andŷ andf candidates are encoded with another. The score for each candidate is calculated as the dot product of the encoded context and encoded candidate."
"One way to group graph embedding techniques is based on the type of input they can incorporate. Inputs can be homogeneous where all nodes are of the same type, heterogeneous with multiple types of nodes and auxiliary information graphs that contain node, edge or neighborhood features. In homogeneous graphs, the challenge is to encode the neighborhood topology of the nodes in a computationally feasible manner <REF>, . The latent vectors are expected to preserve different orders of node proximity (e.g. ) and different ranges of structural identity (e.g. , ). Therefore, the rich contextual information they carry makes node embeddings useful for multiple unsupervised learning tasks such as predicting missing links  as well as recommendation and ranking of the most relevant nodes , . Furthermore, modifying the properties of random walks can assist the learned embeddings in encapsulating both local and global graph properties , , . The problem of heterogeneous graph embedding was addressed with metapath2vec , where metapaths among specific entities types are defined and then random walks are generated only in accordance to those metapath schemes. This approach was extended in  to include node attributes and multiplex edges. Further advancements have allowed the incorporation of node and edge feature vectors (auxiliary information) to facilitate inductive learning of representations . In these works, the estimation of node embeddings proceeds through typical sampling-based approaches , .   Many internet-scale recommendation systems use graph embedding techniques to supply millions of customers with potentially useful or interesting content related to their past interests , , , . These systems typically model vertices as users, content, or products on very large graphs, and several instances of graph embeddings techniques  have been applied to networks with millions of unique entities, with even a few applications in the financial services space  using autoencoders to create embeddings on account transaction data. Embeddings from these approaches are typically used in downstream applications like product recommendation ,  and maximizing proper ad placement . This transfer learning approach is very similar to the impact that word embeddings have had for a variety of NLP tasks , , . To our knowledge, the method proposed in this paper is the first application of graph-embeddings to financial transactions."
"First, this challenge highly depends on the attention mechanism across different domains. Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN) are naturally suitable to encode the language text and visual image respectively; however, encoded features of RNN has autoregressive property which is different from the local dependency of CNN. The multi-head self-attention transformer <REF> can mimic the convolution operation, and allow each head to use different linear transformations, where in turn different heads can learn different relationships. Unlike RNN, it reduces the length of the paths of states from the higher layer to all states in the lower layer to one, and thus facilitates more effective learning. For example, the BERT model , that is completely built upon self-attention, has achieved remarkable performance in 11 natural language tasks. Therefore, we employ transformer in both the text encoder and decoder of our model, and design a novel joint attention mechanism to simulate the relationships among the three domains. Besides, the mixed data format requires the desired attention to support the flexible data stream. In other words, the batch fetched at each iteration can be either uni-modal text data or multi-modal text-image paired data, allowing the model to be adaptive to various data during inference as well."
"Multi-task learning <REF> [3] has recently been applied to natural language processing. By training on multiple tasks jointly, a model learns abstract representations that are task agnostic. This approach can effectively be used as pretraining and is shown to improve many natural language processing tasks . We investigate this approach and apply it to vision and language tasks namely cross-modal retrieval, image captioning and sentence paraphrasing. Our model learns generalized latent representations of image and text. This approach reduces both inference time and memory requirements when compared to task-specific models."
"Being able to comprehend a document and output correct answer given a query/question about content in the document, often referred as machine reading comprehension (RC) or question answering (QA), is an important and challenging task in natural language processing (NLP). Plenty of data sets have been constructed to facilitate research on this topic, such as SQuAD <REF>, NarrativeQA  and CoQA . Many neural models have been proposed to tackle the machine RC/QA problem , and great success has been achieved, especially after the release of the BERT ."
"Considering the limitations of unidirectional architecture applied in previous pre-training models like GPT, Devlin et al. <REF> propose a new one named BERT (Bidirectional Encoder Representation from Transformers) . With the masked language model (MLM) and next sentence prediction task, BERT is able to pre-train deep contextualized representations with bidirectional Transformer, encoding both left and right context to word representations. As Transformer architecture cannot extract sequential information, Devlin et al. add positional embeddings to encode position. Owing to bidirectional language model and Transformer architecture, BERT outperforms state-of-the-art models in eleven NLP tasks. In particular, for MRC tasks, BERT is so competitive that just utilizing BERT with simple answer prediction approaches can show promising performance. Despite of its outstanding performance, pre-training process of BERT is time and resource consuming which makes it nearly impossible to be pre-trained without abundant computational resources."
"More recent approaches to sentiment, however, have concentrated on learning the effects of negation in an end-to-end fashion. Current state-of-the-art approaches employ neural networks which implicitly learn to resolve negation, by either directly training on sentiment annotated data <REF>, or by pre-training the model on a language modeling task . State-of-the-art neural methods, however, have not attempted to harness explicit negation detection models and annotated negation datasets to improve results. We hypothesize that multi-task learning (MTL)  is an appropriate framework to incorporate negation information into neural models."
"We have presented a hierarchical multi-task learning framework for discourse coherence that takes advantage of inductive transfer between two tasks: predicting the GR type of words at the bottom layers of the network and predicting a document-level coherence score at the top layers. We assessed the extent to which our framework generalizes to different domains and prediction tasks, and demonstrated its effectiveness against a number of baselines not only on standard binary evaluation coherence tasks, but also on tasks involving the prediction of varying degrees of coherence, achieving a new state of the art. As part of future work, we would like to investigate the use of contextualized embeddings (e.g., BERT, <REF>) for coherence assessment -as such representations have been shown to carry syntactic information of words  -and whether they allow multi-task learning frameworks to learn complementary aspects of language."
"Deep learning has been a powerful and successful tool to lead the era of artificial intelligence (AI) in recent few years. It has achieved surprising or even over human-level performance on image classification <REF>, speech recognition , reading comprehension  etc. On the other hand, autonomous driving, personal assistant devices (Google home, Alexa etc), AI for health-care are emerging topics with the aforementioned breakthroughs from deep learning."
"Interestingly, however, the models with both contextual learned features (ELMo) and handengineered token-level features perform slightly better than those without the hand-engineered features across the board, suggesting that there is some (small) amount of contextual information relevant to generalization that the contextual learned features are missing. This performance boost may be diminished by improved contextual encoders, such as BERT <REF>."
"Transformer. More recently, <REF> introduced the transformer model as a more sophisticated architecture for sequence to sequence transduction. Its underlying architecture follows the encoder-decoder paradigm, but no recurrent connections between tokens are used which reduces the training time for the model. In order to capture relations between tokens, a complex attention mechanism called multi-headed self-attention is applied and combined with positional encoding for signaling the order of tokens. Due to its success, variants of the transformer model for machine translation are currently being developed in a very fast pace. In the past, language modelling has commonly been interpreted as a left-to-right task, similar to incremental human language processing . As a consequence, the self-attention layer could only attend to previous tokens.  argue that this approach unnecessarily limits the expressivity of the sentence representation. They propose to change the training objective from predicting the next word to predicting a randomly masked token in the sentence by considering both the left and right context. This task is also known as the cloze task .  use this training objective to train a multi-layer bidirectional transformer (called BERT) and find that it strongly outperforms the previous state of the art on the GLUE evaluation corpus . By now, they have also released a multilingual version of BERT for 104 languages.  BERT and LASER obtain comparable results on the cross-lingual entailment dataset . For this article, we decided to use LASER because the model already outputs sentence representations that have a uniform dimensionality independent of the length of the sentence. This makes it possible to avoid additional experimental parameters for scaling the dimensionality of the sentence representations. The model has been trained by combining multiple multilingual corpora from the OPUS website  accumulating to a total of 223 million parallel sentences. 4 Note that the sentence-based model is optimized for translation whereas the word-based model aims at optimizing both monolingual semantic similarity and cross-lingual translation constraints. These different training objectives might have an influence on the model's ability to capture semantic differences between languages."
"Deep Neural Networks (DNNs) have achieved great success in a broad range of applications in image recognition <REF>, natural language processing , and games . Latest DNN architectures, such as ResNet , DenseNet  and Wide-ResNet , incorporate hundreds of millions of parameters to achieve state-of-the-art predictive performance. However, the expanding number of parameters not only increases the risk of overfitting, but also leads to high computational costs. Many practical real-time applications of DNNs, such as for smart phones, drones and the IoT (Internet of Things) devices, call for compute and memory efficient models as these devices typically have very limited computation and memory capacities."
"User intent understanding plays a fundamental role in modern information retrieval. A better understanding of ""what a user wants"" helps search engines return more relevant documents, suggest more useful queries, and provide more precise answers. Intent understanding is also challenging: two queries with the same intent may have no term overlap (e.g. ""cheap cars"", ""low-priced autos"") while two queries with slight variations may have completely different meanings (e.g. ""horse racing"", ""racing horses""). A brittle bag-of-words style of query representation faces such challenges as vocabulary mismatch and ambiguity <REF>. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Distributed representations (embeddings) provide a path to address these challenges and have been increasingly explored with the rapid development of neural methods. Representing text as continuous embeddings enables softer matches than term overlap. The embeddings also incorporate additional information. For example, word2vec maps semantically similar words together by learning they often appear in similar surrounding texts . The embeddings also support richer compositionality than bag-of-words using neural networks. For example, ELMo and BERT use deep neural architectures to provide more context-aware embeddings ."
"• The BERT model belongs to the class of contextual word embeddings. This approach has been rapidly become popular with works by <REF>, ,  and . Instead of using a direct mapping of words to vector representations, these approaches pre-train a neural language model on a large amount of text. The language model's internal state for each input word is then used as a corresponding word vector representation for a different task. They can be viewed as inducing word vector representations that are specific to the surrounding context. We compare against the current state-of-the-art model BERT . For this, the output of BERT's last Transformer layer is used as the sequence (v 1 , ..., v n ).  : Median rank, accuracy @10 and @100 and mean reciprocal rank of single-compared to multisense target vectors. The first row is the model architecture proposed by ."
"Finally, the increasingly popular contextualized embedding algorithms, e.g., BERT <REF> or ELMo , are also probabilistic in nature and should thus be affected by stability problems. A direct transfer of our type specific evaluation strategy is impossible. However, an indirect one could be achieved by averaging token-specific contextualized embeddings to generate type representations."
"Transformer encoder-decoder models <REF> have become popular in natural language processing. The Transformer architecture allows to successfully train a deep stack of self-attention layers  via residual connections  and layer normalization . The positional encodings , typically based on sinusoidal functions, are used to provide the self-attention with the sequence order information. Across various applications, systematic improvements have been reported over the standard, multi-layer long short-term memory (LSTM)  recurrent neural network based models. While originally designed as an encoder-decoder architecture in machine translation, the encoder (e.g., ) and the decoder (e.g., ) components are also separately used in corresponding problems depending on whether the problem disposes the whole sequence for prediction or not."
"Recently, deep learning models have been proposed to learn powerful input representations for text classification <REF> as well as the XMC problem . In fact, not until last year, the Natural Language Processing (NLP) community is witnessing a dramatic paradigm shift towards pretrained deep language representation models, which achieves state-of-the-art across many NLP tasks such as question answering, semantic role labeling, parsing, sentence classification with very few labels, and more. Bidirectional Encoder Representations from Transformers (a.k.a BERT ) represents one of the latest developments in this line of work. BERT outperforms its predecessors, ELMo  and GPT , staggeringly exceeding state-of-the-art by a wide margin on multiple natural language understanding tasks. Nevertheless, it is very challenging to finetune BERT models on XMC task without a careful design. The main challenges are the difficulty to capture label dependency from heterogeneous sources and the tractability to scale to extreme-label setting because of the huge model size and the additional Softmax layer with size depending linearly on the output space."
"We followed the unsupervised training approach in <REF> to train two MT systems, one for EN↔GU and a second for HI→GU. 7 This involves training unsupervised NMT models with an additional supervised MT training step. Initialisation of the models is done by pre-training parameters using a masked language modelling objective as in Bert , individually for each language (MLM, which stands for masked language modelling) and/or cross-lingually (TLM, which stands for translation language modelling). The TLM objective is the MLM objective applied to the concatenation of parallel sentences. See  for more details."
"In this paper, we proposed a hierarchical attention model for the buyer-seller negotiation. Our model, Visual Negotiator, consists of 1) an attention-based approach that can initially estimate the value of the item conditioned on its visual and textual features, and 2) a hierarchical end-to-end dialogue model that generates an utterance and proposes a price based on the initial pricing. Experiments on CraigslistBargain dataset shows the superior performance of the proposed model linguistically as well as price-wise. Although the proposed models generate dialogues akin to humans, we believe that there is a long way to build a system that can compete humans in understanding, planning, and following a strong strategy towards its goal. In future we consider improving the current approach by: (1) expanding the dataset to encompass more samples to improve the initial price estimator module; (2) applying reinforcement learning on both language generation and price estimation of the system; and (3) applying pre-trained language models based on transformers, such as BERT <REF>, that may improve the understanding and generation performance."
"It is proven that the context-dependent embedding method named Bidirectional Encoder Representations from Transformers (BERT) achieved new state-of-the-art results on some downstream tasks like question answering and text classification <REF>. Due to these exciting achievements, in this paper, sentence embeddings are derived by the pre-trained BERT model 1 . For sentence s = {t 0 , t 1 , · · · , t n , t n+1 }, where t i (0 ≤ i ≤ n + 1) indicates the i th word in sentence, t 0 is a special tag CLS used for classification tasks and t n+1 is another special tag SEP utilized to split the sentences. Every word in the sentence including CLS and SEP will be encoded into a lowdimensional embedding w i (w i ∈ R d ) based on BERT. In this paper, the average of the hidden states of the penultimate transformer layer along the time axis is exploited to represent the sentence. Concretely, the representation of sentence s is expressed as the following equation."
"Second, we propose adding incorporative distant representation into the model via a self-attention mechanism, which can focus on the keywords of dependent clauses that are far from the considered word. Self-attention has been used in many recent state-of-the-art models, most notably the transformer <REF> and BERT . BERT has outperformed Bi-LSTM on numerous tasks, including question answering and language inference. Therefore, we choose to use self-attention modules to extract distant representations along with local representations to improve model accuracy."
"It has been shown that neural networks, such as LSTM and CNN, can outperform conventional machine learning methods without requiring handcrafted features. <REF> applied a CNN-CRF model and gained competitive results to the best statistical models. More recently, the LSTM-CRF architecture has been used on NER tasks.   extract word-level context information and  futher introduced hierarchy structure by incorporating BiLSTM-based character embeddings. Many works integrating word-level information and character-level information have been found to achieve good performance , , , , . External knowledge has also been exploited for NER tasks. To utilize character-level knowledge, character-level pre-trained  and co-trained  neural language models were introduced. Recently, many works exploit learning pre-trained language representations with deep language models to improve the performance of downstream NLP tasks, such as ELMo  and BERT ."
"Bidirectional models. Classically, bidirectional language models such as log-linear models and Markov random fields have been pursued, but they require either approximate inference <REF> or approximate sampling . Unlike bidirectional models, autoregressive models must impose a specific ordering, and this has been shown to matter across natural language processing tasks . Bidirectionality such as in encoders have been shown to significantly improve results in neural machine translation . Most recently, BERT has shown bidirectional representations can significantly improve transfer tasks . In this work, discrete autoregressive flows enable bidirectionality while maintaining the benefits of a (tractable) generative model."
"In this work, we propose fully non-recurrent and label-recurrent model paradigms including self-attention and convolution for comparison to state-of-the-art recurrent models in terms of accuracy and speed. To achieve this, we design a framework for joint IC-SL models that is modularized into different components and makes the task agnostic to type of neural network used. This, in turn, makes the model architecture simpler, easy to understand and renders the task network agnostic, allowing for easier plug and play using existing components, such as pre-trained contextual word embeddings <REF>, etc. This is essential for easier model debugging and quicker experimentation, especially in industrial setting."
"We also benchmark several pretrained embeddings on both semantic similarity and syntactic similarity datasets, including GloVe <REF>, 3 SkipThought , 4 InferSent , 5 ELMo , 6 and BERT . 7 For GloVe, we average word embeddings to form sentence embeddings. For ELMo, we average the hidden states from three layers and then average the hidden states across time steps. For BERT, we use the averaged hidden states from the last attention block."
"The importance of supervised information for ML development cannot be understated. The ImageNet <REF>), a collection of more than 1.5 million labelled images distributed over 1, 000 classes, facilitates the development of new model such as ResNet  that surpasses human performance in image recognition. In another instance, the Stanford Question Answering Dataset (SQuAD) , a reading comprehension dataset consisting of 150, 000 questions and answers, is critical to the development of a powerful language representation model BERT . Recognizing the importance of high-quality data for ML model developments, the chemistry community has recently compiled a comprehensive collection of benchmarking datasets, the MoleculeNet , including a variety of supervised learning tasks. Despite this effort by the Pande group at Stanford, the amount of data in the MoleculeNet is inadequate in comparison to the typical size of ML training datasets. For instance, there are less than 150K molecular entries for training models to predict the quantum mechanical properties of small organic molecules. The biggest dataset QM9 within MoleculeNet is further restricted to curating molecules composed of Hydrogen (H), Carbon (C), Nitrogen (N), Oxygen (O) and Florine (F). Therefore, the highly successful results mentioned earlier are not guaranteed to generalize beyond the scope of existing datasets. A better data variety, such as the presence of more atom types and larger molecular size, can help to more thoroughly investigate and improve some aspects of ML models such as generalibility, transferability and few-shot learning capability."
"where k is the number of masked items. As shown in <REF>,  In practice, we usually perform the masking operation for items in a user session with γ =30 to 40 percent. In addition, to learn a robust model, we do not replace all these masked items with ""__"" in practice. This is in line with existing work ] that a small percentage of noisy and real items will help reduce overfitting and improve the accuracy of deep learning models. Empirically, the best performance is achieved by the following procedure: 60% to 70% of the time (denoted by η=70% ∼ 80%), masked items are replaced with ""__"", half of the remaining time masked items are replaced with real items, and half with randomly sampled fake items from the item pool. By doing this, the CNN decoder does not know which item will be predicted and which item is the real one, and as a result, it is forced to learn the contextual representation of all its surrounded items, i.e., both the left and right context. The gap-filling based SRS is referred to as GfNextItNet."
"A first idea would be to merge the search spaces into a fixed-length, linear space. However, the length of the sequence can be prohibitively large for large |SS|. To remedy this, we propose to merge different search spaces into a larger conditional search space, where the available actions are not only conditioned on past actions, but also on the sampled search space and task pair (ss i , t j ) ∼ Uniform(A). This idea is illustrated in <REF>, where a merged conditional search space of hyperparameters of a convolutional neural net (ss 1 ) and a feedforward net (ss 2 ) is shown. S and T denote the start and terminal state, whereas the annotations on the edges show their availability conditioned on the chosen search space. Notice how the conditional search space allows for merging the states for the fully connected layer of the CNN and the last layer of the FFN. Now, the joint training can be efficiently performed in this conditional search space. Attention-based agent. Models based solely on attention have recently shown promising results in translation , question answering and language inference . So far unexplored in the context of RL-based NAS, a self-attentionbased architecture for the agent possesses several desirable properties for transfer and joint training on multiple search spaces and tasks."
"We evaluate our proposed method by training a parser that directly predicts action sequences from pre-trained BERT <REF> word representations. Two independent projection matrices are applied to the feature vector for the last sub-word unit within each word: one projection produces scores for actions corresponding to that word, and the other for actions at the following fencepost. The model is trained to maximize the likelihood of the correct action sequence, where BERT parameters are fine-tuned as part of training. We compare our model with our previous chart parser , which was fine-tuned from the same initial BERT representations. Unlike the tetra-tagging approach, the chart parser constructs feature vectors for each span in the sentence and uses the cubic-time CKY algorithm for inference."
"There have also been several recent attempts to encode long texts as vectors, mainly for the purpose of using those vectors for text classification tasks. In two such cases <REF>, , SOTA results have been achieved for multiple tasks, while using text classification vectors of equal size to the (single token) embedding. While both the <CLS> vector presented in  and the summation of the contextual token vectors in the document presented in  have achieved impressive results, the constraint of represent both words and long sequences of text as vectors in the same space becomes harder as our text grows longer. This paper is organized as follows: Section 2 describes how long texts are embedded. Section 3 describes the training process required to make such embedding meaningful. Section 4 describes the generation and refining processes for novel texts. Section 5 describes who we keep proper nouns (such as character names) consistent throughout the document, and Section 6 reviews the entire system and its training process."
"We extract contextualized language embeddings from textual sign descriptions using the state-of-the-art language representation model BERT <REF>. BERT architecture basically consists of a stack of encoders; specifically, multi-layer bidirectional transformers . The model's main advantage over word2vec  and glove  representations is that BERT model is contextual and the extracted representations of the words change with respect to other words in a sentence.  shows the t-SNE visualization of all sign class BERT embeddings. A close inspection to this feature space reveals that classes that appear closer in t-SNE embeddings Tap the fingertips of the right curved 5 hand on the right shoulder with a repeated movement."
"Language model pre-training has attracted wide attention and fine-tuning on pre-trained language model has shown to be effective for improving many downstream natural language processing tasks. Dai(Dai and Le, 2015) pre-trained unlabeled data to improve Sequence Learning with recurrent networks. Howard <REF> proposed a general transfer learning method, Universal Language Model Fine-tuning (ULMFiT), with the key techniques for fine-tuning a language model. Radford  proposed that by generative pre-training of a language model on a diverse corpus of unlabeled text, large gains on a diverse range of tasks could be realized. Radford  achieved large improvements on many sentence-level tasks from the GLUE benchmark . BERT  obtained new state-ofthe-art results on a broad range of diverse tasks. BERT pre-trained deep bidirectional representations which jointly conditioned on both left and right context in all layers, following by discriminative fine-tuning on each specific task. Unlike previous works fine-tuning pre-trained language model to perform discriminative tasks, we aim to apply pre-trained BERT on generative tasks by perform the masked language model(MLM) task. To generate sentences that are compatible with given labels, we retrofit BERT to conditional BERT, by introducing a conditional masked language model task and fine-tuning BERT on the task."
We modify the Transformer encoder <REF> as depicted in . Our encoder maps a sentence into a fixed size vector. This is simple and avoids choosing a fixed length compression rate between the input and the latent representation . Our strategy to produce a fixed sized representation from transformer is analogous to the special token employed for sentence classification in .
"Recently, neural networks based on a self-attention mechanism have become popular in the field of Natural Language Processing, as a result of their significant performance improvement over RNN-based methods <REF>. In particular,  introduced a method known as Multi-Head Attention, achieving optimal results with configurations having 8-16 heads. The multi-headed attention consists of several Scaled-Dot Product attention layers (rearranged multiplication and concatenation for better parallelism):"
"The rumour of 1,000 foreigners harvesting children's organs strikes again Based on this observation we focused on the best performing neural models for SNLI 2 as well as BERT <REF>. We used these models in an ensemble and fine-tuned with soft labels in a process summarized in figure 1, that will be outlined in the following section."
"Besides the self-attention, each layer in the encoder and decoder contains a fully connected feed-forward network applied to each position separately and identically. Positional encoding by sine and cosine functions is also added to each embedding, which provides information about the positions in the sequence. Transformer has become a powerful unsupervised representation of word embedding in natural language tasks, and more details about the Transformer and its application can be found in <REF>, ."
"This paper makes the following contributions: i) Learn multi-modal data embeddings using Deep Canonical Correlation Analysis in a One-Step and Two-Step framework to combine text, audio and video views for the improvement of sentiment/emotion detection. The Two-Step DCCA framework further helps to explore the interplay between audio, video and text features when learning multi-modal embeddings for classification tasks. ii) Encode text using BERT <REF>, the current stateof-the-art in text encoders to obtain fixed-length representations for text. There is little literature that uses pre-trained BERT encoders as features without additional fine-tuning. This work adds to the growing body of work that applies BERT as a prefixed feature and iii) perform empirical evaluations on benchmark data sets such as CMU-MOSI  and CMU-MOSEI  along with a new Debate Emotion data set introduced by ."
"Transformer <REF> has achieved the state-of-the-art performance in many NLP tasks . The encoder and decoder in Transformer has N identical layers, and each layer in encoder consists of two different sub-layers: multi-head self-attention and feed-forward network, while the decoder has an additional multi-head attention sub-layer. Multi-head attention is to perform the attention function h times in parallel, allowing the model to jointly attend to information from different representation subspaces at different positions. Residual connection is employed between each sub-layer. Transformer can better model the interactions between any two tokens in the sequence and the computation of each token in the encoder and decoder can be parallel during training, which shows advantages over the RNN based models. To the best of our knowledge, this is the first work to apply Transformer in G2P conversion."
"Language models (LMs) based on sequential LSTMs <REF> have numerous practical applications, but it has also been shown that they do not always develop accurate syntactic generalisations . Thus, one strategy for improving LSTMs is to change their biases to facilitate more linguistically valid generalisations. This paper introduces a scalable method for introducing syntactic biases to LSTMs (and indeed, to any left-to-right language model trained with a cross-entropy objective) by distilling knowledge  from recurrent neural network grammars . RNNGs have been shown to successfully capture non-local syntactic dependencies , achieve excellent parsing performance , and correlate well with encephalography signals . Unfortunately, these benefits come at the expense of scalability, since the hierarchical constituent composition process ( §3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching , RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such, RNNGs are an impractical alternative to computationally convenient architectures that are used to build language models from massive corpora ."
"In this paper, we propose a new model, Dual Co-Matching Network, to match a questionanswer pair to a given passage bidirectionally. Our network leverages the latest breakthrough in NLP: BERT <REF> contextual embedding. In the origin BERT paper, the final hidden vector corresponding to first input token ([CLS]) is used Passage:Alice's mother died when she was five. Although her brothers and sisters were loving and caring, their love couldn't take the place of a mother's. In 1925 Alice became my mother and told me that her family couldn't even afford her a doll. One afternoon in   as the aggregation representation and then a standard classification loss is computed with a classification layer. We think this method is too rough to handle the passage-question-answer triplet because it only roughly concatenates the passage and question as the first sequence and uses question as the second sequence, without considering the relationship between the question and the passage. So we propose a new method to model the relationship among the passage, the question and the candidate answer."
"BERT <REF>, the latest refinement of a series of neural models that make heavy use of pretraining , has led to impressive gains in many natural language processing tasks, ranging from sentence classification to question answering to sequence labeling. In this demonstration, we integrate BERT with the open-source Anserini IR toolkit to create BERTserini, an end-to-end opendomain question answering (QA) system."
"is also a stack of six decoders with each decocder containing the same components as the encoder, but with an additional attention layer that helps the decoder focus on relevant parts of the input sentence. Work using transformer models is still in its infancy. <REF> and  showed impressive results on several NLP tasks. Their work improved the existing state-of-the-art across a wide range of tasks such as language modeling, children's book test, reading comprehension, machine translation, question answering, modeling long range dependencies (LAMBADA), Winograd Schema challenge and summarization."
"This threaten model fits the scenario of using pre-trained models on large-scale dataset as a general feature extractor, which is a common practice in the deep-learning community. In computer vision (CV) field, it is quite common to use ImageNet <REF> pre-trained convolutional neural network (CNN), such as AlexNet , VGG , GoogleNet , ResNet , MobileNet , as a general image feature extractors for other tasks. And in natural language processing (NLP) field, using pre-trained word vector , such as BERT , for other language tasks is also common."
"Deep attention networks are becoming increasingly powerful in solving challenging tasks in various fields, including natural language processing <REF>, and computer vision . Compared to convolution layers and recurrent neural layers like LSTM , attention operators are able to capture long-range dependencies and relationships among input elements, thereby boosting performance . In addition to images and texts, attention operators are also applied on graphs . In graph attention operators (GAOs), each node in a graph attend to all neighboring nodes, including itself. By employing attention mechanism, GAOs enable learnable weights for neighboring feature vectors when aggregating information from neighbors. However, a practical challenge of using GAOs on graph data is that they consume excessive computational resources, including computational cost and memory usage. The time and space complexities of GAOs are both quadratic to the number of nodes in graphs. At the same time, GAOs belong to the family of soft attention , instead of hard attention . It has been shown that hard attention usually achieves better performance than soft attention, since hard attention only attends to important features ."
"Avoiding to build representations from the ground up is a sensible goal for representation learning. Hence, unsurprisingly, knowledge transfer is an active research area in Machine Learning. There has been great successes for transfer in the field of Computer Vision <REF>. Transfer approaches usually reuse the hidden states of deep neural networks trained on canonical tasks as a pre-training step. More recently, transfer in Natural Language Processing (NLP) also made a leap forward. Unsupervised approaches showed encouraging results on several benchmarks ."
"Increasingly complex neural networks have achieved highly competitive results for many NLP tasks <REF>, but they prevent human experts from understanding how and why a prediction is made. Understanding how a prediction is made can be very important for certain domains, such as the medical domain. Recent research has started to investigate models with self-explaining capability, i.e. extracting evidence to support their final predictions . For example, in order to make diagnoses based on the medical report in , the highlighted symptoms may be extracted as evidence."
"Two recent advances in the unsupervised modeling of natural language, Embeddings of Language Models (ELMo)  and Bidirectional Encoder Representations from Transformers (BERT) <REF>, have led to drastic improvements across a variety of shared tasks. Both of these methods use transfer learning, a method whereby a multi-layered language model is first trained on a large unlabeled corpus. The weights of the model are then frozen and used as input to a task specific model . This method is particularly well-suited for work in the medical domain where datasets tend to be relatively small due to the high cost of expert annotation."
"ELMo (Peters et al., 2018) and BERT <REF> can be used to generate contextualized word representations by combining internal states of different layers in neural language models. Contextualized word representation can help to improve performance in various NLP tasks by incorporating contextual information, essentially allowing for the same word to have distinct context-dependent meanings. This could be particularly powerful for chemical NER since generic chemical names (e.g. salts, acid) may have different meanings in other domains. We therefore explore the impact of using contextualized word representations for chemical patents. We train ELMo on the same corpus of 84K  patents (detailed in ), which we use for training the ChemPatent embeddings (described in Section 3.4). We use the ELMo implementation provided by Peters et al.  with default hyper-parameters. 2 Such neural language models require a large amount of computational resources to train. In ELMo, a maximum character sequence length of tokens is set to make training feasible. However, systematic chemical names in chemical patents are often longer than the typical maximum sequence length of these neural language models. As very long tokens tend to be systematic chemical names, we reduced the max length of word from 50 to 25 and replace tokens longer than 25 characters by a special token ""Long Token""."
"Recently, efforts have been invested in developing large-scale datasets for commonsense reasoning. In SWAG <REF>, given a textual description of an event, a probable subsequent event needs to be inferred. However, it has been quickly realized that models trained on large amounts of unlabeled data  capture well this type of information and performance on SWAG is already at human level. VCR  is another very recent attempt that focuses on the visual aspects of common sense. Such new attempts highlight the breadth of commonsense phenomena, and make it evident that research on common sense has only scratched the surface. Thus, there is need for datasets and models that will further our understanding of what is captured by current NLU models, and what are the main lacunae."
"Natural Language Understanding (NLU) has received increasing research attention in recent years. With language models trained on large corpora <REF>, algorithms show better performance than humans on some benchmarks . Compared to humans, however, most endto-end trained systems are rather weak on common sense. For example, it is straightforward for a human to understand that someone can put a turkey into a fridge but he can never put an elephant into a fridge with basic commonsense reasoning, but it can be non-trivial for a system to tell the difference. Arguably, commonsense reasoning should be a central capability in a practical NLU system ; it is, therefore, important to be able to evaluate how well a model can do for sense making."
"Over the last 15 years, approaches to sentiment analysis which concentrated on creating and curating sentiment lexicons <REF> or used n-grams for classification  have been replaced by models that are able to exploit compositionality  or implicitly learn relations between tokens . These neural models push the state of the art to over 90% accuracy on binary sentence-level sentiment analysis."
"• One of the major challenges that O DB raises is the construction of the subjective database, that requires extracting relevant information from text and designing the subjective database schema. We developed a novel extraction pipeline which requires little NLP expertise from the schema designer and also facilitates the automatic discovery of potential markers for subjective attributes. We show that our extraction pipeline achieves state-of-the-art performance by leveraging the most recent advances in NLP techniques such as BERT <REF>."
"Language model pre-training has been shown to be effective for improving many natural language processing tasks <REF>. The pretrained model BERT proposed by  has especially significant impact. It has been applied to multiple NLP tasks and obtains new state-of-theart results on eleven tasks. The tasks that BERT has been applied to are typically modeled as classification problems and sequence labeling problems. It has also been applied to the SQuAD question answering  problem, in which the objective is to find the starting point and ending point of an answer span."
"The approaches discussed above are implemented over the Freebase knowledge graph. <REF> propose an attention-based method to compute different representations of the NLQ for each relation in the logical form, and evaluate their approach on LC-QuAD and QALD-7. They also show that transfer learning across KGQA datasets is an effective method of offsetting the general lack of training data, by pre-training their models on LC-QuAD, and fine-tuning on QALD. Additionally, their work demonstrates that the use of pre-trained language models  for KGQA is a potentially beneficial technique for further increasing the model's performance.  also explores the use of pre-trained language models for the KGQA task, over SimpleQuestions (which uses Freebase)."
"Data Augmentation for MRC Several attempts have been made to augment training data for machine reading comprehension. We catego- rize these work according to the type of the augmentation data: external data source, paragraphs or questions. <REF> fine-tune BERT on the SQuAD dataset jointly with another dataset TriviaQA .  paraphrase paragraphs with backtranslation. Another line of work adheres to generate answerable questions.  propose to generate questions based on the unlabeled text for semisupervised question answering.  propose a rule-based system to generate multiplechoice questions with candidate options upon the paragraphs. We aim at generating unanswerable questions as a means of data augmentation."
"However, most of the approaches cannot capture the similar semantic from the training dataset since only sentences, which contain the target entity pair, are employed to train the RE model. Many existing works, such as Word2vec <REF>, GloVe , and BERT , etc., can extract the semantic information of words from the unlabeled corpora, rather than entities. In contrast to extracting semantic information of words, we aim at mining the semantic information of entities to furthermore improve the performance of RE model."
"These approaches have, on multiple occasions, broken the state-of-the-art records (SOTAs) across the board on a range of NLP tasks and datasets <REF>  . However, all of these datasets are designed for deep learning: they are typically large enough that they warrant the use of deep learning (5000+ examples per class), without the necessity of transfer learning. It is our view that what transfer learning does, in these cases, is push the boundaries of performance."
"The results in <REF> indicate that, in a large multi-task setting, high resource tasks are starved for capacity while low resource tasks benefit significantly from transfer, and the extent of interference and transfer are strongly related. However, this trade-off could be controlled by applying proper data sampling strategies. To enable more control over sampling, we investigate batch balancing strategies , along the lines of the temperature based variant used for training multilingual BERT . For a given language pair, l, let D l be the size of the available parallel corpus. Then if we adopt a naive strategy and sample from the union of the datasets, the probability of the sample being from language l is p l = D l Σ k D k . However, this strategy would starve low resource languages. To control the ratio of samples from different language pairs, we sample a fixed number of sentences from the training data, with the probability of a sentence belonging to language pair l being proportional to p 1 T l , where T is the sampling temperature. As a result, T = 1 corresponds to true data distribution and T = 100 corresponds to (almost) equal number of samples for each language (close to a uniform distribution with over-sampled low-resource languages). Please see  for an illustration of the effect of temperature based sampling overlaid on our dataset distribution. We repeat the experiment in Section 4.1 with temperature based sampling, setting T = 5 for : Effect of varying the sampling temperature on the performance of multilingual models. From left to right, languages are arranged in decreasing order of available training data. Results are reported relative to those of the bilingual baselines (2). Performance on individual language pairs is reported using dots and a trailing average is used to show the trend. The colors correspond to the following sampling strategies: (i) Green: True data distribution (T = 1) (ii) Blue: Equal sampling from all language pairs (T = 100) (iii) Red: Intermediate distribution (T = 5). Best viewed in color. a balanced sampling strategy, and depict our results in . Results over different language groups by resource size are also summarized in . We notice that the balanced sampling strategy improves performance on the high resource languages for both translation directions (compared to T = 100), while also retaining high transfer performance on low resource languages. However, performance on high and medium resource languages still lags behind their bilingual baselines by significant margins."
"The largest available source of symbolic music data is the Lakh MIDI Dataset <REF> which contains over 9000 hours of music. This dataset is structurally heterogeneous (different instruments per piece) making it challenging to model directly. However, intuition suggests that we might be able to benefit from the musical knowledge ingrained in this dataset to improve our performance on chiptune generation. Accordingly, we propose a procedure to heuristically map the arbitrary ensembles of music in Lakh MIDI into the four-voice ensemble of the NES. We then pre-train our generative model on this dataset, and fine-tune it on NES-MDB. We find that this strategy improves the quantitative performance of our generative model by 10%. Such transfer learning approaches are common practice in state-ofthe-art natural language processing , and here we develop new methodology to employ these techniques in the music generation setting (as opposed to analysis )."
"Word embedding <REF> and parsing information  have been proved to be beneficial in traditional TTS pipelines. In this paper, instead of taking character or phone sequence as the only input, we propose to utilize information from pre-trained word embedding and grammatical structure of sentences to improve the system performance. This can be viewed as feature based pre-training , which borrows knowledge from features generated by models trained with large data corpus. The word embedding is pre-trained with neural machine translation (NMT) task , which is based on a sequence-to-sequence encoder decoder model with an attention mechanism. The grammatical structure is extracted by the Stanford Parser tool , which is a statistical parser using knowledge of language gained from hand-parsed sentences."
"As a first evaluation on our new dataset we compare to what extent five distributional embedding models, word2vec , Anchored Packed Trees , fastText <REF>, ELMo , and BERT , and two bi-directional LSTM (biLSTM) encoders, pre-trained on SNLI  and DNC , respectively, are able to perform natural language inference on temporal predications. In our evaluation, we refrain from fine-tuning any of the models as our goal is to assess to what extent tense and aspect are captured in these models per se. As a pre-requisite diagnostic task for natural language inference between temporal predications we analysed whether the models encode the morphosyntax of tense and aspect and found that they capture a considerable amount of morphosyntactic information in their respective embedding spaces. However, neither of the models outperforms a majority class baseline on our proposed dataset due to their reliance on contextual similarity for performing inference, suggesting that models based on distributional semantics struggle with the more latent nature of tense and aspect. Our contributions in this paper are as follows:"
"We augment training with self-critical policy learning <REF> using sentence-level Smatch scores  as reward. This objective is particularly well suited for AMR parsing, since it overcomes the issues arising from the lack of token-level AMR-to-text alignments. In addition, we perform several modifications which are inspired from neural machine translation  and by the recent trends on contextualized representations ."
"We employ a baseline model based upon <REF> and , which is a convolutional neural network (CNN) with position embeddings and a ranking loss (referred to as CRCNN in this paper). We initialize the model with pre-trained word embeddings: the senna embeddings by  for the general domain dataset and the PubMed-PMC-wikipedia embeddings released by  for the medical domain. We test several perturbations on top of CRCNN model, such as piecewise max-pooling, as suggested by  and the more recent ELMo embeddings by . To compare different featurizations of contextualized embeddings, we also employ the embeddings generated by the BERT model (rather than the standard fine-tuning approach). For ELMo, we use the Original (5.5B) model weights in semeval and PubMed contributed model weights in the medical datasets released by . For BERT, we use the BERT-large uncased model (without whole word masking) in semeval released by , BioBERT by  in ddi and Clinical BERT by  in i2b2."
"In 2018, the BERT (Bidirectional Encoder Representations from Transformers) language representation model achieved state-of-the-art performance across NLP tasks ranging from sentiment analysis to question answering <REF>. Recently, the OpenAI GPT-2 (Generative Pretrained Transformer-2) model achieved stateof-the-art results across several language modeling benchmarks in a zero-shot setting ."
"While there has been significant progress in the vision and language communities thanks to recent advances in deep representations <REF>, much of this progress has been on 'internet AI' rather than embodied AI. The focus of the former is pattern recognition in images, videos, and text on datasets typically curated from the internet . The focus of the latter is to enable action by an embodied agent (e.g. a robot) in an environment. This brings to the fore active perception, long-term planning, learning from interaction, and holding a dialog grounded in an environment."
"Figure 4: A running example of our dynamic context based IU boundary detector. In this example, the model learns to determine the classification of the current anchor, ""姬"" (we insert an additional symbol, SEP to be consistent with the training format in the work of <REF>). If the probability (0.4 in left side case) of decision for a boundary at the present anchor is smaller than a threshold, i.e., 0.7, then it is necessary to consider more context (additional context: ""这个"") to make a reliable decision (0.8 in right side case)."
"Learning machine comprehension models requires a lot of question answering data. Therefore, transfer learning from pre-training language models based on a large-scale unlabeled corpus is useful for improving the model accuracy. In particular, BERT <REF> achieved stateof-the-art results when performing various tasks including the single-turn machine comprehension dataset SQuAD . BERT takes a concatenation of two sequences as input during pre-training and can capture the relationship between the two sequences. When adapting BERT for MC, we use a question and a passage as input and fine-tune the pre-trained BERT model to extract an answer from the paragraph. However, BERT can accept only two sequences of 512 tokens and thus cannot handle CMC naively.  proposed a method for CMC that is based on an architecture for single-turn MC and uses BERT as a feature-based approach. To convert CMC into a single-turn MC task, the method uses a reformulated question, which is the concatenation of the question and answer sequences in a multi-turn context with a special token. It then uses BERT to obtain contextualized embeddings for the reformulated question and paragraph, respectively. However, it cannot use BERT to capture the interaction between each sequence in the multi-turn context and the paragraph."
"Note there are other works on learning attention that could potentially increase the prediction accuracy <REF>, but none of them are directly comparable to RA-CNN. We introduced the smallest difference (whether the sentence vector is trained using the rationale label) between AT-CNN and RA-CNN to make a fair comparison between their generated explanations."
"Self-supervised context modeling. Recently, there has between a lot of work on self-supervised context modeling for language representations <REF>. In particular, the BERT model, which stands for Bidirectional Encoder Representations from Transformers , pre-trains deep bidirectional representations by jointly conditioning on both left and right context in all layers. The pre-trained BERT representations can be fine-tuned with just one additional output layer to create state-of-theart models for a wide range of tasks, such as question answering and language inference. Our representation builds on this approach and adapts it to continuous video data by adding a contrastive loss, which avoids quantizing the visual descriptors into tokens ."
"Advances in deep learning techniques (e.g., attention mechanisms, memory modules, and architecture search) have considerably improved natural language processing (NLP) models on many important tasks. For example, machine performance on both Chinese-English machine translation and document question answering on the Stanford question answering dataset (SQuAD; <REF> has been claimed to have surpassed human levels . While the tasks that initiated learning-based NLP models were motivated by external demands and are important applications in their own right (e.g., machine translation, question answering, automatic speech recognition, text to speech, etc.), there is a marked and troubling tendency for recent datasets to be set up to be easy to solve with little in the way of generalization or abstraction; for instance, ever larger datasets created by crowd-sourcing processes that may not well approximate the natural distributions they are intended to span, although there are some notable counter-examples . When there exist multiple datasets that are representative of the exact same task from different domains (e.g., various question answering datasets), we rarely evaluate on all of them. This state of affairs promotes development of models that only work well for a specific purpose, overestimates our success at having solved the general task, fails to reward sample efficient generalization that requires the ability to discover and make use of rich linguistic structures, and ultimately limits progress."
"Semantic Similarity Augmentation Using distributed word representation (word embeddings) <REF>, one can identify semantically similar words . This approach requires either pre-trained word embedding models for the language at hand or enough data from the target application to be able to build the embedding model. This approach thus does not require access to a dictionary or thesaurus for a language in order to find synonyms. This can benefit languages where such resources might be harder to obtain but there might be enough unsupervised text data to be able to build the embedding models. With recent advances in building complete language models , further advances can be made to identify syntactic and semantic similarity for word augmentation."
"Pre-trained language representations such as ELMo <REF>, OpenAI GPT , BERT , ERNIE 1.0    and XLNet  have been proven to be effective for improving the performances of various natural language understanding tasks including sentiment classification , natural language inference , named entity recognition  and so on."
"Pre-trained natural language processing (NLP) systems <REF> have been shown to transfer remarkably well on downstream tasks including text classification, question answering, machine translation, and summarization . Such approaches involve a pre-training phase followed by the addition of task-specific layers and a subsequent re-training or fine-tuning of the conjoined model. Each taskspecific layer relies on an inductive bias related to the kind of target task. For question answering, a task-specific span-decoder is often used to extract a span of text verbatim from a portion of the input text . For text classification, a task-specific classification layer with fixed classes is typically used instead. This latter task-specific inductive bias is unnecessary. On several tasks predominantly treated as text classification, we find that reformulating them as spanextraction problems and relying on a task-specific * Equal contribution. span-decoder yields superior performance to using a task-specific classification layer."
"Non-local approaches: To denoise images, non-local means <REF> leverages the simple observation that Gaussian noise can be removed by non-locally weighted averaging all pixels in an image. Recently, non-local neural networks  have been proposed to capture long-range dependencies in video understanding;  uses the non-local module to denoise feature maps to defend against adversarial attacks. Another instantiation of non-local neural networks, known as relational networks , has shown effectiveness in visual reasoning , meta-learning , object detection , and reinforcement learning . Its counterpart in natural language processing, attention, is arguably the most fruitful recent advance in this discipline.  replaces recurrent neural networks  with a model called Transformer, consisting of several stacked multi-head attention modules. Transformer-based models  outperform other recurrent models by a considerable amount in natural language processing. In our work, we also use Transformer to learn contextual information of point clouds."
"Network. Transformer network <REF> was first introduced for neural machine translation (NMT) tasks, where the encoder and decoder side each leverages a self-attention  transformer. After each layer of the self-attention, the encoder and decoder are connected by an additional decoder sublayer where the decoder attends to each element of the source text for each element of the target text. We refer the reader to  for a more detailed explanation of the model. In addition to NMT, transformer networks have also been successfully applied to other tasks, including language modeling , semantic role labeling , word sense disambiguation , learning sentence representations , and video activity recognition . This paper absorbs a strong inspiration from the NMT transformer to extend to a multimodal setting. Whereas the NMT transformer focuses on unidirectional translation from source to target texts, human multimodal language time-series are neither as well-represented nor discrete as word embeddings, with sequences of each modality having vastly different frequencies. Therefore, we propose not to explicitly translate from one modality to the others (which could be extremely challenging), but to latently adapt elements across modalities via the attention. Our model (MulT) therefore has no encoder-decoder structure, but it is built up from multiple stacks of pairwise and bidirectional crossmodal attention blocks that directly attend to low-level features (while removing the self-attention). Empirically, we show that our proposed approach improves beyond standard transformer on various human multimodal language tasks."
"Section 3 details how DL models considered are trained, the fundamental arithmetic operations involved during training, and their e ects on di erent hardware systems. Speci cally, Section 3.2 dissects CNN models for computer vision, while Section 3.3 explores the state-of-the-art Bidirectional Encoder Representations from Transformers (BERT) model for natural language processing (NLP) <REF>. e detailed performance analysis is done along a few important dimensions. Section 4.1 presents the performance of key global communication kernels used in the benchmarks considered. Section 4.2 discusses performance and scalability of large and high-throughput DL models. Section 4.3 compares performance when the benchmarks are expressed in an easy-to-code multi-GPU architecture enabled by system so ware described in Section 2.2."
"In this paper, we intend to propose metrics sensitive to both quality and diversity simultaneously, assigning low scores not only to models generating low-quality samples but also to the ones with low-diversity samples (including the mode collapsed models). To this end, we first propose the MS-Jaccard as an n-gram based measure that considers the quality and diversity of generated samples simultaneously. It attempts to find the similarity of the set of generated samples by a model and the set of real (or test) samples. Then, a featurebased measure is proposed to compare the real data distribution and the generative model distribution in the feature space. Indeed, by borrowing the idea of FID <REF> that is a popular feature-based evaluation metric in im-age generation tasks and advent of a recent highly deep model named BERT  as a reference feature extractor for natural language texts, a metric is proposed for evaluation of natural language generation. Finally, appropriate divergences between the oracle distribution and the (learned) model distribution is introduced for when the probabilistic oracle is considered as synthetic data distribution (and thus the target distribution is available for evaluation)."
"Contextualized Representations Our approach is also relevant to recent work on contextualized language representations that are pretrained on large-scale language corpora. These representations can then be simply integrated or fine-tuned for improved performance on many downstream tasks <REF>. SSL , CoVe , and ELMo  all learned contextualized representations through training RNN language models and encoder-decoders. Follow-up work extended these ideas, but replaced the RNN with a deep transformer  that was trained to learn language patterns on a large story dataset. BERT  more clearly extended the idea of using Transformers for language modeling by making the encoded representations bidirectional and adding two new loss functions: a masked token loss and next sentence prediction loss for more accurate discourse representations. More recently, GPT2  expanded the scale of pretrained language models, and showed promising results on zero-shot tasks."
"How does it compare to pretrained encoders? Pretrained encoders (e.g. ELMo and BERT <REF>) improve neural models with improved representations, while our framework augments the graph using first-order logic. It is important to study the interplay of these two orthogonal directions. We can see in , our augmented model consistently outperforms baseline even with the presence of ELMo embeddings."
"The best classifier has already been integrated in a system that is able to efficiently mine comparative sentences from web-scale sources and to identify the direction of the comparisons: CAM-the comparative argumentative machine <REF>. CAM mines sentences from the web-scale Common Crawl and uses them to argumentatively compare objects specified by a user (e.g., whether Python is better than MATLAB for NLP).  Promising directions for future work are exploiting neural classification approaches, integrating features based on contextualized word representations , and better handling direction shifters like negations and complex implicit syntactic comparative constructions."
"Recently, we have witnessed a big success in the capability of computers to seemingly understand natural language text and to generate plausible responses to conversations <REF>. A challenging task of building dialogue systems lies in evaluating the quality of their responses. Typically, evaluating goal-oriented dialogue systems is done via human-generated judgment like a task completion test or user satisfaction score ). However, the task of evaluating open-ended dialogue systems is not well defined as there is no * Equal Contribution clear explicit goal for conversations. Indeed, dialog systems are ultimately created to satisfy the user's need which can be associated with how entertaining and engaging the conversation was. It is unclear how to define a metric that can account comprehensibly for the semantic meaning of the responses. Moreover, grasping the underlying meaning of text has always been fraught with difficulties, which are essentially attributed to the complexities and ambiguities in natural language. Generally, a good dialogue can be described as an exchange of information that sustain coherence through a train of thoughts and a flow of topics. Therefore, a plausible way to evaluate open-ended dialogue systems is to measure the consistency of the responses. For example, a neural dialogue system can respond to the utterance ""Do you like animals?"" by ""Yes, I have three cats"", thereafter replies to ""How many cats do you have"" by ""I don't have cats."". Here, we can notice that the dialogue system failed to provide a coherent answer and instead generated an inconsistent response."
"Self-a ention is an a ention mechanism relating di erent positions of a single sequence <REF>, which has been used successfully in a variety of tasks . Transformer , a transduction model relying on self-a ention, has been widely used and greatly improved the performance for language processing tasks . In recent language representation research, bidirectional Transformer encoder has exhibited the best performance (BERT ) when compared to le -to-right Transformer decoder (OpenAI GPT ) and bidirectional LSTM (ELMo ). Recently, multi-head self-a ention is also introduced to model users' behavior sequences for sequential recommendation . Di erent from them, in this paper, we adopt the self-a ention to model the compatibility in fashion out t generation. "
"Unfortunately, many of these models are known to be heavily parameterized, with state-of-the-art models easily containing millions or billions of parameters <REF>. This renders practical deployment challenging. As such, the enabling of efficient and lightweight * Work done while at University of Maryland. adaptations of these models, without significantly degrading performance, would certainly have a positive impact on many real world applications."
"Text Features: We represent the textual utterances in the dataset using BERT <REF>, which provides a sentence representation u t ∈ R dt for every utterance u. In particular, we average the last four transformer layers of the first token ([CLS]) in the utterance -using the BERTBase model -to get a unique utterance representation of size d t = 768. We also considered averaging Common Crawl pre-trained 300 dimensional GloVe word vectors  for each token; however, it resulted in lower performance as compared to BERT-based features."
"Results. As shown in <REF>, RefReader achieves state-of-the-art performance, outperforming strong pretrained and retrained systems (e.g., , as well as domainspecific heuristics (Parellelism+URL). Language model pretraining yields an absolute gain of 3.2 in F 1 . This demonstrates the ability of RefReader to leverage unlabeled text, which is a distinctive feature in comparison with prior work. When training is carried out in the unsupervised setting (with the language modeling objective only), the model is still capable of learning the latent coreferential structure between pronouns and names to some extent, outperforming a supervised coreference system that gives competitive results on OntoNotes . We also test a combination of RefReader and BERT , using BERT's contextualized word embeddings as base features x t (concatenation of the top 4 layers), which yields substantial improvements in accuracy. While this model still resolves references incrementally, it cannot be said to be purely incremental, because BERT uses ""future"" information to build its contextualized embeddings. 5 Note that the gender 5 Future work may explore the combination of RefReader bias increases slightly, possibly due to bias in the data used to train BERT."
"Our bi-directional transformer architecture predicts every token in the training data ( <REF>). We achieve this by introducing a cloze-style training objective where the model must predict the center word given left-to-right and right-to-left context representations. Our model separately computes both forward and backward states with * Equal contribution.  : Illustration of the model. Block i is a standard transformer decoder block. Green blocks operate left to right by masking future time-steps and blue blocks operate right to left. At the top, states are combined with a standard multi-head self-attention module whose output is fed to a classifier that predicts the center token. a masked self-attention architecture, that closely resembles a language model. At the top of the network, the forward and backward states are combined to jointly predict the center word. This approach allows us to consider both contexts when predicting words and to incur loss for every word in the training set, if the model does not assign it high likelihood. Experiments on the GLUE ) benchmark show strong gains over the state of the art for each task, including a 9.1 point gain on RTE over . These improvements are consistent with, if slightly behind, those achieved by the concurrently developed BERT pretraining approach , which we will discuss in more detail in the next section. We also show that it is possible to stack taskspecific architectures for NER and constituency parsing on top of our pretrained representations, and achieve new state-of-the-art performance lev-els for both tasks. We also present extensive experimental analysis to better understand these results, showing that (1) cross sentence pretraining is crucial for many tasks; (2) pre-training continues to improve performance with up to 18B tokens and would likely continue to improve with more data; and finally (3) our novel cloze-driven training regime is more effective than predicting left and right tokens separately."
"Models We used three models: BERT <REF>, BiLSTM+ELMo+Attn , and ESIM ."
"In this work, we further explore the use of substitute-based approaches for WSI. After verifying that the approach transfers to the recently introduced BERT deep masked LM <REF> (with a very significant improvement in WSI scores), we make two additional contributions to the mentioned method: (a) we present a method to move from a fixed number of senses across target words to choosing a dynamic number of senses for each target (as supported by most other WSI methods, e.g., ; and (b) we suggest to use the substitutes as a mean of interpreting / analyzing the resulting sense cluster by considering prominent word substitutes. This enables a more in-depth error analysis, showcasing and quantifying the remaining kinds of errors in WSI."
"Using 3 datasets altogether provides another marginal improvement. Our model obtains the best results among existing methods that do not use a large language model (e.g., ELMo). Our ELMo version also outperforms any other models which are under the same setting. We note that BERT <REF> uses a much larger model than ours(around 20x), and we leave the performance of combining BERT with MTL as interesting future work."
"• It introduces structure-invariant testing (SIT), a novel, widely applicable methodology for validating machine translation software; • It describes a practical implementation of SIT by adapting BERT <REF>] to generate similar sentences and leveraging syntax parsers to represent sentence structures; • It presents the evaluation of SIT using only 200 unlabeled sentences crawled from the Web to successfully find 56 buggy translations in Google Translate and 61 buggy translations in Bing Microsoft Translator with high accuracy; and • It discusses the diverse bugs found by SIT, including under-translation, over-translation, incorrect modification, word/phrase mistranslation, and unclear logic, of which none could be found by state-of-the-art metrics (i.e., BLEU and ROUGE)."
The recently introduced BERT model ( <REF> exhibits strong performance on several language understanding benchmarks. To what extent does it capture syntax-sensitive structures?
"Training deep networks has always been a challenging problem, mainly due to the difficulties in optimization for deep architecture. Breakthroughs have been made in computer vision to enable deeper model construction via advanced initialization schemes <REF>, multi-stage training strategy , and : Performances of Transformer models with different number of encoder/decoder blocks (recorded on x-axis) on WMT14 En→De translation task. † denotes the result reported in  novel model architectures . While constructing very deep neural networks with tens and even more than a hundred blocks have shown effectiveness in image recognition , question answering and text classification , scaling up model capacity with very deep network remains challenging for NMT. The NMT models are generally constructed with up to 6 encoder and decoder blocks in both state-of-the-art research work and champion systems of machine translation competition. For example, the LSTM-based models are usually stacked for 4  or 6 (Chen et al., 2018) blocks, and the state-of-the-art Transformer models are equipped with a 6-block encoder and decoder . Increasing the NMT model depth by directly stacking more blocks results in no improvement or performance drop ), and even leads to optimization failure ."
"• BERT <REF>) -a deep, bidirectional transformer model with sequence classification layers on the top. The BERT language model was pre-trained, so only the sequence classifier was initialized and trained on the SemEval data. We used the PyTorch implementation of the case-insensitive 'base' version 1 with the optimal number of epochs (10) determined on the development set. When tested on the training data in cross-validation, this solution alone achieved a micro-accuracy of 0.717."
"In our previous work <REF>, we proposed a multi-criteria learning framework for CWS, which uses a shared layer to extract the common underlying features and a private layer for each criterion to extract criteria-specific features. He et al.  used a shared BiLSTM+CRF to deal with all the criteria by adding two artificial tokens at the beginning and end of an input sentence to specify the required target criteria. Huang et al.  proposed a domain adaptive segmenter to capture diverse criteria based on Bidirectional Encoder Representations from Transformers (BERT) ."
"Xu et al. <REF> proposed a local detection approach for NER based on fixed-size ordinally forgetting encoding (FOFE) , FOFE explores both character-level and wordlevel representations for each fragment and its contexts. In the multi-modal NER system by Moon et al. , for noisy user-generated data like tweets and Snapchat captions, word embeddings, character embeddings, and visual features are merged with modality attention. Ghaddar and Langlais  found that it was unfair that lexical features had been mostly discarded in neural NER systems. They proposed an alternative lexical representation which is trained offline and can be added to any neural NER system. The lexical representation is computed for each word with a 120-dimensional vector, where each element encodes the similarity of the word with an entity type. Recently, Devlin et al.  proposed a new language representation model called BERT, bidirectional encoder representations from transformers. BERT uses masked language models to enable pre-trained deep bidirectional representations. For a given token, its input representation is comprised by summing the corresponding position, segment and token embeddings."
"The models that are learned have two other advantages, which are not explored in this paper. They have interpretable latent variables, corresponding to the template variables, and on several benchmark tasks the results exceed the current state-of-the-art. For the case of question-answering, the models we propose are compatible with recently-developed approaches for producing contextual representations of language by large-scale pre-training <REF>."
"We implement a hierarchical neural model with independent decoders for each coarse-grained morphological feature and show that incorporating POS information in the shared encoder helps improve prediction for other features. Furthermore, our multi-lingual transfer methods not only help improve results for related languages but also eliminate the need of training individual models for each dataset from scratch. In future, we plan to explore the use of pre-trained multi-lingual word embeddings such as BERT <REF>, in our encoder.  and 6 document the comprehensive results of our submissions. MULTI-SOURCE was our previous submission to the shared task. We conducted additional experimentas with the addition of selfattention and also report the results for MULTI-SOURCE+SELF-ATTENTION. We report both the accuracy and F1 metric."
"In this work, we focus on the setting where queries and their associated candidate passages are given but no relevance judgment is available. Instead of solely relying on the labels from single source (BM25 score), we propose to leverage the weak supervision signals from diverse sources. <REF> proposed a general data programming framework to create data and train models in a weakly supervised manner. To tailor to the ranking tasks, instead of generating a ranked list of passages for each query, we generate binary labels for each query-passage pair. In our neural ranking models, we focus on BERT-based ranking model  (architecture shown in , which achieves new state-of-the-art performance on two public benchmark datasets with full supervision. The contributions of this work are in two fold: (a) we propose a simple data programming framework for ranking tasks; (b) we train a BERT ranking model using our framework, by considering two simple sources of weak supervision signals, unsupervised ranking methods (BM25 and TF-IDF scores) and unsupervised semantic feature representation, we show our model outperforms BM25 baseline by a large margin (around 20% relative improvement in top-1 accuracy on average) and the previous state-of-the-art performance (around 10% relative improvement in top-1 accuracy on average) on three datasets without using ground-truth training labels."
"The NLP community is, however, witnessing a dramatic paradigm shift toward the pretrained deep language representation model, which achieves state of the art in question answering, sentiment classification, and similarity modeling, to name a few. Bidirectional Encoder Representations from Transformers (BERT; <REF> represents one of the latest developments in this line of work. It outperforms its predecessors, ELMo  and GPT , staggeringly exceeding state of the art by a wide margin on multiple natural language understanding tasks."
"In this section, we evaluate the performance of SRL embeddings on two kinds of text comprehension tasks, textual entailment and reading comprehension. Both of the concerned tasks are quite challenging, and could be even more difficult considering that the latest performance improvement has been already very marginal. However, we present a new solution instead of heuristically stacking network design techniques. Namely, we show that SRL embeddings could be potential to give further advances due to its meaningful linguistic augments, which has not been studied yet for the concerned tasks. <REF> shows the hyper-parameters of our models. In our experiments, we basically follow the same hyper-parameters for each model as the original settings from their corresponding literatures  except those specified (e.g. SRL embedding dimension). For both of the tasks, we also report the results by using pre-trained BERT ) as word representation in our baseline models 6 . The hyperparameters were selected using the Dev  We use the last layer of BERT output. Since BERT is in subword-level while semantics role labels are in word-level,  set, and the reported Dev and Test scores are averaged over 5 random seeds using those hyper-parameters."
"Recently, we have witnessed deep neural networks (DNNs) have delivered super-human accuracy in a variety of practical uses, such as facial recognition <REF>, object detection , self-driving cars  and speech understanding . Along with the huge success of deep learning also comes many kinds of adversarial attacks , among which trojan attacks  are a relatively novel one. Technically, this kind of attacks inserts contaminated data samples into the training data of a deep learning system, seeking to trick the system into learning a trojan backdoor through Conference'17, July 2017, Washington, DC, USA The first two authors contributed equally."
"https://facebookresearch. github.io/ELI5, https://github.com/ facebookresearch/ELI5 Question: How do Jellyfish function without brains or nervous systems? <REF> (60 words) Answer: Jellyfish may not have a brain, but they have a rough nervous system and innate behaviours. However, they are very simple creatures. They're invertebrate: creatures without a backbone. Most jellyfish have really short life spans. Sometimes just a couple of hours.  As their name implies, they are largely composed of basically jelly inside a thin membrane. They're over 95% water. questions are complex and cannot be easily addressed by a short response  or by extracting a word or phrase from an evidence document . Answers also represent one of several valid ways of addressing the query. Many state-of-the-art question answering models perform well compared to human performance for extractive answer selection ). However, their success does not directly carry over to our setting."
"Since 2012, the field of artificial intelligence has reported remarkable progress on a broad range of capabilities including object recognition, game playing, machine translation, and more <REF>. This progress has been achieved by increasingly large and computationally-intensive deep learning models.    reproduced from  plots training cost increase over time for state-of-the-art deep learning models starting with AlexNet in 2012  to AlphaZero in 2017 . The chart shows an overall increase of 300,000x, with training cost doubling every few months. An even sharper trend can be observed in NLP word embedding approaches by looking at ELMo  followed by BERT , openGPT-2 , and XLNet . An important paper  has estimated the carbon footprint of several NLP models and argued that this trend is both environmentally unfriendly (which we refer to as Red AI) and expensive, raising barriers to participation in NLP research. This trend is driven by the strong focus of the AI community on obtaining ""state-of-the-art"" results, 2 as exemplified by the rising popularity of leaderboards , which typically report accuracy measures but omit any mention of cost or efficiency (see, for example, leaderboards.allenai.org). Despite the clear benefits of improving model accuracy in AI, the focus on this single metric ignores the economic, environmental, or social cost of reaching the reported accuracy."
"In this paper, we focus on detecting causal sentences, i.e., sentences conveying at least one causal relation. This is a first step towards mining causal relations from texts. Once causal sentences have been detected, computationally more intensive relation extraction methods can be used to identify the exact entities that participate in the causal relations and their roles (cause, effect). To bypass the scarcity of causal instances in relation extraction datasets, we exploit transfer learning, namely ELMO <REF> and   , comparing against a bidirectional GRU with self-attention . We experiment with generic public relation extraction datasets and a new larger biomedical causal sentence detection dataset, a subset of which we make publicly available. 1 Unlike recently reported results in other NLP tasks , we find that transfer learn-ing helps only in datasets with hundreds of training instances. When a few thousands of training instances are available, BIGRUATT reaches a performance plateau (both in generic and biomedical texts), then increasing the size of the dataset or employing transfer learning does not improve performance. We believe this is the first work to (a) focus on causal sentence detection as a binary classification task, (b) consider causal sentence detection in both generic and biomedical texts, and (c) explore the effect of transfer learning in this task."
"Recent years have witnessed several success brought by deep neural networks (DNNs) in various domains. Such high expressive models outperform other models in fields, including image recognition <REF>, natural language processing , as well as the advanced applications such as healthcare analysis , brain circuits analysis , and functionality of mutations in DNA ."
"Other studies have experimentally tested the abilities of transformers to learn structures. Most related to our work, <REF> compared the ability of transformers and LSTMs to learn hierarchical structure, specifically English subjectverb agreeement and evaluating logical formulas. Based on their experimental results, they suggested that LSTMs are better at learning hierarchical structure.  experimentally investigated the power of self-attention to extract word order information, finding differences between recurrent and self-attention models; however, these were modulated by the training objective.  and  show that BERT  encodes syntactic information."
"The approach of extracting speaker embeddings via reconstructing different parts of a sequence can be consider as an application of self-supervised learning, where a network is trained with a loss on a pretext task, without the need for human annotation. Models using self-supervised learning for initialization are now state-of-the-art in several domains and tasks, such as action recognition, reinforcement learning, and natural language understanding <REF>]."
"In this paper, we propose a multi-criteria method of CWS. Our model uses a domain projection layer to adopt multiple datasets with various granularities. We adopt the bidirectional pre-training encoder from the transformer (BERT) <REF> to introduce external knowledge. BERT can be regarded contextual representations and it has achieved great success in some NLU tasks . But both the fine-tuning and inference procedures of the provided models are computationally inefficient due to a large number of parameters."
"Deep learning has seen great progress over the last decade, partially thanks to the development of methods that have facilitated scaling the effective capacity of neural networks. This trend has been most visible for image classification, as demonstrated by the accuracy improvements on ImageNet with the increase in model capacity <REF>. A similar phenomenon can also be observed in the context of natural language processing  where simple shallow models of sentence representations  are outperformed by their deeper and larger counterparts ."
"• The InterPE is randomly initialized and updated during the training procedure. Similar to the segment embedding in BERT <REF>, we differentiate sentences/images by adding E s to token embeddings of the s-th sentence/image; s denotes the order of a sentence/image in a story. s ∈ {1, 2, 3, 4, 5}. where pos is the position and i is the dimension. d model denotes the dimension of the input and output token. The IntraPE parameters are fixed, and the sinusoidal representation allows the model to extrapolate the sentence length that is longer than the training instances. The IntraPE PE s is added to the s-th token embedding in a sentence. s ∈ {1, 2, ..., n} and n is the max sentence length."
"Attention was introduced by <REF> for the encoder-decoder in a neural sequence transduction model to allow for content-based summarization of information from a variable length source sentence. The ability of attention to learn to focus on important regions within a context has made it a critical component in neural transduction models for several modalities . Using attention as a primary mechanism for representation learning has seen widespread adoption in deep learning after , which entirely replaced recurrence with self-attention. Self-attention is defined as attention applied to a single context instead of across multiple contexts (in other words, the query, keys, and values, as defined later in this section, are all extracted from the same context). The ability of self-attention to directly model long-distance interactions and its parallelizability, which leverages the strengths of modern hardware, has led to state-of-the-art models for various tasks ."